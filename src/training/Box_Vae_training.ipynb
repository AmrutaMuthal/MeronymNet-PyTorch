{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c7852d-523c-473c-b4bb-3444a7cc8fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import dense_to_sparse, to_dense_adj\n",
    "import copy\n",
    "import torch_geometric.nn as gnn\n",
    "\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torch.utils.data as data_utils\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import losses\n",
    "from AutoEncoder import AutoEncoder\n",
    "\n",
    "import sys\n",
    "import cv2\n",
    "import os\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d55f10-4613-4464-944f-dce1c9439364",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "colors = [(1, 0, 0),\n",
    "          (0.737, 0.561, 0.561),\n",
    "          (0.255, 0.412, 0.882),\n",
    "          (0.545, 0.271, 0.0745),\n",
    "          (0.98, 0.502, 0.447),\n",
    "          (0.98, 0.643, 0.376),\n",
    "          (0.18, 0.545, 0.341),\n",
    "          (0.502, 0, 0.502),\n",
    "          (0.627, 0.322, 0.176),\n",
    "          (0.753, 0.753, 0.753),\n",
    "          (0.529, 0.808, 0.922),\n",
    "          (0.416, 0.353, 0.804),\n",
    "          (0.439, 0.502, 0.565),\n",
    "          (0.784, 0.302, 0.565),\n",
    "          (0.867, 0.627, 0.867),\n",
    "          (0, 1, 0.498),\n",
    "          (0.275, 0.51, 0.706),\n",
    "          (0.824, 0.706, 0.549),\n",
    "          (0, 0.502, 0.502),\n",
    "          (0.847, 0.749, 0.847),\n",
    "          (1, 0.388, 0.278),\n",
    "          (0.251, 0.878, 0.816),\n",
    "          (0.933, 0.51, 0.933),\n",
    "          (0.961, 0.871, 0.702)]\n",
    "colors = (np.asarray(colors)*255)\n",
    "canvas_size = 550\n",
    "\n",
    "def plot_bbx(bbx):\n",
    "    bbx = bbx*canvas_size\n",
    "    canvas = np.ones((canvas_size,canvas_size,3), np.uint8) * 255\n",
    "    for i, coord in enumerate(bbx):\n",
    "        x_minp, y_minp,x_maxp , y_maxp= coord\n",
    "        if [x_minp, y_minp,x_maxp , y_maxp]!=[0,0,0,0]:\n",
    "            cv2.rectangle(canvas, (int(x_minp), int(y_minp)), (int(x_maxp) , int(y_maxp) ), colors[i], 6)\n",
    "    return canvas\n",
    "\n",
    "def load_data(batch_size: int):\n",
    "    \"\"\" Load train, validation and test sets into data loaders.\n",
    "    \n",
    "    Args:\n",
    "        batch_size: batch size to be used when creating data loader\n",
    "    \n",
    "    Returns: train and validation data loaders\n",
    "    \"\"\"\n",
    "    outfile = 'D:/meronym_data/X_train.np'\n",
    "    with open(outfile, 'rb') as pickle_file:\n",
    "        X_train = pickle.load(pickle_file)\n",
    "\n",
    "    outfile = 'D:/meronym_data/class_v.np'\n",
    "    with open(outfile, 'rb') as pickle_file:\n",
    "        class_v = pickle.load(pickle_file)\n",
    "\n",
    "    outfile = 'D:/meronym_data/adj_train.np'\n",
    "    with open(outfile, 'rb') as pickle_file:\n",
    "        adj_train = pickle.load(pickle_file)\n",
    "\n",
    "    outfile = 'D:/meronym_data/X_train_val.np'\n",
    "    with open(outfile, 'rb') as pickle_file:\n",
    "        X_train_val = pickle.load(pickle_file)\n",
    "\n",
    "    outfile = 'D:/meronym_data/class_v_val.np'\n",
    "    with open(outfile, 'rb') as pickle_file:\n",
    "        class_v_val = pickle.load(pickle_file)\n",
    "\n",
    "    outfile = 'D:/meronym_data/adj_train_val.np'\n",
    "    with open(outfile, 'rb') as pickle_file:\n",
    "        adj_train_val = pickle.load(pickle_file)\n",
    "\n",
    "    outfile = 'D:/meronym_data/X_test.np'\n",
    "    with open(outfile, 'rb') as pickle_file:\n",
    "        X_test = pickle.load(pickle_file)\n",
    "\n",
    "    outfile = 'D:/meronym_data/adj_test.np'\n",
    "    with open(outfile, 'rb') as pickle_file:\n",
    "        adj_test = pickle.load(pickle_file)\n",
    "\n",
    "    outfile = 'D:/meronym_data/class_v_test.np'\n",
    "    with open(outfile, 'rb') as pickle_file:\n",
    "        class_v_test = pickle.load(pickle_file)\n",
    "\n",
    "    X_train[X_train<=0] = 0\n",
    "    X_train_val[X_train_val<=0] = 0\n",
    "    X_test[X_test<=0] = 0\n",
    "\n",
    "    X_train[X_train>=1] = 1\n",
    "    X_train_val[X_train_val>=1] = 1\n",
    "    X_test[X_test>=1] = 1\n",
    "    \n",
    "    random.seed(100)\n",
    "    train_idx = np.random.randint(1,len(X_train),len(X_train))\n",
    "    val_idx = np.random.randint(1,len(X_train_val),len(X_train_val))\n",
    "    test_idx = np.random.randint(1,len(X_test),len(X_test))\n",
    "    \n",
    "    seed = 345\n",
    "\n",
    "    torch.set_default_dtype(torch.float32)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    validation = True\n",
    "    if validation:\n",
    "        train_list =[]\n",
    "        for idx, batch in enumerate(zip(copy.deepcopy(X_train[train_idx]),\n",
    "                                        copy.deepcopy(class_v[train_idx]),\n",
    "                                        copy.deepcopy(adj_train[train_idx]))):\n",
    "            edge_index, _ = dense_to_sparse(torch.from_numpy(batch[2]).cuda().float())\n",
    "            train_list.append(Data(x = torch.from_numpy(batch[0]).cuda().float(),\n",
    "                                   y = torch.from_numpy(batch[1]).cuda().float(),\n",
    "                                   edge_index = edge_index\n",
    "                                        )\n",
    "                             )\n",
    "\n",
    "        batch_train_loader = DataLoader(train_list, batch_size=batch_size)\n",
    "\n",
    "        val_list = []\n",
    "        for idx, batch in enumerate(zip(copy.deepcopy(X_train_val[val_idx]),\n",
    "                                        copy.deepcopy(class_v_val[val_idx]), \n",
    "                                        copy.deepcopy(adj_train_val[val_idx]))):\n",
    "            edge_index, _ = dense_to_sparse(torch.from_numpy(batch[2]).cuda().float())\n",
    "            val_list.append(Data(x = torch.from_numpy(batch[0]).cuda().float(),\n",
    "                                 y = torch.from_numpy(batch[1]).cuda().float(),\n",
    "                                 edge_index = edge_index\n",
    "                                        )\n",
    "                             )\n",
    "        batch_val_loader = DataLoader(val_list, batch_size=batch_size)\n",
    "    else:\n",
    "        train_list =[]\n",
    "        for idx, batch in enumerate(zip(copy.deepcopy(X_train[train_idx]),\n",
    "                                        copy.deepcopy(class_v[train_idx]),\n",
    "                                        copy.deepcopy(adj_train[train_idx]))):\n",
    "            edge_index, _ = dense_to_sparse(torch.from_numpy(batch[2]).cuda())\n",
    "            train_list.append(Data(x = torch.from_numpy(batch[0]).cuda(),\n",
    "                                   y = torch.from_numpy(batch[1]).cuda(),\n",
    "                                   edge_index = edge_index\n",
    "                                        )\n",
    "                             )\n",
    "\n",
    "        for idx, batch in enumerate(zip(copy.deepcopy(X_train_val[val_idx]),\n",
    "                                        copy.deepcopy(class_v_val[val_idx]), \n",
    "                                        copy.deepcopy(adj_train_val[val_idx]))):\n",
    "            edge_index, _ = dense_to_sparse(torch.from_numpy(batch[2]).cuda())\n",
    "            train_list.append(Data(x = torch.from_numpy(batch[0]).cuda(),\n",
    "                                 y = torch.from_numpy(batch[1]).cuda(),\n",
    "                                 edge_index = edge_index\n",
    "                                        )\n",
    "                             )\n",
    "        batch_train_loader = DataLoader(train_list, batch_size=batch_size)\n",
    "\n",
    "        val_list = []\n",
    "        for idx, batch in enumerate(zip(copy.deepcopy(X_test[test_idx]),\n",
    "                                        copy.deepcopy(class_v_test[test_idx]), \n",
    "                                        copy.deepcopy(adj_test[test_idx]))):\n",
    "            edge_index, _ = dense_to_sparse(torch.from_numpy(batch[2]).cuda())\n",
    "            val_list.append(Data(x = torch.from_numpy(batch[0]).cuda(),\n",
    "                                 y = torch.from_numpy(batch[1]).cuda(),\n",
    "                                 edge_index = edge_index\n",
    "                                        )\n",
    "                             )\n",
    "        batch_val_loader = DataLoader(val_list, batch_size=batch_size) \n",
    "        \n",
    "        return batch_train_loader, batch_val_loader\n",
    "    \n",
    "def main():\n",
    "    \n",
    "    latent_dims = 32\n",
    "    batch_size = 32\n",
    "    num_nodes = 24\n",
    "    bbx_size = 4\n",
    "    num_classes = 10\n",
    "    label_shape = 1\n",
    "    nb_epochs = 50\n",
    "    klw = losses.frange_cycle_linear(nb_epochs*10)\n",
    "    learning_rate = 0.00002\n",
    "    \n",
    "    batch_train_loader, batch_val_loader = load_data(batch_size)\n",
    "\n",
    "\n",
    "    reconstruction_loss_arr = []\n",
    "    kl_loss_arr = []\n",
    "    bbox_loss_arr = []\n",
    "    adj_loss_arr = []\n",
    "    node_loss_arr = []\n",
    "\n",
    "    reconstruction_loss_val_arr = []\n",
    "    kl_loss_val_arr = []\n",
    "    bbox_loss_val_arr = []\n",
    "    adj_loss_val_arr = []\n",
    "    node_loss_val_arr = []\n",
    "\n",
    "    vae = AutoEncoder(latent_dims,num_nodes,bbx_size,num_classes,label_shape)\n",
    "    vae.cuda()\n",
    "    optimizer = optim.Adam(vae.parameters(), lr=learning_rate)\n",
    "    writer = SummaryWriter('D:/meronym_data/runs/20k-ld32-lr-'+str(learning_rate)+'-batch-'+str(batch_size))\n",
    "    icoef = 0\n",
    "\n",
    "    for epoch in range(nb_epochs):\n",
    "\n",
    "        batch_loss = 0.0\n",
    "        batch_kl_loss = 0.0\n",
    "        batch_bbox_loss = 0.0\n",
    "        batch_adj_loss = 0.0\n",
    "        batch_node_loss = 0.0\n",
    "        images = []\n",
    "\n",
    "        vae.train()\n",
    "        i=0\n",
    "        for train_data in batch_train_loader:\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                train_data = train_data.cuda()\n",
    "\n",
    "            node_data_true = train_data.x\n",
    "            label_true = node_data_true[:,:1]\n",
    "            class_true = train_data.y\n",
    "            adj_true = train_data.edge_index\n",
    "            batch = train_data.batch\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = vae(adj_true, node_data_true, label_true , class_true)\n",
    "            node_data_pred, label_pred, adj_pred, class_pred, z_mean, z_logvar = output\n",
    "\n",
    "            kl_loss = losses.kl_loss(z_mean, z_logvar)\n",
    "            adj_loss = losses.adj_loss(adj_pred, adj_true, batch, num_nodes)\n",
    "            bbox_loss = losses.bbox_loss(node_data_pred, node_data_true[:,1:])\n",
    "            node_loss = losses.node_loss(label_pred,label_true)\n",
    "            class_loss = losses.class_loss(class_pred, class_true)\n",
    "\n",
    "            kl_weight = klw[icoef]\n",
    "\n",
    "            recostruction_loss = kl_loss*kl_weight + (bbox_loss + node_loss + adj_loss + class_loss)*24*5\n",
    "            recostruction_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            batch_loss += recostruction_loss.item()\n",
    "            batch_kl_loss += kl_loss.item()\n",
    "            batch_bbox_loss += bbox_loss.item()\n",
    "            batch_adj_loss += adj_loss.item()\n",
    "            batch_node_loss += node_loss.item()\n",
    "\n",
    "\n",
    "\n",
    "            global_step = epoch*len(batch_train_loader)+i\n",
    "            image_shape = [num_nodes, bbx_size]\n",
    "\n",
    "            writer.add_scalar(\"Loss/train/reconstruction_loss\", batch_loss/(i+1), global_step)\n",
    "            writer.add_scalar(\"Loss/train/kl_loss\", batch_kl_loss/(i+1), global_step)\n",
    "            writer.add_scalar(\"Loss/train/bbox_loss\", batch_bbox_loss/(i+1), global_step)\n",
    "            writer.add_scalar(\"Loss/train/adjacency_loss\", batch_adj_loss/(i+1), global_step)\n",
    "            writer.add_scalar(\"Loss/train/node_loss\", batch_node_loss/(i+1), global_step)\n",
    "            i+=1   \n",
    "        image = plot_bbx(np.reshape((node_data_true[:24,1:]*label_true[:24]).detach().to(\"cpu\").numpy(),\n",
    "                                    image_shape)).astype(float)/255\n",
    "        writer.add_image('train/images/input', image, global_step, dataformats='HWC')\n",
    "        image = plot_bbx((node_data_pred[0]*label_true[:24]).detach().to(\"cpu\").numpy()).astype(float)/255\n",
    "        writer.add_image('train/images/generated', image, global_step, dataformats='HWC')\n",
    "\n",
    "        reconstruction_loss_arr.append(batch_loss/(i+1))\n",
    "        kl_loss_arr.append(batch_kl_loss/(i+1))\n",
    "        bbox_loss_arr.append(batch_bbox_loss/(i+1))\n",
    "        adj_loss_arr.append(batch_adj_loss/(i+1))\n",
    "        node_loss_arr.append(batch_node_loss/(i+1))\n",
    "\n",
    "\n",
    "        print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, batch_loss/(i+1) ))\n",
    "\n",
    "        batch_loss = 0.0\n",
    "        batch_kl_loss = 0.0\n",
    "        batch_bbox_loss = 0.0\n",
    "        batch_adj_loss = 0.0\n",
    "        batch_node_loss = 0.0\n",
    "        images = []\n",
    "        vae.eval()\n",
    "        for i, val_data in enumerate(batch_val_loader, 0):\n",
    "            val_data.cuda()\n",
    "            node_data_true = val_data.x\n",
    "            label_true = node_data_true[:,:1]\n",
    "            class_true = val_data.y\n",
    "            adj_true = val_data.edge_index\n",
    "            batch = val_data.batch\n",
    "\n",
    "            kl_weight = klw[icoef]\n",
    "\n",
    "            output = vae(adj_true, node_data_true, label_true , class_true)\n",
    "            node_data_pred, label_pred, adj_pred, class_pred, z_mean, z_logvar = output\n",
    "\n",
    "            kl_loss = losses.kl_loss(z_mean, z_logvar)\n",
    "            adj_loss = losses.adj_loss(adj_pred, adj_true, batch, num_nodes)\n",
    "            bbox_loss = losses.bbox_loss(node_data_pred, node_data_true[:,1:])\n",
    "            node_loss = losses.node_loss(label_pred,label_true)\n",
    "            class_loss = losses.class_loss(class_pred, class_true)\n",
    "\n",
    "            recostruction_loss = kl_loss*kl_weight + (bbox_loss + node_loss + adj_loss + class_loss)*24*5\n",
    "\n",
    "            batch_loss += recostruction_loss.item()\n",
    "            batch_kl_loss += kl_loss.item()\n",
    "            batch_bbox_loss += bbox_loss.item()\n",
    "            batch_adj_loss += adj_loss.item()\n",
    "            batch_node_loss += node_loss.item()\n",
    "\n",
    "        image = plot_bbx(np.reshape((node_data_true[:24,1:]*label_true[:24]).detach().to(\"cpu\").numpy(),\n",
    "                                    image_shape)).astype(float)/255\n",
    "        writer.add_image('val/images/input', image, (epoch+1)*len(batch_train_loader), dataformats='HWC')\n",
    "        image = plot_bbx((node_data_pred[0]*label_true[:24]).detach().to(\"cpu\").numpy()).astype(float)/255\n",
    "        writer.add_image('val/images/generated', image, (epoch+1)*len(batch_train_loader), dataformats='HWC')\n",
    "\n",
    "        reconstruction_loss_val_arr.append(batch_loss/(i+1))\n",
    "        kl_loss_val_arr.append(batch_kl_loss/(i+1))\n",
    "        bbox_loss_val_arr.append(batch_bbox_loss/(i+1))\n",
    "        adj_loss_val_arr.append(batch_adj_loss/(i+1))\n",
    "        node_loss_val_arr.append(batch_node_loss/(i+1))\n",
    "\n",
    "        writer.add_scalar(\"Loss/val/reconstruction_loss\", batch_loss/(i+1), global_step)\n",
    "        writer.add_scalar(\"Loss/val/kl_loss\", batch_kl_loss/(i+1), global_step)\n",
    "        writer.add_scalar(\"Loss/val/kl_loss\", batch_kl_loss/(i+1), global_step)\n",
    "        writer.add_scalar(\"Loss/val/bbox_loss\", batch_bbox_loss/(i+1), global_step)\n",
    "        writer.add_scalar(\"Loss/val/adjacency_loss\", batch_adj_loss/(i+1), global_step)\n",
    "        writer.add_scalar(\"Loss/val/node_loss\", batch_node_loss/(i+1), global_step)\n",
    "\n",
    "        if kl_loss_arr[-1]>0.5 and abs(bbox_loss_arr[-1] - bbox_loss_val_arr[-1]) < 0.2:\n",
    "            icoef = icoef + 1  \n",
    "\n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "    print('Finished Training')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
