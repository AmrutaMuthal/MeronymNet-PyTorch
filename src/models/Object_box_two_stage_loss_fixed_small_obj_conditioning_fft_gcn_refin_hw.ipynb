{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cfd1439",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5671166e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_LAUNCH_BLOCKING=1\n"
     ]
    }
   ],
   "source": [
    "%set_env CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd5cb46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21434d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import collections\n",
    "import cv2\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e88c7fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "from torch_geometric.data import DataLoader\n",
    "import torch.utils.data as data_utils\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e46b5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from losses import BoxVAE_losses as loss\n",
    "from evaluation import metrics\n",
    "from utils import plot_utils\n",
    "from utils import data_utils as data_loading\n",
    "from components.DenseAutoencoder import DenseAutoencoder\n",
    "from components.DenseAutoencoder import Decoder\n",
    "from components.TwoStageAutoEncoder import TwoStageAutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d19349cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b59f5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mask_generation import masked_sketch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9cd8d4a6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "latent_dims = 64\n",
    "num_nodes = 16\n",
    "bbx_size = 4\n",
    "num_classes = 7\n",
    "label_shape = 1\n",
    "nb_epochs = 1000\n",
    "klw = loss.frange_cycle_linear(nb_epochs)\n",
    "learning_rate = 0.00015\n",
    "hidden1 = 32\n",
    "hidden2 = 16\n",
    "hidden3 = 128\n",
    "dense_hidden1=8\n",
    "dense_hidden2=4\n",
    "adaptive_margin = True\n",
    "fine_tune_box = False\n",
    "output_log = False\n",
    "area_encoding = False\n",
    "run_prefix = \"two_stage_small_obj_conditioning_fft_gcn_reccurent_refine_training_noise_hw\"\n",
    "variational=False\n",
    "coupling = True\n",
    "obj_bbx_conditioning = True\n",
    "use_fft_on_bbx = True\n",
    "use_gcn_in_decoder = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d7f3f906",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "batch_train_loader, batch_val_loader = data_loading.load_data(obj_data_postfix = '_obj_boundary',\n",
    "                                                              part_data_post_fix = '_scaled',\n",
    "                                                              file_postfix = '_combined',\n",
    "                                                              seed=345,\n",
    "                                                              batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "361b90d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73f4529b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e573aa63",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vae' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m vae\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vae' is not defined"
     ]
    }
   ],
   "source": [
    "del vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5d41ca05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "[1,   399] loss: 360.623\n",
      "200\n",
      "[2,   399] loss: 207.750\n",
      "200\n",
      "[3,   399] loss: 194.562\n",
      "200\n",
      "[4,   399] loss: 185.846\n",
      "200\n",
      "[5,   399] loss: 177.877\n",
      "200\n",
      "[6,   399] loss: 170.335\n",
      "200\n",
      "[7,   399] loss: 163.253\n",
      "200\n",
      "[8,   399] loss: 156.689\n",
      "200\n",
      "[9,   399] loss: 150.656\n",
      "200\n",
      "[10,   399] loss: 145.029\n",
      "200\n",
      "[11,   399] loss: 139.666\n",
      "200\n",
      "[12,   399] loss: 134.992\n",
      "200\n",
      "[13,   399] loss: 130.654\n",
      "200\n",
      "[14,   399] loss: 126.524\n",
      "200\n",
      "[15,   399] loss: 122.271\n",
      "200\n",
      "[16,   399] loss: 117.351\n",
      "200\n",
      "[17,   399] loss: 112.126\n",
      "200\n",
      "[18,   399] loss: 106.447\n",
      "200\n",
      "[19,   399] loss: 101.495\n",
      "200\n",
      "[20,   399] loss: 96.925\n",
      "200\n",
      "[21,   399] loss: 92.099\n",
      "200\n",
      "[22,   399] loss: 87.561\n",
      "200\n",
      "[23,   399] loss: 83.757\n",
      "200\n",
      "[24,   399] loss: 79.409\n",
      "200\n",
      "[25,   399] loss: 75.147\n",
      "200\n",
      "[26,   399] loss: 69.909\n",
      "200\n",
      "[27,   399] loss: 62.447\n",
      "200\n",
      "[28,   399] loss: 57.144\n",
      "200\n",
      "[29,   399] loss: 53.357\n",
      "200\n",
      "[30,   399] loss: 50.648\n",
      "200\n",
      "[31,   399] loss: 48.489\n",
      "200\n",
      "[32,   399] loss: 46.605\n",
      "200\n",
      "[33,   399] loss: 44.975\n",
      "200\n",
      "[34,   399] loss: 43.661\n",
      "200\n",
      "[35,   399] loss: 42.387\n",
      "200\n",
      "[36,   399] loss: 41.203\n",
      "200\n",
      "[37,   399] loss: 40.212\n",
      "200\n",
      "[38,   399] loss: 39.167\n",
      "200\n",
      "[39,   399] loss: 38.285\n",
      "200\n",
      "[40,   399] loss: 37.467\n",
      "200\n",
      "[41,   399] loss: 36.725\n",
      "200\n",
      "[42,   399] loss: 36.048\n",
      "200\n",
      "[43,   399] loss: 35.369\n",
      "200\n",
      "[44,   399] loss: 34.717\n",
      "200\n",
      "[45,   399] loss: 34.132\n",
      "200\n",
      "[46,   399] loss: 33.580\n",
      "200\n",
      "[47,   399] loss: 33.063\n",
      "200\n",
      "[48,   399] loss: 32.579\n",
      "200\n",
      "[49,   399] loss: 32.128\n",
      "200\n",
      "[50,   399] loss: 31.705\n",
      "200\n",
      "[51,   399] loss: 31.273\n",
      "200\n",
      "[52,   399] loss: 30.760\n",
      "200\n",
      "[53,   399] loss: 30.224\n",
      "200\n",
      "[54,   399] loss: 29.763\n",
      "200\n",
      "[55,   399] loss: 29.359\n",
      "200\n",
      "[56,   399] loss: 28.981\n",
      "200\n",
      "[57,   399] loss: 28.627\n",
      "200\n",
      "[58,   399] loss: 28.295\n",
      "200\n",
      "[59,   399] loss: 27.965\n",
      "200\n",
      "[60,   399] loss: 27.645\n",
      "200\n",
      "[61,   399] loss: 27.327\n",
      "200\n",
      "[62,   399] loss: 27.007\n",
      "200\n",
      "[63,   399] loss: 26.697\n",
      "200\n",
      "[64,   399] loss: 26.400\n",
      "200\n",
      "[65,   399] loss: 26.112\n",
      "200\n",
      "[66,   399] loss: 25.831\n",
      "200\n",
      "[67,   399] loss: 25.557\n",
      "200\n",
      "[68,   399] loss: 25.288\n",
      "200\n",
      "[69,   399] loss: 25.024\n",
      "200\n",
      "[70,   399] loss: 24.764\n",
      "200\n",
      "[71,   399] loss: 24.515\n",
      "200\n",
      "[72,   399] loss: 24.273\n",
      "200\n",
      "[73,   399] loss: 24.040\n",
      "200\n",
      "[74,   399] loss: 23.809\n",
      "200\n",
      "[75,   399] loss: 23.580\n",
      "200\n",
      "[76,   399] loss: 23.348\n",
      "200\n",
      "[77,   399] loss: 23.094\n",
      "200\n",
      "[78,   399] loss: 22.752\n",
      "200\n",
      "[79,   399] loss: 22.376\n",
      "200\n",
      "[80,   399] loss: 22.007\n",
      "200\n",
      "[81,   399] loss: 21.714\n",
      "200\n",
      "[82,   399] loss: 21.458\n",
      "200\n",
      "[83,   399] loss: 21.215\n",
      "200\n",
      "[84,   399] loss: 20.976\n",
      "200\n",
      "[85,   399] loss: 20.739\n",
      "200\n",
      "[86,   399] loss: 20.509\n",
      "200\n",
      "[87,   399] loss: 20.285\n",
      "200\n",
      "[88,   399] loss: 20.068\n",
      "200\n",
      "[89,   399] loss: 19.860\n",
      "200\n",
      "[90,   399] loss: 19.657\n",
      "200\n",
      "[91,   399] loss: 19.460\n",
      "200\n",
      "[92,   399] loss: 19.271\n",
      "200\n",
      "[93,   399] loss: 19.084\n",
      "200\n",
      "[94,   399] loss: 18.901\n",
      "200\n",
      "[95,   399] loss: 18.723\n",
      "200\n",
      "[96,   399] loss: 18.549\n",
      "200\n",
      "[97,   399] loss: 18.383\n",
      "200\n",
      "[98,   399] loss: 18.222\n",
      "200\n",
      "[99,   399] loss: 18.064\n",
      "200\n",
      "[100,   399] loss: 17.912\n",
      "200\n",
      "[101,   399] loss: 17.761\n",
      "200\n",
      "[102,   399] loss: 18.664\n",
      "200\n",
      "[103,   399] loss: 18.530\n",
      "200\n",
      "[104,   399] loss: 18.392\n",
      "200\n",
      "[105,   399] loss: 18.236\n",
      "200\n",
      "[106,   399] loss: 18.109\n",
      "200\n",
      "[107,   399] loss: 17.956\n",
      "200\n",
      "[108,   399] loss: 17.838\n",
      "200\n",
      "[109,   399] loss: 17.697\n",
      "200\n",
      "[110,   399] loss: 17.579\n",
      "200\n",
      "[111,   399] loss: 17.454\n",
      "200\n",
      "[112,   399] loss: 17.327\n",
      "200\n",
      "[113,   399] loss: 17.204\n",
      "200\n",
      "[114,   399] loss: 17.096\n",
      "200\n",
      "[115,   399] loss: 16.972\n",
      "200\n",
      "[116,   399] loss: 16.847\n",
      "200\n",
      "[117,   399] loss: 16.748\n",
      "200\n",
      "[118,   399] loss: 16.642\n",
      "200\n",
      "[119,   399] loss: 16.529\n",
      "200\n",
      "[120,   399] loss: 16.434\n",
      "200\n",
      "[121,   399] loss: 16.319\n",
      "200\n",
      "[122,   399] loss: 16.216\n",
      "200\n",
      "[123,   399] loss: 16.104\n",
      "200\n",
      "[124,   399] loss: 16.007\n",
      "200\n",
      "[125,   399] loss: 15.916\n",
      "200\n",
      "[126,   399] loss: 15.828\n",
      "200\n",
      "[127,   399] loss: 15.717\n",
      "200\n",
      "[128,   399] loss: 15.624\n",
      "200\n",
      "[129,   399] loss: 15.549\n",
      "200\n",
      "[130,   399] loss: 15.443\n",
      "200\n",
      "[131,   399] loss: 15.355\n",
      "200\n",
      "[132,   399] loss: 15.270\n",
      "200\n",
      "[133,   399] loss: 15.179\n",
      "200\n",
      "[134,   399] loss: 15.096\n",
      "200\n",
      "[135,   399] loss: 15.008\n",
      "200\n",
      "[136,   399] loss: 14.917\n",
      "200\n",
      "[137,   399] loss: 14.824\n",
      "200\n",
      "[138,   399] loss: 14.736\n",
      "200\n",
      "[139,   399] loss: 14.654\n",
      "200\n",
      "[140,   399] loss: 14.602\n",
      "200\n",
      "[141,   399] loss: 14.505\n",
      "200\n",
      "[142,   399] loss: 14.431\n",
      "200\n",
      "[143,   399] loss: 14.344\n",
      "200\n",
      "[144,   399] loss: 14.271\n",
      "200\n",
      "[145,   399] loss: 14.206\n",
      "200\n",
      "[146,   399] loss: 14.121\n",
      "200\n",
      "[147,   399] loss: 14.063\n",
      "200\n",
      "[148,   399] loss: 13.985\n",
      "200\n",
      "[149,   399] loss: 13.898\n",
      "200\n",
      "[150,   399] loss: 13.836\n",
      "200\n",
      "[151,   399] loss: 13.765\n",
      "200\n",
      "[152,   399] loss: 13.710\n",
      "200\n",
      "[153,   399] loss: 13.638\n",
      "200\n",
      "[154,   399] loss: 13.575\n",
      "200\n",
      "[155,   399] loss: 13.509\n",
      "200\n",
      "[156,   399] loss: 13.443\n",
      "200\n",
      "[157,   399] loss: 13.384\n",
      "200\n",
      "[158,   399] loss: 13.325\n",
      "200\n",
      "[159,   399] loss: 13.248\n",
      "200\n",
      "[160,   399] loss: 13.194\n",
      "200\n",
      "[161,   399] loss: 13.135\n",
      "200\n",
      "[162,   399] loss: 13.070\n",
      "200\n",
      "[163,   399] loss: 13.006\n",
      "200\n",
      "[164,   399] loss: 12.952\n",
      "200\n",
      "[165,   399] loss: 12.887\n",
      "200\n",
      "[166,   399] loss: 12.837\n",
      "200\n",
      "[167,   399] loss: 12.780\n",
      "200\n",
      "[168,   399] loss: 12.727\n",
      "200\n",
      "[169,   399] loss: 12.671\n",
      "200\n",
      "[170,   399] loss: 12.605\n",
      "200\n",
      "[171,   399] loss: 12.551\n",
      "200\n",
      "[172,   399] loss: 12.503\n",
      "200\n",
      "[173,   399] loss: 12.460\n",
      "200\n",
      "[174,   399] loss: 12.389\n",
      "200\n",
      "[175,   399] loss: 12.336\n",
      "200\n",
      "[176,   399] loss: 12.293\n",
      "200\n",
      "[177,   399] loss: 12.247\n",
      "200\n",
      "[178,   399] loss: 12.179\n",
      "200\n",
      "[179,   399] loss: 12.135\n",
      "200\n",
      "[180,   399] loss: 12.092\n",
      "200\n",
      "[181,   399] loss: 12.042\n",
      "200\n",
      "[182,   399] loss: 11.979\n",
      "200\n",
      "[183,   399] loss: 11.930\n",
      "200\n",
      "[184,   399] loss: 11.892\n",
      "200\n",
      "[185,   399] loss: 11.842\n",
      "200\n",
      "[186,   399] loss: 11.789\n",
      "200\n",
      "[187,   399] loss: 11.738\n",
      "200\n",
      "[188,   399] loss: 11.697\n",
      "200\n",
      "[189,   399] loss: 11.645\n",
      "200\n",
      "[190,   399] loss: 11.598\n",
      "200\n",
      "[191,   399] loss: 11.561\n",
      "200\n",
      "[192,   399] loss: 11.516\n",
      "200\n",
      "[193,   399] loss: 11.464\n",
      "200\n",
      "[194,   399] loss: 11.415\n",
      "200\n",
      "[195,   399] loss: 11.368\n",
      "200\n",
      "[196,   399] loss: 11.329\n",
      "200\n",
      "[197,   399] loss: 11.284\n",
      "200\n",
      "[198,   399] loss: 11.237\n",
      "200\n",
      "[199,   399] loss: 11.191\n",
      "200\n",
      "[200,   399] loss: 11.152\n",
      "200\n",
      "[201,   399] loss: 11.107\n",
      "200\n",
      "[202,   399] loss: 12.418\n",
      "200\n",
      "[203,   399] loss: 12.278\n",
      "200\n",
      "[204,   399] loss: 12.183\n",
      "200\n",
      "[205,   399] loss: 12.101\n",
      "200\n",
      "[206,   399] loss: 12.012\n",
      "200\n",
      "[207,   399] loss: 11.942\n",
      "200\n",
      "[208,   399] loss: 11.872\n",
      "200\n",
      "[209,   399] loss: 11.811\n",
      "200\n",
      "[210,   399] loss: 11.751\n",
      "200\n",
      "[211,   399] loss: 11.687\n",
      "200\n",
      "[212,   399] loss: 11.629\n",
      "200\n",
      "[213,   399] loss: 11.557\n",
      "200\n",
      "[214,   399] loss: 11.506\n",
      "200\n",
      "[215,   399] loss: 11.452\n",
      "200\n",
      "[216,   399] loss: 11.386\n",
      "200\n",
      "[217,   399] loss: 11.341\n",
      "200\n",
      "[218,   399] loss: 11.283\n",
      "200\n",
      "[219,   399] loss: 11.240\n",
      "200\n",
      "[220,   399] loss: 11.177\n",
      "200\n",
      "[221,   399] loss: 11.135\n",
      "200\n",
      "[222,   399] loss: 11.092\n",
      "200\n",
      "[223,   399] loss: 11.040\n",
      "200\n",
      "[224,   399] loss: 10.992\n",
      "200\n",
      "[225,   399] loss: 10.938\n",
      "200\n",
      "[226,   399] loss: 10.893\n",
      "200\n",
      "[227,   399] loss: 10.832\n",
      "200\n",
      "[228,   399] loss: 10.795\n",
      "200\n",
      "[229,   399] loss: 10.737\n",
      "200\n",
      "[230,   399] loss: 10.704\n",
      "200\n",
      "[231,   399] loss: 10.657\n",
      "200\n",
      "[232,   399] loss: 10.609\n",
      "200\n",
      "[233,   399] loss: 10.556\n",
      "200\n",
      "[234,   399] loss: 10.533\n",
      "200\n",
      "[235,   399] loss: 10.469\n",
      "200\n",
      "[236,   399] loss: 10.428\n",
      "200\n",
      "[237,   399] loss: 10.377\n",
      "200\n",
      "[238,   399] loss: 10.346\n",
      "200\n",
      "[239,   399] loss: 10.301\n",
      "200\n",
      "[240,   399] loss: 10.235\n",
      "200\n",
      "[241,   399] loss: 10.210\n",
      "200\n",
      "[242,   399] loss: 10.162\n",
      "200\n",
      "[243,   399] loss: 10.127\n",
      "200\n",
      "[244,   399] loss: 10.080\n",
      "200\n",
      "[245,   399] loss: 10.035\n",
      "200\n",
      "[246,   399] loss: 9.994\n",
      "200\n",
      "[247,   399] loss: 9.949\n",
      "200\n",
      "[248,   399] loss: 9.904\n",
      "200\n",
      "[249,   399] loss: 9.856\n",
      "200\n",
      "[250,   399] loss: 9.817\n",
      "200\n",
      "[251,   399] loss: 9.778\n",
      "200\n",
      "[252,   399] loss: 9.733\n",
      "200\n",
      "[253,   399] loss: 9.679\n",
      "200\n",
      "[254,   399] loss: 9.659\n",
      "200\n",
      "[255,   399] loss: 9.609\n",
      "200\n",
      "[256,   399] loss: 9.565\n",
      "200\n",
      "[257,   399] loss: 9.532\n",
      "200\n",
      "[258,   399] loss: 9.486\n",
      "200\n",
      "[259,   399] loss: 9.442\n",
      "200\n",
      "[260,   399] loss: 9.405\n",
      "200\n",
      "[261,   399] loss: 9.365\n",
      "200\n",
      "[262,   399] loss: 9.331\n",
      "200\n",
      "[263,   399] loss: 9.294\n",
      "200\n",
      "[264,   399] loss: 9.250\n",
      "200\n",
      "[265,   399] loss: 9.206\n",
      "200\n",
      "[266,   399] loss: 9.178\n",
      "200\n",
      "[267,   399] loss: 9.135\n",
      "200\n",
      "[268,   399] loss: 9.090\n",
      "200\n",
      "[269,   399] loss: 9.064\n",
      "200\n",
      "[270,   399] loss: 9.013\n",
      "200\n",
      "[271,   399] loss: 8.976\n",
      "200\n",
      "[272,   399] loss: 8.943\n",
      "200\n",
      "[273,   399] loss: 8.907\n",
      "200\n",
      "[274,   399] loss: 8.866\n",
      "200\n",
      "[275,   399] loss: 8.819\n",
      "200\n",
      "[276,   399] loss: 8.788\n",
      "200\n",
      "[277,   399] loss: 8.747\n",
      "200\n",
      "[278,   399] loss: 8.717\n",
      "200\n",
      "[279,   399] loss: 8.681\n",
      "200\n",
      "[280,   399] loss: 8.633\n",
      "200\n",
      "[281,   399] loss: 8.601\n",
      "200\n",
      "[282,   399] loss: 8.566\n",
      "200\n",
      "[283,   399] loss: 8.525\n",
      "200\n",
      "[284,   399] loss: 8.500\n",
      "200\n",
      "[285,   399] loss: 8.445\n",
      "200\n",
      "[286,   399] loss: 8.417\n",
      "200\n",
      "[287,   399] loss: 8.388\n",
      "200\n",
      "[288,   399] loss: 8.345\n",
      "200\n",
      "[289,   399] loss: 8.317\n",
      "200\n",
      "[290,   399] loss: 8.287\n",
      "200\n",
      "[291,   399] loss: 8.239\n",
      "200\n",
      "[292,   399] loss: 8.197\n",
      "200\n",
      "[293,   399] loss: 8.173\n",
      "200\n",
      "[294,   399] loss: 8.140\n",
      "200\n",
      "[295,   399] loss: 8.112\n",
      "200\n",
      "[296,   399] loss: 8.073\n",
      "200\n",
      "[297,   399] loss: 8.048\n",
      "200\n",
      "[298,   399] loss: 8.001\n",
      "200\n",
      "[299,   399] loss: 7.972\n",
      "200\n",
      "[300,   399] loss: 7.935\n",
      "200\n",
      "[301,   399] loss: 7.913\n",
      "200\n",
      "[302,   399] loss: 7.877\n",
      "200\n",
      "[303,   399] loss: 7.845\n",
      "200\n",
      "[304,   399] loss: 7.806\n",
      "200\n",
      "[305,   399] loss: 7.769\n",
      "200\n",
      "[306,   399] loss: 7.741\n",
      "200\n",
      "[307,   399] loss: 7.710\n",
      "200\n",
      "[308,   399] loss: 7.679\n",
      "200\n",
      "[309,   399] loss: 7.653\n",
      "200\n",
      "[310,   399] loss: 7.620\n",
      "200\n",
      "[311,   399] loss: 7.591\n",
      "200\n",
      "[312,   399] loss: 7.560\n",
      "200\n",
      "[313,   399] loss: 7.527\n",
      "200\n",
      "[314,   399] loss: 7.506\n",
      "200\n",
      "[315,   399] loss: 7.465\n",
      "200\n",
      "[316,   399] loss: 7.432\n",
      "200\n",
      "[317,   399] loss: 7.415\n",
      "200\n",
      "[318,   399] loss: 7.382\n",
      "200\n",
      "[319,   399] loss: 7.353\n",
      "200\n",
      "[320,   399] loss: 7.334\n",
      "200\n",
      "[321,   399] loss: 7.301\n",
      "200\n",
      "[322,   399] loss: 7.269\n",
      "200\n",
      "[323,   399] loss: 7.237\n",
      "200\n",
      "[324,   399] loss: 7.204\n",
      "200\n",
      "[325,   399] loss: 7.177\n",
      "200\n",
      "[326,   399] loss: 7.152\n",
      "200\n",
      "[327,   399] loss: 7.123\n",
      "200\n",
      "[328,   399] loss: 7.110\n",
      "200\n",
      "[329,   399] loss: 7.073\n",
      "200\n",
      "[330,   399] loss: 7.049\n",
      "200\n",
      "[331,   399] loss: 7.018\n",
      "200\n",
      "[332,   399] loss: 6.995\n",
      "200\n",
      "[333,   399] loss: 6.964\n",
      "200\n",
      "[334,   399] loss: 6.947\n",
      "200\n",
      "[335,   399] loss: 6.910\n",
      "200\n",
      "[336,   399] loss: 6.892\n",
      "200\n",
      "[337,   399] loss: 6.863\n",
      "200\n",
      "[338,   399] loss: 6.850\n",
      "200\n",
      "[339,   399] loss: 6.810\n",
      "200\n",
      "[340,   399] loss: 6.787\n",
      "200\n",
      "[341,   399] loss: 6.760\n",
      "200\n",
      "[342,   399] loss: 6.753\n",
      "200\n",
      "[343,   399] loss: 6.717\n",
      "200\n",
      "[344,   399] loss: 6.693\n",
      "200\n",
      "[345,   399] loss: 6.664\n",
      "200\n",
      "[346,   399] loss: 6.642\n",
      "200\n",
      "[347,   399] loss: 6.625\n",
      "200\n",
      "[348,   399] loss: 6.595\n",
      "200\n",
      "[349,   399] loss: 6.576\n",
      "200\n",
      "[350,   399] loss: 6.549\n",
      "200\n",
      "[351,   399] loss: 6.539\n",
      "200\n",
      "[352,   399] loss: 6.506\n",
      "200\n",
      "[353,   399] loss: 6.481\n",
      "200\n",
      "[354,   399] loss: 6.464\n",
      "200\n",
      "[355,   399] loss: 6.438\n",
      "200\n",
      "[356,   399] loss: 6.406\n",
      "200\n",
      "[357,   399] loss: 6.390\n",
      "200\n",
      "[358,   399] loss: 6.375\n",
      "200\n",
      "[359,   399] loss: 6.346\n",
      "200\n",
      "[360,   399] loss: 6.322\n",
      "200\n",
      "[361,   399] loss: 6.301\n",
      "200\n",
      "[362,   399] loss: 6.284\n",
      "200\n",
      "[363,   399] loss: 6.265\n",
      "200\n",
      "[364,   399] loss: 6.244\n",
      "200\n",
      "[365,   399] loss: 6.224\n",
      "200\n",
      "[366,   399] loss: 6.210\n",
      "200\n",
      "[367,   399] loss: 6.181\n",
      "200\n",
      "[368,   399] loss: 6.159\n",
      "200\n",
      "[369,   399] loss: 6.139\n",
      "200\n",
      "[370,   399] loss: 6.114\n",
      "200\n",
      "[371,   399] loss: 6.101\n",
      "200\n",
      "[372,   399] loss: 6.083\n",
      "200\n",
      "[373,   399] loss: 6.063\n",
      "200\n",
      "[374,   399] loss: 6.042\n",
      "200\n",
      "[375,   399] loss: 6.016\n",
      "200\n",
      "[376,   399] loss: 6.004\n",
      "200\n",
      "[377,   399] loss: 5.989\n",
      "200\n",
      "[378,   399] loss: 5.968\n",
      "200\n",
      "[379,   399] loss: 5.941\n",
      "200\n",
      "[380,   399] loss: 5.927\n",
      "200\n",
      "[381,   399] loss: 5.911\n",
      "200\n",
      "[382,   399] loss: 5.889\n",
      "200\n",
      "[383,   399] loss: 5.864\n",
      "200\n",
      "[384,   399] loss: 5.845\n",
      "200\n",
      "[385,   399] loss: 5.833\n",
      "200\n",
      "[386,   399] loss: 5.810\n",
      "200\n",
      "[387,   399] loss: 5.793\n",
      "200\n",
      "[388,   399] loss: 5.774\n",
      "200\n",
      "[389,   399] loss: 5.757\n",
      "200\n",
      "[390,   399] loss: 5.747\n",
      "200\n",
      "[391,   399] loss: 5.724\n",
      "200\n",
      "[392,   399] loss: 5.706\n",
      "200\n",
      "[393,   399] loss: 5.687\n",
      "200\n",
      "[394,   399] loss: 5.672\n",
      "200\n",
      "[395,   399] loss: 5.653\n",
      "200\n",
      "[396,   399] loss: 5.644\n",
      "200\n",
      "[397,   399] loss: 5.621\n",
      "200\n",
      "[398,   399] loss: 5.610\n",
      "200\n",
      "[399,   399] loss: 5.589\n",
      "200\n",
      "[400,   399] loss: 5.570\n",
      "200\n",
      "[401,   399] loss: 5.556\n",
      "200\n",
      "[402,   399] loss: 5.535\n",
      "200\n",
      "[403,   399] loss: 5.527\n",
      "200\n",
      "[404,   399] loss: 5.512\n",
      "200\n",
      "[405,   399] loss: 5.486\n",
      "200\n",
      "[406,   399] loss: 5.478\n",
      "200\n",
      "[407,   399] loss: 5.459\n",
      "200\n",
      "[408,   399] loss: 5.443\n",
      "200\n",
      "[409,   399] loss: 5.423\n",
      "200\n",
      "[410,   399] loss: 5.408\n",
      "200\n",
      "[411,   399] loss: 5.397\n",
      "200\n",
      "[412,   399] loss: 5.384\n",
      "200\n",
      "[413,   399] loss: 5.356\n",
      "200\n",
      "[414,   399] loss: 5.348\n",
      "200\n",
      "[415,   399] loss: 5.334\n",
      "200\n",
      "[416,   399] loss: 5.319\n",
      "200\n",
      "[417,   399] loss: 5.305\n",
      "200\n",
      "[418,   399] loss: 5.286\n",
      "200\n",
      "[419,   399] loss: 5.272\n",
      "200\n",
      "[420,   399] loss: 5.268\n",
      "200\n",
      "[421,   399] loss: 5.248\n",
      "200\n",
      "[422,   399] loss: 5.228\n",
      "200\n",
      "[423,   399] loss: 5.221\n",
      "200\n",
      "[424,   399] loss: 5.208\n",
      "200\n",
      "[425,   399] loss: 5.192\n",
      "200\n",
      "[426,   399] loss: 5.177\n",
      "200\n",
      "[427,   399] loss: 5.157\n",
      "200\n",
      "[428,   399] loss: 5.147\n",
      "200\n",
      "[429,   399] loss: 5.134\n",
      "200\n",
      "[430,   399] loss: 5.115\n",
      "200\n",
      "[431,   399] loss: 5.102\n",
      "200\n",
      "[432,   399] loss: 5.103\n",
      "200\n",
      "[433,   399] loss: 5.080\n",
      "200\n",
      "[434,   399] loss: 5.067\n",
      "200\n",
      "[435,   399] loss: 5.059\n",
      "200\n",
      "[436,   399] loss: 5.038\n",
      "200\n",
      "[437,   399] loss: 5.026\n",
      "200\n",
      "[438,   399] loss: 5.017\n",
      "200\n",
      "[439,   399] loss: 5.007\n",
      "200\n",
      "[440,   399] loss: 4.984\n",
      "200\n",
      "[441,   399] loss: 4.980\n",
      "200\n",
      "[442,   399] loss: 4.961\n",
      "200\n",
      "[443,   399] loss: 4.956\n",
      "200\n",
      "[444,   399] loss: 4.943\n",
      "200\n",
      "[445,   399] loss: 4.929\n",
      "200\n",
      "[446,   399] loss: 4.917\n",
      "200\n",
      "[447,   399] loss: 4.906\n",
      "200\n",
      "[448,   399] loss: 4.897\n",
      "200\n",
      "[449,   399] loss: 4.878\n",
      "200\n",
      "[450,   399] loss: 4.859\n",
      "200\n",
      "[451,   399] loss: 4.852\n",
      "200\n",
      "[452,   399] loss: 4.855\n",
      "200\n",
      "[453,   399] loss: 4.834\n",
      "200\n",
      "[454,   399] loss: 4.818\n",
      "200\n",
      "[455,   399] loss: 4.812\n",
      "200\n",
      "[456,   399] loss: 4.791\n",
      "200\n",
      "[457,   399] loss: 4.792\n",
      "200\n",
      "[458,   399] loss: 4.775\n",
      "200\n",
      "[459,   399] loss: 4.764\n",
      "200\n",
      "[460,   399] loss: 4.752\n",
      "200\n",
      "[461,   399] loss: 4.739\n",
      "200\n",
      "[462,   399] loss: 4.729\n",
      "200\n",
      "[463,   399] loss: 4.726\n",
      "200\n",
      "[464,   399] loss: 4.716\n",
      "200\n",
      "[465,   399] loss: 4.695\n",
      "200\n",
      "[466,   399] loss: 4.691\n",
      "200\n",
      "[467,   399] loss: 4.678\n",
      "200\n",
      "[468,   399] loss: 4.661\n",
      "200\n",
      "[469,   399] loss: 4.653\n",
      "200\n",
      "[470,   399] loss: 4.649\n",
      "200\n",
      "[471,   399] loss: 4.637\n",
      "200\n",
      "[472,   399] loss: 4.617\n",
      "200\n",
      "[473,   399] loss: 4.611\n",
      "200\n",
      "[474,   399] loss: 4.601\n",
      "200\n",
      "[475,   399] loss: 4.593\n",
      "200\n",
      "[476,   399] loss: 4.587\n",
      "200\n",
      "[477,   399] loss: 4.571\n",
      "200\n",
      "[478,   399] loss: 4.564\n",
      "200\n",
      "[479,   399] loss: 4.557\n",
      "200\n",
      "[480,   399] loss: 4.548\n",
      "200\n",
      "[481,   399] loss: 4.535\n",
      "200\n",
      "[482,   399] loss: 4.518\n",
      "200\n",
      "[483,   399] loss: 4.511\n",
      "200\n",
      "[484,   399] loss: 4.509\n",
      "200\n",
      "[485,   399] loss: 4.492\n",
      "200\n",
      "[486,   399] loss: 4.484\n",
      "200\n",
      "[487,   399] loss: 4.482\n",
      "200\n",
      "[488,   399] loss: 4.465\n",
      "200\n",
      "[489,   399] loss: 4.460\n",
      "200\n",
      "[490,   399] loss: 4.440\n",
      "200\n",
      "[491,   399] loss: 4.438\n",
      "200\n",
      "[492,   399] loss: 4.435\n",
      "200\n",
      "[493,   399] loss: 4.414\n",
      "200\n",
      "[494,   399] loss: 4.403\n",
      "200\n",
      "[495,   399] loss: 4.397\n",
      "200\n",
      "[496,   399] loss: 4.383\n",
      "200\n",
      "[497,   399] loss: 4.388\n",
      "200\n",
      "[498,   399] loss: 4.366\n",
      "200\n",
      "[499,   399] loss: 4.363\n",
      "200\n",
      "[500,   399] loss: 4.344\n",
      "200\n",
      "[501,   399] loss: 4.343\n",
      "200\n",
      "[502,   399] loss: 4.332\n",
      "200\n",
      "[503,   399] loss: 4.326\n",
      "200\n",
      "[504,   399] loss: 4.315\n",
      "200\n",
      "[505,   399] loss: 4.307\n",
      "200\n",
      "[506,   399] loss: 4.295\n",
      "200\n",
      "[507,   399] loss: 4.290\n",
      "200\n",
      "[508,   399] loss: 4.275\n",
      "200\n",
      "[509,   399] loss: 4.269\n",
      "200\n",
      "[510,   399] loss: 4.261\n",
      "200\n",
      "[511,   399] loss: 4.243\n",
      "200\n",
      "[512,   399] loss: 4.237\n",
      "200\n",
      "[513,   399] loss: 4.234\n",
      "200\n",
      "[514,   399] loss: 4.222\n",
      "200\n",
      "[515,   399] loss: 4.224\n",
      "200\n",
      "[516,   399] loss: 4.206\n",
      "200\n",
      "[517,   399] loss: 4.204\n",
      "200\n",
      "[518,   399] loss: 4.196\n",
      "200\n",
      "[519,   399] loss: 4.187\n",
      "200\n",
      "[520,   399] loss: 4.183\n",
      "200\n",
      "[521,   399] loss: 4.171\n",
      "200\n",
      "[522,   399] loss: 4.164\n",
      "200\n",
      "[523,   399] loss: 4.148\n",
      "200\n",
      "[524,   399] loss: 4.143\n",
      "200\n",
      "[525,   399] loss: 4.144\n",
      "200\n",
      "[526,   399] loss: 4.128\n",
      "200\n",
      "[527,   399] loss: 4.117\n",
      "200\n",
      "[528,   399] loss: 4.114\n",
      "200\n",
      "[529,   399] loss: 4.109\n",
      "200\n",
      "[530,   399] loss: 4.092\n",
      "200\n",
      "[531,   399] loss: 4.095\n",
      "200\n",
      "[532,   399] loss: 4.086\n",
      "200\n",
      "[533,   399] loss: 4.073\n",
      "200\n",
      "[534,   399] loss: 4.067\n",
      "200\n",
      "[535,   399] loss: 4.058\n",
      "200\n",
      "[536,   399] loss: 4.048\n",
      "200\n",
      "[537,   399] loss: 4.044\n",
      "200\n",
      "[538,   399] loss: 4.038\n",
      "200\n",
      "[539,   399] loss: 4.033\n",
      "200\n",
      "[540,   399] loss: 4.023\n",
      "200\n",
      "[541,   399] loss: 4.009\n",
      "200\n",
      "[542,   399] loss: 4.002\n",
      "200\n",
      "[543,   399] loss: 3.995\n",
      "200\n",
      "[544,   399] loss: 3.991\n",
      "200\n",
      "[545,   399] loss: 3.983\n",
      "200\n",
      "[546,   399] loss: 3.983\n",
      "200\n",
      "[547,   399] loss: 3.968\n",
      "200\n",
      "[548,   399] loss: 3.970\n",
      "200\n",
      "[549,   399] loss: 3.955\n",
      "200\n",
      "[550,   399] loss: 3.945\n",
      "200\n",
      "[551,   399] loss: 3.944\n",
      "200\n",
      "[552,   399] loss: 3.940\n",
      "200\n",
      "[553,   399] loss: 3.921\n",
      "200\n",
      "[554,   399] loss: 3.920\n",
      "200\n",
      "[555,   399] loss: 3.912\n",
      "200\n",
      "[556,   399] loss: 3.904\n",
      "200\n",
      "[557,   399] loss: 3.902\n",
      "200\n",
      "[558,   399] loss: 3.893\n",
      "200\n",
      "[559,   399] loss: 3.882\n",
      "200\n",
      "[560,   399] loss: 3.882\n",
      "200\n",
      "[561,   399] loss: 3.874\n",
      "200\n",
      "[562,   399] loss: 3.865\n",
      "200\n",
      "[563,   399] loss: 3.862\n",
      "200\n",
      "[564,   399] loss: 3.851\n",
      "200\n",
      "[565,   399] loss: 3.848\n",
      "200\n",
      "[566,   399] loss: 3.840\n",
      "200\n",
      "[567,   399] loss: 3.831\n",
      "200\n",
      "[568,   399] loss: 3.827\n",
      "200\n",
      "[569,   399] loss: 3.812\n",
      "200\n",
      "[570,   399] loss: 3.810\n",
      "200\n",
      "[571,   399] loss: 3.806\n",
      "200\n",
      "[572,   399] loss: 3.803\n",
      "200\n",
      "[573,   399] loss: 3.804\n",
      "200\n",
      "[574,   399] loss: 3.790\n",
      "200\n",
      "[575,   399] loss: 3.773\n",
      "200\n",
      "[576,   399] loss: 3.775\n",
      "200\n",
      "[577,   399] loss: 3.767\n",
      "200\n",
      "[578,   399] loss: 3.761\n",
      "200\n",
      "[579,   399] loss: 3.754\n",
      "200\n",
      "[580,   399] loss: 3.748\n",
      "200\n",
      "[581,   399] loss: 3.742\n",
      "200\n",
      "[582,   399] loss: 3.742\n",
      "200\n",
      "[583,   399] loss: 3.727\n",
      "200\n",
      "[584,   399] loss: 3.730\n",
      "200\n",
      "[585,   399] loss: 3.718\n",
      "200\n",
      "[586,   399] loss: 3.707\n",
      "200\n",
      "[587,   399] loss: 3.705\n",
      "200\n",
      "[588,   399] loss: 3.694\n",
      "200\n",
      "[589,   399] loss: 3.697\n",
      "200\n",
      "[590,   399] loss: 3.691\n",
      "200\n",
      "[591,   399] loss: 3.683\n",
      "200\n",
      "[592,   399] loss: 3.675\n",
      "200\n",
      "[593,   399] loss: 3.665\n",
      "200\n",
      "[594,   399] loss: 3.669\n",
      "200\n",
      "[595,   399] loss: 3.657\n",
      "200\n",
      "[596,   399] loss: 3.649\n",
      "200\n",
      "[597,   399] loss: 3.637\n",
      "200\n",
      "[598,   399] loss: 3.644\n",
      "200\n",
      "[599,   399] loss: 3.626\n",
      "200\n",
      "[600,   399] loss: 3.629\n",
      "200\n",
      "[601,   399] loss: 3.621\n",
      "200\n",
      "[602,   399] loss: 3.618\n",
      "200\n",
      "[603,   399] loss: 3.614\n",
      "200\n",
      "[604,   399] loss: 3.607\n",
      "200\n",
      "[605,   399] loss: 3.600\n",
      "200\n",
      "[606,   399] loss: 3.591\n",
      "200\n",
      "[607,   399] loss: 3.583\n",
      "200\n",
      "[608,   399] loss: 3.584\n",
      "200\n",
      "[609,   399] loss: 3.570\n",
      "200\n",
      "[610,   399] loss: 3.571\n",
      "200\n",
      "[611,   399] loss: 3.569\n",
      "200\n",
      "[612,   399] loss: 3.562\n",
      "200\n",
      "[613,   399] loss: 3.557\n",
      "200\n",
      "[614,   399] loss: 3.542\n",
      "200\n",
      "[615,   399] loss: 3.538\n",
      "200\n",
      "[616,   399] loss: 3.539\n",
      "200\n",
      "[617,   399] loss: 3.533\n",
      "200\n",
      "[618,   399] loss: 3.525\n",
      "200\n",
      "[619,   399] loss: 3.524\n",
      "200\n",
      "[620,   399] loss: 3.517\n",
      "200\n",
      "[621,   399] loss: 3.514\n",
      "200\n",
      "[622,   399] loss: 3.503\n",
      "200\n",
      "[623,   399] loss: 3.497\n",
      "200\n",
      "[624,   399] loss: 3.498\n",
      "200\n",
      "[625,   399] loss: 3.488\n",
      "200\n",
      "[626,   399] loss: 3.487\n",
      "200\n",
      "[627,   399] loss: 3.476\n",
      "200\n",
      "[628,   399] loss: 3.479\n",
      "200\n",
      "[629,   399] loss: 3.465\n",
      "200\n",
      "[630,   399] loss: 3.461\n",
      "200\n",
      "[631,   399] loss: 3.460\n",
      "200\n",
      "[632,   399] loss: 3.458\n",
      "200\n",
      "[633,   399] loss: 3.450\n",
      "200\n",
      "[634,   399] loss: 3.435\n",
      "200\n",
      "[635,   399] loss: 3.438\n",
      "200\n",
      "[636,   399] loss: 3.432\n",
      "200\n",
      "[637,   399] loss: 3.426\n",
      "200\n",
      "[638,   399] loss: 3.415\n",
      "200\n",
      "[639,   399] loss: 3.418\n",
      "200\n",
      "[640,   399] loss: 3.416\n",
      "200\n",
      "[641,   399] loss: 3.409\n",
      "200\n",
      "[642,   399] loss: 3.402\n",
      "200\n",
      "[643,   399] loss: 3.395\n",
      "200\n",
      "[644,   399] loss: 3.393\n",
      "200\n",
      "[645,   399] loss: 3.400\n",
      "200\n",
      "[646,   399] loss: 3.383\n",
      "200\n",
      "[647,   399] loss: 3.376\n",
      "200\n",
      "[648,   399] loss: 3.368\n",
      "200\n",
      "[649,   399] loss: 3.369\n",
      "200\n",
      "[650,   399] loss: 3.357\n",
      "200\n",
      "[651,   399] loss: 3.356\n",
      "200\n",
      "[652,   399] loss: 3.357\n",
      "200\n",
      "[653,   399] loss: 3.351\n",
      "200\n",
      "[654,   399] loss: 3.347\n",
      "200\n",
      "[655,   399] loss: 3.338\n",
      "200\n",
      "[656,   399] loss: 3.332\n",
      "200\n",
      "[657,   399] loss: 3.329\n",
      "200\n",
      "[658,   399] loss: 3.321\n",
      "200\n",
      "[659,   399] loss: 3.317\n",
      "200\n",
      "[660,   399] loss: 3.316\n",
      "200\n",
      "[661,   399] loss: 3.307\n",
      "200\n",
      "[662,   399] loss: 3.303\n",
      "200\n",
      "[663,   399] loss: 3.298\n",
      "200\n",
      "[664,   399] loss: 3.300\n",
      "200\n",
      "[665,   399] loss: 3.290\n",
      "200\n",
      "[666,   399] loss: 3.289\n",
      "200\n",
      "[667,   399] loss: 3.284\n",
      "200\n",
      "[668,   399] loss: 3.279\n",
      "200\n",
      "[669,   399] loss: 3.274\n",
      "200\n",
      "[670,   399] loss: 3.270\n",
      "200\n",
      "[671,   399] loss: 3.264\n",
      "200\n",
      "[672,   399] loss: 3.258\n",
      "200\n",
      "[673,   399] loss: 3.253\n",
      "200\n",
      "[674,   399] loss: 3.251\n",
      "200\n",
      "[675,   399] loss: 3.243\n",
      "200\n",
      "[676,   399] loss: 3.247\n",
      "200\n",
      "[677,   399] loss: 3.235\n",
      "200\n",
      "[678,   399] loss: 3.232\n",
      "200\n",
      "[679,   399] loss: 3.226\n",
      "200\n",
      "[680,   399] loss: 3.224\n",
      "200\n",
      "[681,   399] loss: 3.228\n",
      "200\n",
      "[682,   399] loss: 3.215\n",
      "200\n",
      "[683,   399] loss: 3.206\n",
      "200\n",
      "[684,   399] loss: 3.211\n",
      "200\n",
      "[685,   399] loss: 3.198\n",
      "200\n",
      "[686,   399] loss: 3.196\n",
      "200\n",
      "[687,   399] loss: 3.196\n",
      "200\n",
      "[688,   399] loss: 3.188\n",
      "200\n",
      "[689,   399] loss: 3.188\n",
      "200\n",
      "[690,   399] loss: 3.180\n",
      "200\n",
      "[691,   399] loss: 3.180\n",
      "200\n",
      "[692,   399] loss: 3.169\n",
      "200\n",
      "[693,   399] loss: 3.164\n",
      "200\n",
      "[694,   399] loss: 3.167\n",
      "200\n",
      "[695,   399] loss: 3.154\n",
      "200\n",
      "[696,   399] loss: 3.153\n",
      "200\n",
      "[697,   399] loss: 3.149\n",
      "200\n",
      "[698,   399] loss: 3.149\n",
      "200\n",
      "[699,   399] loss: 3.149\n",
      "200\n",
      "[700,   399] loss: 3.136\n",
      "200\n",
      "[701,   399] loss: 3.130\n",
      "200\n",
      "[702,   399] loss: 3.127\n",
      "200\n",
      "[703,   399] loss: 3.130\n",
      "200\n",
      "[704,   399] loss: 3.116\n",
      "200\n",
      "[705,   399] loss: 3.117\n",
      "200\n",
      "[706,   399] loss: 3.110\n",
      "200\n",
      "[707,   399] loss: 3.111\n",
      "200\n",
      "[708,   399] loss: 3.102\n",
      "200\n",
      "[709,   399] loss: 3.096\n",
      "200\n",
      "[710,   399] loss: 3.101\n",
      "200\n",
      "[711,   399] loss: 3.097\n",
      "200\n",
      "[712,   399] loss: 3.088\n",
      "200\n",
      "[713,   399] loss: 3.080\n",
      "200\n",
      "[714,   399] loss: 3.080\n",
      "200\n",
      "[715,   399] loss: 3.081\n",
      "200\n",
      "[716,   399] loss: 3.073\n",
      "200\n",
      "[717,   399] loss: 3.067\n",
      "200\n",
      "[718,   399] loss: 3.067\n",
      "200\n",
      "[719,   399] loss: 3.060\n",
      "200\n",
      "[720,   399] loss: 3.058\n",
      "200\n",
      "[721,   399] loss: 3.058\n",
      "200\n",
      "[722,   399] loss: 3.050\n",
      "200\n",
      "[723,   399] loss: 3.050\n",
      "200\n",
      "[724,   399] loss: 3.038\n",
      "200\n",
      "[725,   399] loss: 3.044\n",
      "200\n",
      "[726,   399] loss: 3.031\n",
      "200\n",
      "[727,   399] loss: 3.031\n",
      "200\n",
      "[728,   399] loss: 3.028\n",
      "200\n",
      "[729,   399] loss: 3.017\n",
      "200\n",
      "[730,   399] loss: 3.024\n",
      "200\n",
      "[731,   399] loss: 3.013\n",
      "200\n",
      "[732,   399] loss: 3.018\n",
      "200\n",
      "[733,   399] loss: 3.010\n",
      "200\n",
      "[734,   399] loss: 3.005\n",
      "200\n",
      "[735,   399] loss: 2.999\n",
      "200\n",
      "[736,   399] loss: 2.997\n",
      "200\n",
      "[737,   399] loss: 2.992\n",
      "200\n",
      "[738,   399] loss: 2.984\n",
      "200\n",
      "[739,   399] loss: 2.985\n",
      "200\n",
      "[740,   399] loss: 2.981\n",
      "200\n",
      "[741,   399] loss: 2.973\n",
      "200\n",
      "[742,   399] loss: 2.972\n",
      "200\n",
      "[743,   399] loss: 2.965\n",
      "200\n",
      "[744,   399] loss: 2.973\n",
      "200\n",
      "[745,   399] loss: 2.961\n",
      "200\n",
      "[746,   399] loss: 2.958\n",
      "200\n",
      "[747,   399] loss: 2.956\n",
      "200\n",
      "[748,   399] loss: 2.955\n",
      "200\n",
      "[749,   399] loss: 2.948\n",
      "200\n",
      "[750,   399] loss: 2.941\n",
      "200\n",
      "[751,   399] loss: 2.941\n",
      "200\n",
      "[752,   399] loss: 2.940\n",
      "200\n",
      "[753,   399] loss: 2.934\n",
      "200\n",
      "[754,   399] loss: 2.929\n",
      "200\n",
      "[755,   399] loss: 2.925\n",
      "200\n",
      "[756,   399] loss: 2.925\n",
      "200\n",
      "[757,   399] loss: 2.917\n",
      "200\n",
      "[758,   399] loss: 2.920\n",
      "200\n",
      "[759,   399] loss: 2.915\n",
      "200\n",
      "[760,   399] loss: 2.910\n",
      "200\n",
      "[761,   399] loss: 2.903\n",
      "200\n",
      "[762,   399] loss: 2.903\n",
      "200\n",
      "[763,   399] loss: 2.897\n",
      "200\n",
      "[764,   399] loss: 2.890\n",
      "200\n",
      "[765,   399] loss: 2.895\n",
      "200\n",
      "[766,   399] loss: 2.893\n",
      "200\n",
      "[767,   399] loss: 2.881\n",
      "200\n",
      "[768,   399] loss: 2.882\n",
      "200\n",
      "[769,   399] loss: 2.878\n",
      "200\n",
      "[770,   399] loss: 2.876\n",
      "200\n",
      "[771,   399] loss: 2.872\n",
      "200\n",
      "[772,   399] loss: 2.863\n",
      "200\n",
      "[773,   399] loss: 2.860\n",
      "200\n",
      "[774,   399] loss: 2.859\n",
      "200\n",
      "[775,   399] loss: 2.858\n",
      "200\n",
      "[776,   399] loss: 2.859\n",
      "200\n",
      "[777,   399] loss: 2.851\n",
      "200\n",
      "[778,   399] loss: 2.844\n",
      "200\n",
      "[779,   399] loss: 2.842\n",
      "200\n",
      "[780,   399] loss: 2.843\n",
      "200\n",
      "[781,   399] loss: 2.834\n",
      "200\n",
      "[782,   399] loss: 2.835\n",
      "200\n",
      "[783,   399] loss: 2.829\n",
      "200\n",
      "[784,   399] loss: 2.828\n",
      "200\n",
      "[785,   399] loss: 2.823\n",
      "200\n",
      "[786,   399] loss: 2.820\n",
      "200\n",
      "[787,   399] loss: 2.820\n",
      "200\n",
      "[788,   399] loss: 2.815\n",
      "200\n",
      "[789,   399] loss: 2.808\n",
      "200\n",
      "[790,   399] loss: 2.808\n",
      "200\n",
      "[791,   399] loss: 2.805\n",
      "200\n",
      "[792,   399] loss: 2.797\n",
      "200\n",
      "[793,   399] loss: 2.790\n",
      "200\n",
      "[794,   399] loss: 2.793\n",
      "200\n",
      "[795,   399] loss: 2.792\n",
      "200\n",
      "[796,   399] loss: 2.791\n",
      "200\n",
      "[797,   399] loss: 2.785\n",
      "200\n",
      "[798,   399] loss: 2.778\n",
      "200\n",
      "[799,   399] loss: 2.775\n",
      "200\n",
      "[800,   399] loss: 2.776\n",
      "200\n",
      "[801,   399] loss: 2.772\n",
      "200\n",
      "[802,   399] loss: 2.770\n",
      "200\n",
      "[803,   399] loss: 2.768\n",
      "200\n",
      "[804,   399] loss: 2.761\n",
      "200\n",
      "[805,   399] loss: 2.761\n",
      "200\n",
      "[806,   399] loss: 2.762\n",
      "200\n",
      "[807,   399] loss: 2.751\n",
      "200\n",
      "[808,   399] loss: 2.749\n",
      "200\n",
      "[809,   399] loss: 2.748\n",
      "200\n",
      "[810,   399] loss: 2.744\n",
      "200\n",
      "[811,   399] loss: 2.740\n",
      "200\n",
      "[812,   399] loss: 2.740\n",
      "200\n",
      "[813,   399] loss: 2.732\n",
      "200\n",
      "[814,   399] loss: 2.728\n",
      "200\n",
      "[815,   399] loss: 2.728\n",
      "200\n",
      "[816,   399] loss: 2.725\n",
      "200\n",
      "[817,   399] loss: 2.722\n",
      "200\n",
      "[818,   399] loss: 2.718\n",
      "200\n",
      "[819,   399] loss: 2.712\n",
      "200\n",
      "[820,   399] loss: 2.713\n",
      "200\n",
      "[821,   399] loss: 2.715\n",
      "200\n",
      "[822,   399] loss: 2.706\n",
      "200\n",
      "[823,   399] loss: 2.707\n",
      "200\n",
      "[824,   399] loss: 2.703\n",
      "200\n",
      "[825,   399] loss: 2.697\n",
      "200\n",
      "[826,   399] loss: 2.693\n",
      "200\n",
      "[827,   399] loss: 2.693\n",
      "200\n",
      "[828,   399] loss: 2.688\n",
      "200\n",
      "[829,   399] loss: 2.684\n",
      "200\n",
      "[830,   399] loss: 2.685\n",
      "200\n",
      "[831,   399] loss: 2.680\n",
      "200\n",
      "[832,   399] loss: 2.674\n",
      "200\n",
      "[833,   399] loss: 2.676\n",
      "200\n",
      "[834,   399] loss: 2.667\n",
      "200\n",
      "[835,   399] loss: 2.666\n",
      "200\n",
      "[836,   399] loss: 2.666\n",
      "200\n",
      "[837,   399] loss: 2.660\n",
      "200\n",
      "[838,   399] loss: 2.658\n",
      "200\n",
      "[839,   399] loss: 2.655\n",
      "200\n",
      "[840,   399] loss: 2.651\n",
      "200\n",
      "[841,   399] loss: 2.649\n",
      "200\n",
      "[842,   399] loss: 2.648\n",
      "200\n",
      "[843,   399] loss: 2.641\n",
      "200\n",
      "[844,   399] loss: 2.640\n",
      "200\n",
      "[845,   399] loss: 2.642\n",
      "200\n",
      "[846,   399] loss: 2.638\n",
      "200\n",
      "[847,   399] loss: 2.629\n",
      "200\n",
      "[848,   399] loss: 2.627\n",
      "200\n",
      "[849,   399] loss: 2.625\n",
      "200\n",
      "[850,   399] loss: 2.623\n",
      "200\n",
      "[851,   399] loss: 2.622\n",
      "200\n",
      "[852,   399] loss: 2.619\n",
      "200\n",
      "[853,   399] loss: 2.615\n",
      "200\n",
      "[854,   399] loss: 2.616\n",
      "200\n",
      "[855,   399] loss: 2.609\n",
      "200\n",
      "[856,   399] loss: 2.610\n",
      "200\n",
      "[857,   399] loss: 2.603\n",
      "200\n",
      "[858,   399] loss: 2.595\n",
      "200\n",
      "[859,   399] loss: 2.597\n",
      "200\n",
      "[860,   399] loss: 2.596\n",
      "200\n",
      "[861,   399] loss: 2.590\n",
      "200\n",
      "[862,   399] loss: 2.591\n",
      "200\n",
      "[863,   399] loss: 2.592\n",
      "200\n",
      "[864,   399] loss: 2.580\n",
      "200\n",
      "[865,   399] loss: 2.579\n",
      "200\n",
      "[866,   399] loss: 2.578\n",
      "200\n",
      "[867,   399] loss: 2.574\n",
      "200\n",
      "[868,   399] loss: 2.576\n",
      "200\n",
      "[869,   399] loss: 2.568\n",
      "200\n",
      "[870,   399] loss: 2.565\n",
      "200\n",
      "[871,   399] loss: 2.563\n",
      "200\n",
      "[872,   399] loss: 2.560\n",
      "200\n",
      "[873,   399] loss: 2.559\n",
      "200\n",
      "[874,   399] loss: 2.554\n",
      "200\n",
      "[875,   399] loss: 2.552\n",
      "200\n",
      "[876,   399] loss: 2.550\n",
      "200\n",
      "[877,   399] loss: 2.547\n",
      "200\n",
      "[878,   399] loss: 2.548\n",
      "200\n",
      "[879,   399] loss: 2.543\n",
      "200\n",
      "[880,   399] loss: 2.537\n",
      "200\n",
      "[881,   399] loss: 2.537\n",
      "200\n",
      "[882,   399] loss: 2.535\n",
      "200\n",
      "[883,   399] loss: 2.531\n",
      "200\n",
      "[884,   399] loss: 2.529\n",
      "200\n",
      "[885,   399] loss: 2.527\n",
      "200\n",
      "[886,   399] loss: 2.525\n",
      "200\n",
      "[887,   399] loss: 2.519\n",
      "200\n",
      "[888,   399] loss: 2.518\n",
      "200\n",
      "[889,   399] loss: 2.515\n",
      "200\n",
      "[890,   399] loss: 2.509\n",
      "200\n",
      "[891,   399] loss: 2.507\n",
      "200\n",
      "[892,   399] loss: 2.507\n",
      "200\n",
      "[893,   399] loss: 2.508\n",
      "200\n",
      "[894,   399] loss: 2.498\n",
      "200\n",
      "[895,   399] loss: 2.501\n",
      "200\n",
      "[896,   399] loss: 2.497\n",
      "200\n",
      "[897,   399] loss: 2.494\n",
      "200\n",
      "[898,   399] loss: 2.492\n",
      "200\n",
      "[899,   399] loss: 2.489\n",
      "200\n",
      "[900,   399] loss: 2.484\n",
      "200\n",
      "[901,   399] loss: 2.481\n",
      "200\n",
      "[902,   399] loss: 2.480\n",
      "200\n",
      "[903,   399] loss: 2.479\n",
      "200\n",
      "[904,   399] loss: 2.475\n",
      "200\n",
      "[905,   399] loss: 2.476\n",
      "200\n",
      "[906,   399] loss: 2.468\n",
      "200\n",
      "[907,   399] loss: 2.465\n",
      "200\n",
      "[908,   399] loss: 2.464\n",
      "200\n",
      "[909,   399] loss: 2.461\n",
      "200\n",
      "[910,   399] loss: 2.460\n",
      "200\n",
      "[911,   399] loss: 2.458\n",
      "200\n",
      "[912,   399] loss: 2.458\n",
      "200\n",
      "[913,   399] loss: 2.455\n",
      "200\n",
      "[914,   399] loss: 2.451\n",
      "200\n",
      "[915,   399] loss: 2.450\n",
      "200\n",
      "[916,   399] loss: 2.443\n",
      "200\n",
      "[917,   399] loss: 2.442\n",
      "200\n",
      "[918,   399] loss: 2.440\n",
      "200\n",
      "[919,   399] loss: 2.437\n",
      "200\n",
      "[920,   399] loss: 2.436\n",
      "200\n",
      "[921,   399] loss: 2.431\n",
      "200\n",
      "[922,   399] loss: 2.433\n",
      "200\n",
      "[923,   399] loss: 2.431\n",
      "200\n",
      "[924,   399] loss: 2.426\n",
      "200\n",
      "[925,   399] loss: 2.422\n",
      "200\n",
      "[926,   399] loss: 2.422\n",
      "200\n",
      "[927,   399] loss: 2.417\n",
      "200\n",
      "[928,   399] loss: 2.415\n",
      "200\n",
      "[929,   399] loss: 2.413\n",
      "200\n",
      "[930,   399] loss: 2.410\n",
      "200\n",
      "[931,   399] loss: 2.406\n",
      "200\n",
      "[932,   399] loss: 2.404\n",
      "200\n",
      "[933,   399] loss: 2.406\n",
      "200\n",
      "[934,   399] loss: 2.400\n",
      "200\n",
      "[935,   399] loss: 2.399\n",
      "200\n",
      "[936,   399] loss: 2.393\n",
      "200\n",
      "[937,   399] loss: 2.393\n",
      "200\n",
      "[938,   399] loss: 2.390\n",
      "200\n",
      "[939,   399] loss: 2.388\n",
      "200\n",
      "[940,   399] loss: 2.387\n",
      "200\n",
      "[941,   399] loss: 2.384\n",
      "200\n",
      "[942,   399] loss: 2.379\n",
      "200\n",
      "[943,   399] loss: 2.378\n",
      "200\n",
      "[944,   399] loss: 2.370\n",
      "200\n",
      "[945,   399] loss: 2.371\n",
      "200\n",
      "[946,   399] loss: 2.374\n",
      "200\n",
      "[947,   399] loss: 2.372\n",
      "200\n",
      "[948,   399] loss: 2.366\n",
      "200\n",
      "[949,   399] loss: 2.364\n",
      "200\n",
      "[950,   399] loss: 2.360\n",
      "200\n",
      "[951,   399] loss: 2.357\n",
      "200\n",
      "[952,   399] loss: 2.356\n",
      "200\n",
      "[953,   399] loss: 2.353\n",
      "200\n",
      "[954,   399] loss: 2.351\n",
      "200\n",
      "[955,   399] loss: 2.348\n",
      "200\n",
      "[956,   399] loss: 2.349\n",
      "200\n",
      "[957,   399] loss: 2.343\n",
      "200\n",
      "[958,   399] loss: 2.342\n",
      "200\n",
      "[959,   399] loss: 2.338\n",
      "200\n",
      "[960,   399] loss: 2.339\n",
      "200\n",
      "[961,   399] loss: 2.331\n",
      "200\n",
      "[962,   399] loss: 2.331\n",
      "200\n",
      "[963,   399] loss: 2.330\n",
      "200\n",
      "[964,   399] loss: 2.330\n",
      "200\n",
      "[965,   399] loss: 2.325\n",
      "200\n",
      "[966,   399] loss: 2.323\n",
      "200\n",
      "[967,   399] loss: 2.320\n",
      "200\n",
      "[968,   399] loss: 2.315\n",
      "200\n",
      "[969,   399] loss: 2.316\n",
      "200\n",
      "[970,   399] loss: 2.318\n",
      "200\n",
      "[971,   399] loss: 2.313\n",
      "200\n",
      "[972,   399] loss: 2.310\n",
      "200\n",
      "[973,   399] loss: 2.307\n",
      "200\n",
      "[974,   399] loss: 2.302\n",
      "200\n",
      "[975,   399] loss: 2.301\n",
      "200\n",
      "[976,   399] loss: 2.303\n",
      "200\n",
      "[977,   399] loss: 2.297\n",
      "200\n",
      "[978,   399] loss: 2.296\n",
      "200\n",
      "[979,   399] loss: 2.295\n",
      "200\n",
      "[980,   399] loss: 2.293\n",
      "200\n",
      "[981,   399] loss: 2.288\n",
      "200\n",
      "[982,   399] loss: 2.284\n",
      "200\n",
      "[983,   399] loss: 2.282\n",
      "200\n",
      "[984,   399] loss: 2.283\n",
      "200\n",
      "[985,   399] loss: 2.279\n",
      "200\n",
      "[986,   399] loss: 2.277\n",
      "200\n",
      "[987,   399] loss: 2.276\n",
      "200\n",
      "[988,   399] loss: 2.271\n",
      "200\n",
      "[989,   399] loss: 2.267\n",
      "200\n",
      "[990,   399] loss: 2.268\n",
      "200\n",
      "[991,   399] loss: 2.269\n",
      "200\n",
      "[992,   399] loss: 2.263\n",
      "200\n",
      "[993,   399] loss: 2.262\n",
      "200\n",
      "[994,   399] loss: 2.260\n",
      "200\n",
      "[995,   399] loss: 2.254\n",
      "200\n",
      "[996,   399] loss: 2.250\n",
      "200\n",
      "[997,   399] loss: 2.252\n",
      "200\n",
      "[998,   399] loss: 2.251\n",
      "200\n",
      "[999,   399] loss: 2.251\n",
      "200\n",
      "[1000,   399] loss: 2.246\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "reconstruction_loss_arr = []\n",
    "kl_loss_obj_arr = []\n",
    "kl_loss_part_arr = []\n",
    "bbox_loss_arr = []\n",
    "refined_bbox_loss_arr = []\n",
    "adj_loss_arr = []\n",
    "node_loss_arr = []\n",
    "\n",
    "reconstruction_loss_val_arr = []\n",
    "kl_loss_val_arr = []\n",
    "bbox_loss_val_arr = []\n",
    "refined_bbox_loss_val_arr = []\n",
    "adj_loss_val_arr = []\n",
    "node_loss_val_arr = []\n",
    "\n",
    "bbox_loss_threshold = 1.0\n",
    "vae = TwoStageAutoEncoder(latent_dims,\n",
    "                          num_nodes,\n",
    "                          bbx_size,\n",
    "                          num_classes,\n",
    "                          label_shape,\n",
    "                          hidden1,\n",
    "                          hidden2,\n",
    "                          hidden3,\n",
    "                          dense_hidden1,\n",
    "                          dense_hidden2,\n",
    "                          adaptive_margin,\n",
    "                          output_log,\n",
    "                          area_encoding,\n",
    "                          coupling,\n",
    "                          obj_bbx_conditioning,\n",
    "                          use_fft_on_bbx,\n",
    "                          use_gcn_in_decoder\n",
    "                        )\n",
    "vae.to(device)\n",
    "optimizer = optim.Adam(vae.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100,300], gamma=0.75)\n",
    "model_path = ('/Users/amrutamuthal/Documents/training_runs/model/'+run_prefix+'/Obj-Box-'\n",
    "                        +str(learning_rate)\n",
    "                        +'-batch-'+str(batch_size)\n",
    "                        +'-h1-'+str(hidden1)\n",
    "                        +'-h2-'+str(hidden2)\n",
    "                        +'-h3-'+str(hidden3)+'-test')\n",
    "summary_path = ('/Users/amrutamuthal/Documents/training_runs/runs/'+run_prefix+'/Obj-Box-'\n",
    "                        +str(learning_rate)\n",
    "                        +'-batch-'+str(batch_size)\n",
    "                        +'-h1-'+str(hidden1)\n",
    "                        +'-h2-'+str(hidden2)\n",
    "                        +'-h3-'+str(hidden3)+'-test')\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "writer = SummaryWriter(summary_path)\n",
    "icoef = 0\n",
    "\n",
    "for epoch in range(nb_epochs):\n",
    "    \n",
    "    batch_loss = torch.tensor([0.0])\n",
    "    batch_kl_loss_part = torch.tensor([0.0])\n",
    "    batch_kl_loss_obj = torch.tensor([0.0])\n",
    "    batch_bbox_loss = torch.tensor([0.0])\n",
    "    batch_refined_bbox_loss = torch.tensor([0.0])\n",
    "    batch_obj_bbox_loss = torch.tensor([0.0])\n",
    "    batch_node_loss = torch.tensor([0.0])\n",
    "    IOU_weight_delta = torch.tensor([(1+epoch)/nb_epochs])\n",
    "    images = []\n",
    "    \n",
    "    vae.train()\n",
    "    i=0\n",
    "    for train_data in batch_train_loader:\n",
    "        \n",
    "        node_data_true = train_data.x\n",
    "        label_true = node_data_true[:,:1]\n",
    "        y_true = train_data.y\n",
    "        class_true = y_true[:, :num_classes]\n",
    "        X_obj_true = y_true[:, num_classes:]\n",
    "        X_obj_true_transformed = X_obj_true[:,2:]-X_obj_true[:,:2]\n",
    "        node_data_transformed = torch.cat(\n",
    "            [node_data_true[:, :-2],\n",
    "             node_data_true[:, -2:] -  node_data_true[:, -4:-2]], axis=-1)\n",
    "\n",
    "        adj_true = train_data.edge_index\n",
    "        batch = train_data.batch\n",
    "        \n",
    "        class_true  = torch.flatten(class_true)\n",
    "        \n",
    "\n",
    "        for param in vae.parameters():\n",
    "            param.grad=None\n",
    "        \n",
    "        output = vae(\n",
    "            adj_true,\n",
    "            node_data_transformed,\n",
    "            X_obj_true_transformed,\n",
    "            label_true,\n",
    "            class_true, variational, coupling,\n",
    "            training=(epoch>100))\n",
    "        \n",
    "        node_data_pred = output[0]\n",
    "        node_data_pred_new = torch.cat(\n",
    "            [node_data_pred[:, :, :-2],\n",
    "             node_data_pred[:, :, -2:] + node_data_pred[:, :, -4:-2]], axis=-1)\n",
    "        X_obj_pred = output[1]\n",
    "        X_obj_pred = torch.cat(((torch.tensor([1.0])-X_obj_pred)*torch.tensor([0.5]),\n",
    "                                (torch.tensor([1.0])+X_obj_pred)*torch.tensor([0.5])),\n",
    "                              -1)\n",
    "        label_pred = output[2]\n",
    "        z_mean_part = output[3]\n",
    "        z_logvar_part = output[4]\n",
    "        margin = output[5]\n",
    "        z_mean_obj = output[6]\n",
    "        z_logvar_obj = output[7]\n",
    "\n",
    "        node_data_pred_refined = output[8]\n",
    "        node_data_pred_refined_new = torch.cat(\n",
    "            [node_data_pred_refined[:, :, :-2],\n",
    "             node_data_pred_refined[:, :, -2:] + node_data_pred_refined[:, :, -4:-2]], axis=-1)\n",
    "\n",
    "        obj_bbox_loss = loss.obj_bbox_loss(pred_box=X_obj_pred, true_box=X_obj_true, weight=IOU_weight_delta, has_mse=False)\n",
    "        kl_loss_obj = loss.kl_loss(z_mean_obj, z_logvar_obj)\n",
    "        kl_loss_part = loss.kl_loss(z_mean_part, z_logvar_part)\n",
    "        bbox_loss = loss.weighted_bbox_loss(pred_box=node_data_pred_new, true_box=node_data_true[:,1:], weight=IOU_weight_delta, margin=margin)\n",
    "        node_loss = loss.node_loss(label_pred,label_true)\n",
    "        \n",
    "        # reconstruction_loss = (node_loss)*num_nodes\n",
    "\n",
    "        if use_gcn_in_decoder:\n",
    "            refined_bbox_loss = loss.weighted_bbox_loss(\n",
    "                pred_box=node_data_pred_refined_new,\n",
    "                true_box=node_data_true[:, 1:],\n",
    "                weight=IOU_weight_delta,\n",
    "                margin=margin,\n",
    "            )\n",
    "            reconstruction_loss = (refined_bbox_loss + node_loss) * num_nodes # * epoch / nb_epochs\n",
    "        \n",
    "        else:\n",
    "            refined_bbox_loss = torch.tensor([0.0])\n",
    "        \n",
    "        kl_weight = klw[icoef]\n",
    "        if variational and (kl_weight>0):\n",
    "            reconstruction_loss += kl_loss_part*kl_weight   \n",
    "        \n",
    "        if epoch >200:\n",
    "            reconstruction_loss += bbox_loss\n",
    "\n",
    "        reconstruction_loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        i+=1\n",
    "      \n",
    "        batch_loss += reconstruction_loss\n",
    "        batch_kl_loss_part += kl_loss_part\n",
    "        batch_kl_loss_obj += kl_loss_obj\n",
    "        batch_bbox_loss += bbox_loss\n",
    "        batch_refined_bbox_loss += refined_bbox_loss\n",
    "        batch_obj_bbox_loss += obj_bbox_loss\n",
    "        batch_node_loss += node_loss\n",
    "    \n",
    "        if i%200==0:\n",
    "            print(i)\n",
    "            global_step = epoch*len(batch_train_loader)+i\n",
    "            \n",
    "            writer.add_scalar(\"Loss/train/reconstruction_loss\", batch_loss.item()/(i+1), global_step)\n",
    "            writer.add_scalar(\"Loss/train/kl_loss_part\", batch_kl_loss_part.item()/(i+1), global_step)\n",
    "            writer.add_scalar(\"Loss/train/kl_loss_obj\", batch_kl_loss_obj.item()/(i+1), global_step)\n",
    "            writer.add_scalar(\"Loss/train/bbox_loss\", batch_bbox_loss.item()/(i+1), global_step)\n",
    "            writer.add_scalar(\"Loss/train/refined_bbox_loss\", batch_refined_bbox_loss.item()/(i+1), global_step)\n",
    "            writer.add_scalar(\"Loss/train/obj_bbox_loss\", batch_obj_bbox_loss.item()/(i+1), global_step)\n",
    "            writer.add_scalar(\"Loss/train/node_loss\", batch_node_loss.item()/(i+1), global_step)\n",
    "            \n",
    "            \n",
    "#     scheduler.step()\n",
    "    global_step = epoch*len(batch_train_loader)+i\n",
    "    image_shape = [num_nodes, bbx_size]\n",
    "\n",
    "    image = plot_utils.plot_bbx(np.reshape((node_data_true[:num_nodes,1:5]*label_true[:num_nodes]).detach().to(\"cpu\").numpy(),\n",
    "                                image_shape)).astype(float)/255\n",
    "    writer.add_image('train/images/input', image, global_step, dataformats='HWC')\n",
    "    image = plot_utils.plot_bbx((node_data_pred_new[0]*label_true[:num_nodes]).detach().to(\"cpu\").numpy()).astype(float)/255\n",
    "    writer.add_image('train/images/generated', image, global_step, dataformats='HWC')\n",
    "    image = plot_utils.plot_bbx((node_data_pred_refined_new[0]*label_true[:num_nodes]).detach().to(\"cpu\").numpy()).astype(float)/255\n",
    "    writer.add_image('train/images/refined', image, global_step, dataformats='HWC')\n",
    "    \n",
    "    reconstruction_loss_arr.append(batch_loss.detach().item()/(i+1))\n",
    "    kl_loss_obj_arr.append(batch_kl_loss_obj.detach().item()/(i+1))\n",
    "    kl_loss_part_arr.append(batch_kl_loss_part.detach().item()/(i+1))\n",
    "    bbox_loss_arr.append(batch_bbox_loss.detach().item()/(i+1))\n",
    "    refined_bbox_loss_arr.append(batch_refined_bbox_loss.detach().item()/(i+1))\n",
    "    node_loss_arr.append(batch_node_loss.detach().item()/(i+1))\n",
    "    \n",
    "    print('[%d, %5d] loss: %.3f' %\n",
    "              (epoch + 1, i + 1, batch_loss/(i+1) ))\n",
    "    \n",
    "    \n",
    "    batch_loss = torch.tensor([0.0])\n",
    "    batch_kl_loss_part = torch.tensor([0.0])\n",
    "    batch_kl_loss_obj = torch.tensor([0.0])\n",
    "    batch_bbox_loss = torch.tensor([0.0])\n",
    "    batch_refined_bbox_loss = torch.tensor([0.0])\n",
    "    batch_obj_bbox_loss = torch.tensor([0.0])\n",
    "    batch_node_loss = torch.tensor([0.0])\n",
    "    images = []\n",
    "    vae.eval()\n",
    "    for i, val_data in enumerate(batch_val_loader, 0):\n",
    "        \n",
    "        node_data_true = val_data.x\n",
    "        label_true = node_data_true[:,:1]\n",
    "        y_true = val_data.y\n",
    "        class_true = torch.flatten(y_true[:, :num_classes])\n",
    "        X_obj_true = y_true[:, num_classes:]\n",
    "        X_obj_true_transformed = X_obj_true[:,2:]-X_obj_true[:,:2]\n",
    "        node_data_transformed = torch.cat(\n",
    "            [node_data_true[:, :-2],\n",
    "             node_data_true[:, -2:] -  node_data_true[:, -4:-2]], axis=-1)\n",
    "        adj_true = val_data.edge_index\n",
    "        batch = val_data.batch\n",
    "        \n",
    "        class_true  = torch.flatten(class_true)\n",
    "        \n",
    "        output = vae(adj_true, node_data_transformed, X_obj_true_transformed, label_true , class_true, variational, coupling)\n",
    "        \n",
    "        node_data_pred = output[0]\n",
    "        node_data_pred_new = node_data_pred\n",
    "        node_data_pred_new = torch.cat(\n",
    "            [node_data_pred[:, :, :-2],\n",
    "             node_data_pred[:, :, -2:] + node_data_pred[:, :, -4:-2]], axis=-1)\n",
    "\n",
    "        X_obj_pred = output[1]\n",
    "        X_obj_pred = torch.cat(((torch.tensor([1.0])-X_obj_pred)*torch.tensor([0.5]),\n",
    "                                (torch.tensor([1.0])+X_obj_pred)*torch.tensor([0.5])),\n",
    "                              -1)\n",
    "        label_pred = output[2]\n",
    "        z_mean_part = output[3]\n",
    "        z_logvar_part = output[4]\n",
    "        margin = output[5]\n",
    "        z_mean_obj = output[6]\n",
    "        z_logvar_obj = output[7]\n",
    "\n",
    "        node_data_pred_refined = output[8]\n",
    "        node_data_pred_refined_new = node_data_pred_refined\n",
    "        node_data_pred_refined_new = torch.cat(\n",
    "            [node_data_pred_refined_new[:, :, :-2],\n",
    "             node_data_pred_refined_new[:, :, -2:] + node_data_pred_refined_new[:, :, -4:-2]], axis=-1)\n",
    "        \n",
    "        \n",
    "        obj_bbox_loss = loss.obj_bbox_loss(pred_box=X_obj_pred, true_box=X_obj_true, weight=IOU_weight_delta, has_mse=False)\n",
    "        kl_loss_obj = loss.kl_loss(z_mean_obj, z_logvar_obj)\n",
    "        kl_loss_part = loss.kl_loss(z_mean_part, z_logvar_part)\n",
    "        bbox_loss = loss.weighted_bbox_loss(pred_box=node_data_pred_new, true_box=node_data_true[:,1:], weight=IOU_weight_delta, margin=margin)\n",
    "        refined_bbox_loss = loss.weighted_bbox_loss(pred_box=node_data_pred_refined_new, true_box=node_data_true[:,1:], weight=IOU_weight_delta, margin=margin)\n",
    "        node_loss = loss.node_loss(label_pred,label_true)\n",
    "        \n",
    "        if kl_weight>0:\n",
    "            reconstruction_loss = kl_loss_part*kl_weight + (bbox_loss + node_loss)*num_nodes\n",
    "        else:\n",
    "            reconstruction_loss = (refined_bbox_loss + node_loss)*num_nodes\n",
    "\n",
    "        if epoch > 200:\n",
    "            reconstruction_loss +=  bbox_loss\n",
    "            \n",
    "        batch_loss += reconstruction_loss\n",
    "        batch_kl_loss_part += kl_loss_part\n",
    "        batch_kl_loss_obj += kl_loss_obj\n",
    "        batch_bbox_loss += bbox_loss\n",
    "        batch_refined_bbox_loss += refined_bbox_loss\n",
    "        batch_obj_bbox_loss += obj_bbox_loss\n",
    "        batch_node_loss += node_loss\n",
    "    \n",
    "    image = plot_utils.plot_bbx(np.reshape((node_data_true[:num_nodes,1:5]*label_true[:num_nodes]).detach().to(\"cpu\").numpy(),\n",
    "                                image_shape)).astype(float)/255\n",
    "    writer.add_image('val/images/input', image, global_step, dataformats='HWC')\n",
    "    image = plot_utils.plot_bbx((node_data_pred_new[0]*label_true[:num_nodes]).detach().to(\"cpu\").numpy()).astype(float)/255\n",
    "    writer.add_image('val/images/generated', image, global_step, dataformats='HWC')\n",
    "    image = plot_utils.plot_bbx((node_data_pred_refined_new[0]*label_true[:num_nodes]).detach().to(\"cpu\").numpy()).astype(float)/255\n",
    "    writer.add_image('val/images/refined', image, global_step, dataformats='HWC')\n",
    "\n",
    "    reconstruction_loss_val_arr.append(batch_loss.detach().item()/(i+1))\n",
    "    bbox_loss_val_arr.append(batch_bbox_loss.detach().item()/(i+1))\n",
    "    node_loss_val_arr.append(batch_node_loss.detach().item()/(i+1))\n",
    "    \n",
    "    writer.add_scalar(\"Loss/val/reconstruction_loss\", batch_loss.detach()/(i+1), global_step)\n",
    "    writer.add_scalar(\"Loss/train/kl_loss_part\", batch_kl_loss_part.item()/(i+1), global_step)\n",
    "    writer.add_scalar(\"Loss/train/kl_loss_obj\", batch_kl_loss_obj.item()/(i+1), global_step)\n",
    "    writer.add_scalar(\"Loss/val/bbox_loss\", batch_bbox_loss.item()/(i+1), global_step)\n",
    "    writer.add_scalar(\"Loss/val/refined_bbox_loss\", batch_refined_bbox_loss.item()/(i+1), global_step)\n",
    "    writer.add_scalar(\"Loss/val/obj_bbox_loss\", batch_obj_bbox_loss.item()/(i+1), global_step)\n",
    "    writer.add_scalar(\"Loss/val/node_loss\", batch_node_loss.item()/(i+1), global_step)\n",
    "       \n",
    "    if epoch%50 == 0:\n",
    "        torch.save(vae.state_dict(), model_path + '/model_weights.pth')\n",
    "        \n",
    "    if ((kl_loss_part_arr[-1]>0.5) and \n",
    "        (abs(bbox_loss_arr[-1] - bbox_loss_val_arr[-1]) < 0.07) and \n",
    "        (bbox_loss_arr[-1]<bbox_loss_threshold) and (epoch>300)):\n",
    "        \n",
    "        icoef = icoef + 1\n",
    "        bbox_loss_threshold*=0.9\n",
    "\n",
    "torch.save(vae.state_dict(),model_path + '/model_weights.pth')\n",
    "\n",
    "for i in range(min(100,int(len(node_data_true)/num_nodes))):    \n",
    "    image = plot_utils.plot_bbx(np.reshape((node_data_true[num_nodes*i:num_nodes*(i+1),1:5]).detach().to(\"cpu\").numpy(),\n",
    "                                    image_shape)).astype(float)/255\n",
    "    writer.add_image('result/images/'+str(i)+'-input', image, global_step, dataformats='HWC')\n",
    "    image = plot_utils.plot_bbx((node_data_pred[i]*label_true[num_nodes*i:num_nodes*(i+1)]).detach().to(\"cpu\").numpy()).astype(float)/255\n",
    "    writer.add_image('result/images/'+str(i)+'-generated', image, global_step, dataformats='HWC')\n",
    "    \n",
    "writer.flush()\n",
    "writer.close()\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d9154e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 16, 4])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_data_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a642a867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "173005"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " sum(p.numel() for p in vae.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6b72cd01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ww/3nytfk1917b8nz7dchtckxnm0000gn/T/ipykernel_70094/2805332933.py:39: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  vae.load_state_dict(torch.load(model_path+ '/model_weights.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.6509434  0.         1.         0.7411765 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.01470588 0.9103774  1.        ]\n",
      " [0.5731132  0.08529412 0.8160377  0.62647057]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.6484276  0.24249923 0.8033692  0.6244332 ]\n",
      " [0.18403584 0.2637053  0.1931692  0.27013212]\n",
      " [0.18403582 0.26370528 0.19316918 0.27013206]\n",
      " [0.1862228  0.26402155 0.785681   0.74846214]\n",
      " [0.5726416  0.2788936  0.7364092  0.5837926 ]\n",
      " [0.18403582 0.26370528 0.19316918 0.27013206]\n",
      " [0.18403582 0.26370528 0.19316918 0.27013206]\n",
      " [0.18403582 0.26370528 0.19316918 0.27013206]\n",
      " [0.18403582 0.26370528 0.19316919 0.2701321 ]\n",
      " [0.18403582 0.26370528 0.19316918 0.27013206]\n",
      " [0.18403582 0.26370528 0.19316919 0.2701321 ]\n",
      " [0.18403582 0.26370528 0.19316918 0.27013206]\n",
      " [0.18403599 0.26370606 0.19316976 0.27013314]\n",
      " [0.18403584 0.26370537 0.19316925 0.2701322 ]\n",
      " [0.18403582 0.26370528 0.19316918 0.27013206]\n",
      " [0.18403582 0.26370528 0.19316918 0.27013206]]\n",
      "[[0.45833334 0.         0.86742425 0.3462783 ]\n",
      " [0.         0.26537216 0.9886364  1.        ]\n",
      " [0.5113636  0.2815534  0.9015151  0.43365696]\n",
      " [0.7462121  0.47572815 0.9962121  0.8899676 ]\n",
      " [0.86742425 0.74757284 0.969697   0.8996764 ]\n",
      " [0.53409094 0.39158577 0.8560606  0.9514563 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.28787878 0.5566343  1.         0.98381877]\n",
      " [0.905303   0.6569579  0.9810606  0.7378641 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.4458106  0.25422665 0.67248654 0.47582585]\n",
      " [0.31221786 0.31571838 0.6927732  0.69519746]\n",
      " [0.45029855 0.3036183  0.6472133  0.42889446]\n",
      " [0.6157243  0.45846146 0.7210531  0.6190624 ]\n",
      " [0.6154505  0.5447989  0.6664108  0.61492753]\n",
      " [0.52682054 0.5755193  0.68268454 0.70954   ]\n",
      " [0.29343197 0.27451888 0.29940552 0.2806541 ]\n",
      " [0.293431   0.2745124  0.29940218 0.2806453 ]\n",
      " [0.29343098 0.2745123  0.29940212 0.28064516]\n",
      " [0.4767104  0.5908811  0.7103142  0.7620189 ]\n",
      " [0.6091604  0.6510014  0.7337241  0.7261393 ]\n",
      " [0.29344007 0.27457902 0.29944304 0.28073958]\n",
      " [0.29343098 0.2745123  0.29940212 0.28064513]\n",
      " [0.29343098 0.27451226 0.29940212 0.28064513]\n",
      " [0.29343098 0.27451226 0.29940212 0.28064513]\n",
      " [0.29343098 0.27451226 0.29940212 0.28064513]]\n",
      "[[0.85866666 0.64878047 1.         1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.00266667 0.         0.944      0.902439  ]\n",
      " [0.688      0.09756097 0.9493333  0.81463414]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.21463415 0.06666667 0.50731707]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.68289554 0.5419502  0.7766615  0.6556418 ]\n",
      " [0.22055465 0.3575395  0.2286414  0.36141858]\n",
      " [0.22055067 0.35752872 0.22862892 0.36140385]\n",
      " [0.2405746  0.3459844  0.76887476 0.6151537 ]\n",
      " [0.5771607  0.40207386 0.73866946 0.60375226]\n",
      " [0.22055052 0.3575282  0.22862837 0.36140317]\n",
      " [0.22055052 0.3575282  0.22862837 0.36140317]\n",
      " [0.22055055 0.35752833 0.2286285  0.36140335]\n",
      " [0.22055054 0.35752827 0.22862843 0.36140326]\n",
      " [0.22055052 0.3575282  0.22862837 0.36140317]\n",
      " [0.22055072 0.35752884 0.22862905 0.36140403]\n",
      " [0.22055054 0.35752824 0.22862841 0.36140323]\n",
      " [0.22055356 0.3575382  0.22863925 0.3614168 ]\n",
      " [0.21782073 0.3948879  0.28925824 0.53423846]\n",
      " [0.22055052 0.3575282  0.22862837 0.36140317]\n",
      " [0.22055064 0.35752863 0.22862883 0.36140376]]\n",
      "[[0.11036789 0.         0.9531773  0.7368421 ]\n",
      " [0.         0.38080496 1.         1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.30369598 0.24423198 0.6883509  0.624923  ]\n",
      " [0.27013385 0.4287879  0.73222494 0.77017796]\n",
      " [0.2660449  0.264296   0.2728077  0.2707067 ]\n",
      " [0.2660449  0.264296   0.2728077  0.2707067 ]\n",
      " [0.2660449  0.264296   0.2728077  0.2707067 ]\n",
      " [0.2660449  0.264296   0.2728077  0.2707067 ]\n",
      " [0.2660449  0.264296   0.2728077  0.2707067 ]\n",
      " [0.2660449  0.26429603 0.2728077  0.27070674]\n",
      " [0.2660449  0.264296   0.2728077  0.27070674]\n",
      " [0.26604587 0.26430207 0.27281106 0.270715  ]\n",
      " [0.2660449  0.264296   0.2728077  0.2707067 ]\n",
      " [0.2660449  0.264296   0.2728077  0.2707067 ]\n",
      " [0.2660449  0.26429603 0.2728077  0.27070674]\n",
      " [0.2660449  0.264296   0.2728077  0.2707067 ]\n",
      " [0.2660449  0.264296   0.2728077  0.2707067 ]\n",
      " [0.2660449  0.264296   0.2728077  0.2707067 ]]\n",
      "[[0.45731708 0.         1.         0.65397924]\n",
      " [0.         0.20415226 0.8993902  0.9757785 ]\n",
      " [0.39939025 0.20415226 0.8993902  0.7716263 ]\n",
      " [0.89329267 0.6089965  0.9512195  0.64705884]\n",
      " [0.89329267 0.6089965  0.9512195  0.64705884]\n",
      " [0.49390244 0.6401384  1.         1.        ]\n",
      " [0.85365856 0.6366782  1.         0.8442907 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.4765738  0.2819704  0.7617273  0.583097  ]\n",
      " [0.25562668 0.36119726 0.684494   0.68064606]\n",
      " [0.46162426 0.40150902 0.72138345 0.6266866 ]\n",
      " [0.65282464 0.5610404  0.7308241  0.619383  ]\n",
      " [0.6841906  0.5579026  0.74044997 0.60606503]\n",
      " [0.5876843  0.5712477  0.78691864 0.6791003 ]\n",
      " [0.674672   0.59531283 0.7766514  0.67581874]\n",
      " [0.2555749  0.29914966 0.2626404  0.3046125 ]\n",
      " [0.25557488 0.2991495  0.2626403  0.30461228]\n",
      " [0.25557488 0.2991495  0.2626403  0.3046123 ]\n",
      " [0.25557488 0.2991495  0.2626403  0.30461228]\n",
      " [0.25557488 0.2991495  0.2626403  0.30461228]\n",
      " [0.25557494 0.29914987 0.26264057 0.3046128 ]\n",
      " [0.25557488 0.2991495  0.2626403  0.30461228]\n",
      " [0.25557488 0.2991495  0.2626403  0.30461228]\n",
      " [0.25557488 0.2991495  0.2626403  0.30461228]]\n",
      "[[0.6754386  0.3197279  1.         0.9727891 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.02631579 0.         0.9122807  0.81632656]\n",
      " [0.7631579  0.31292516 0.8903509  0.4829932 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.55263156 0.7210884  0.79385966 0.9183673 ]\n",
      " [0.5394737  0.8095238  0.78070176 0.9455782 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.20614035 0.56462586 0.39473686 0.8979592 ]\n",
      " [0.09210526 0.43537414 0.25877193 0.7619048 ]\n",
      " [0.1491228  0.6938776  0.3640351  1.        ]\n",
      " [0.         0.07482993 0.10526316 0.6938776 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.58933014 0.47746497 0.6761286  0.5784945 ]\n",
      " [0.3258471  0.39528322 0.3308812  0.3981313 ]\n",
      " [0.32584712 0.3952833  0.3308813  0.39813146]\n",
      " [0.34048077 0.38909808 0.64267385 0.5525858 ]\n",
      " [0.58204174 0.4477848  0.6472715  0.53047174]\n",
      " [0.32584727 0.39528388 0.33088183 0.39813223]\n",
      " [0.3258471  0.39528322 0.3308812  0.39813134]\n",
      " [0.5027076  0.5262928  0.5571826  0.59330845]\n",
      " [0.5162332  0.57631755 0.5453455  0.61037207]\n",
      " [0.32584724 0.39528376 0.3308817  0.3981321 ]\n",
      " [0.36396238 0.5433232  0.39173636 0.5853707 ]\n",
      " [0.37434784 0.46616274 0.44195253 0.5249708 ]\n",
      " [0.46840817 0.5553225  0.5115553  0.6115476 ]\n",
      " [0.32450697 0.42073396 0.35990852 0.52362776]\n",
      " [0.3258471  0.39528322 0.3308812  0.3981313 ]\n",
      " [0.3258471  0.39528322 0.3308812  0.3981313 ]]\n",
      "[[0.3227991  0.         1.         0.5416667 ]\n",
      " [0.44243792 0.45833334 0.81038374 0.8138889 ]\n",
      " [0.5011287  0.4888889  0.81038374 0.6333333 ]\n",
      " [0.62528217 0.65555555 0.95259595 1.        ]\n",
      " [0.67945826 0.7722222  0.94808125 1.        ]\n",
      " [0.         0.16388889 0.53047407 0.73055553]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.32943973 0.22058573 0.76956874 0.5312392 ]\n",
      " [0.4816766  0.4710403  0.77193105 0.63883054]\n",
      " [0.56683767 0.40237957 0.77911526 0.49628636]\n",
      " [0.45533496 0.5803307  0.73066694 0.7690766 ]\n",
      " [0.5042527  0.61620504 0.7051758  0.7129462 ]\n",
      " [0.18356597 0.41463494 0.45292467 0.6787181 ]\n",
      " [0.16162863 0.24357353 0.17142196 0.2505568 ]\n",
      " [0.16162397 0.24355082 0.17140521 0.2505258 ]\n",
      " [0.16162395 0.24355075 0.17140515 0.2505257 ]\n",
      " [0.16162407 0.2435513  0.17140557 0.25052646]\n",
      " [0.16162395 0.24355075 0.17140515 0.2505257 ]\n",
      " [0.16162395 0.24355075 0.17140515 0.2505257 ]\n",
      " [0.16162395 0.24355075 0.17140515 0.2505257 ]\n",
      " [0.16162395 0.24355075 0.17140515 0.2505257 ]\n",
      " [0.16162395 0.24355075 0.17140515 0.2505257 ]\n",
      " [0.16162395 0.24355075 0.17140515 0.2505257 ]]\n",
      "[[0.84615386 0.         1.         0.37226278]\n",
      " [0.60946745 0.890511   0.6627219  0.93430656]\n",
      " [0.56213015 0.919708   0.61538464 0.96350366]\n",
      " [0.04733728 0.06569343 0.8579882  0.6277372 ]\n",
      " [0.5857988  0.05839416 0.852071   0.4379562 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.6035503  0.72262776 0.6627219  0.93430656]\n",
      " [0.56213015 0.5182482  0.65088755 0.7518248 ]\n",
      " [0.556213   0.7080292  0.6213018  0.96350366]\n",
      " [0.05325444 0.5693431  0.12426036 0.7591241 ]\n",
      " [0.01775148 0.73722625 0.08284023 0.99270076]\n",
      " [0.12426036 0.57664233 0.20118344 0.72262776]\n",
      " [0.1183432  0.71532845 0.2366864  1.        ]\n",
      " [0.         0.27737227 0.10650887 0.890511  ]\n",
      " [0.0295858  0.95620435 0.07692308 1.        ]\n",
      " [0.18343195 0.95620435 0.22485207 0.99270076]] [[0.5771977  0.3966931  0.62863076 0.46905017]\n",
      " [0.5276867  0.5585507  0.53756857 0.5667641 ]\n",
      " [0.5048498  0.5730504  0.51923585 0.5779934 ]\n",
      " [0.39176616 0.40267947 0.5720496  0.5268818 ]\n",
      " [0.545348   0.4043823  0.59841436 0.47318405]\n",
      " [0.37417313 0.40500304 0.3779073  0.40767092]\n",
      " [0.5534903  0.55163634 0.56880236 0.58539134]\n",
      " [0.53464913 0.506065   0.5524495  0.54424536]\n",
      " [0.5243594  0.54711807 0.54281986 0.5894432 ]\n",
      " [0.3876396  0.50668204 0.41032284 0.54657876]\n",
      " [0.38980106 0.5494821  0.40971783 0.59809554]\n",
      " [0.38681573 0.510473   0.418476   0.55994296]\n",
      " [0.39371654 0.5483892  0.4230178  0.60951537]\n",
      " [0.37227577 0.45370874 0.40339783 0.58662915]\n",
      " [0.3978671  0.58963346 0.4129262  0.6009932 ]\n",
      " [0.4147744  0.59413004 0.42762578 0.6072503 ]]\n",
      "[[0.         0.         0.44507042 0.76347303]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.23661971 0.16167665 1.         1.        ]\n",
      " [0.23661971 0.19161677 0.73802817 0.8742515 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.23815548 0.25355086 0.49419165 0.6287546 ]\n",
      " [0.24206819 0.27367848 0.24952412 0.2798341 ]\n",
      " [0.24206814 0.27367833 0.249524   0.27983385]\n",
      " [0.3701284  0.3362441  0.7978353  0.7479731 ]\n",
      " [0.36914384 0.35444054 0.6373835  0.69327956]\n",
      " [0.24206814 0.2736783  0.249524   0.27983385]\n",
      " [0.24206814 0.2736783  0.249524   0.27983385]\n",
      " [0.24206814 0.2736783  0.249524   0.27983385]\n",
      " [0.24206814 0.2736783  0.249524   0.27983385]\n",
      " [0.24206814 0.2736783  0.249524   0.27983385]\n",
      " [0.24206814 0.27367833 0.249524   0.27983385]\n",
      " [0.24206814 0.2736783  0.249524   0.27983385]\n",
      " [0.24206814 0.2736783  0.249524   0.27983385]\n",
      " [0.24206814 0.2736783  0.249524   0.27983385]\n",
      " [0.24206814 0.2736783  0.249524   0.27983385]\n",
      " [0.24206814 0.2736783  0.249524   0.27983385]]\n",
      "[[0.         0.         0.63736266 0.7147436 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.32307693 0.23076923 1.         1.        ]\n",
      " [0.30989012 0.37179488 0.53406596 0.875     ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.1556445  0.2637477  0.6325995  0.61021423]\n",
      " [0.16093467 0.283165   0.1707359  0.2890626 ]\n",
      " [0.16093467 0.28316498 0.17073588 0.28906256]\n",
      " [0.4376817  0.3714779  0.90380263 0.74337506]\n",
      " [0.38774484 0.4222312  0.54283035 0.6774448 ]\n",
      " [0.16093463 0.28316483 0.17073575 0.28906235]\n",
      " [0.16093463 0.28316483 0.17073575 0.28906235]\n",
      " [0.16093463 0.28316483 0.17073575 0.28906235]\n",
      " [0.16093463 0.28316483 0.17073576 0.28906235]\n",
      " [0.16093463 0.28316483 0.17073575 0.28906235]\n",
      " [0.16093463 0.28316483 0.17073575 0.28906235]\n",
      " [0.16093463 0.28316483 0.17073575 0.28906235]\n",
      " [0.16093463 0.28316483 0.17073575 0.28906235]\n",
      " [0.16093463 0.28316483 0.17073575 0.28906235]\n",
      " [0.16093463 0.28316483 0.17073575 0.28906235]\n",
      " [0.16093463 0.28316483 0.17073575 0.28906235]]\n",
      "[[0.10865191 0.04102564 1.         0.9794872 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.45070422 1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.20518772 0.21356538 0.8654065  0.74521106]\n",
      " [0.13889542 0.23573217 0.14933363 0.2429198 ]\n",
      " [0.13889542 0.23573217 0.14933363 0.24291979]\n",
      " [0.13577695 0.22354135 0.4567408  0.78509194]\n",
      " [0.13889672 0.23573816 0.14933825 0.242928  ]\n",
      " [0.13889542 0.23573214 0.14933361 0.24291976]\n",
      " [0.13889542 0.23573214 0.14933361 0.24291976]\n",
      " [0.13889542 0.23573214 0.14933361 0.24291976]\n",
      " [0.13889542 0.23573214 0.14933361 0.24291976]\n",
      " [0.13889542 0.23573214 0.14933361 0.24291976]\n",
      " [0.13889542 0.23573214 0.14933361 0.24291976]\n",
      " [0.13889542 0.23573214 0.14933361 0.24291976]\n",
      " [0.13889542 0.23573214 0.14933361 0.24291976]\n",
      " [0.13889542 0.23573214 0.14933361 0.24291976]\n",
      " [0.13889542 0.23573214 0.14933361 0.24291976]\n",
      " [0.13889542 0.23573214 0.14933361 0.24291976]]\n",
      "[[0.8191126  0.36206895 1.         0.58965516]\n",
      " [0.         0.         0.96245736 0.62413794]\n",
      " [0.8020478  0.24482758 0.9522184  0.38965517]\n",
      " [0.59726965 0.35517243 0.75085324 0.64827585]\n",
      " [0.665529   0.59310347 0.7303754  0.65172416]\n",
      " [0.6177474  0.27931035 0.8327645  0.5965517 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.06484642 0.57586205 0.23549488 0.8965517 ]\n",
      " [0.12627986 0.7862069  0.23549488 0.8965517 ]\n",
      " [0.25255972 0.44827586 0.5699659  1.        ]\n",
      " [0.45392492 0.92068964 0.5699659  0.99310344]\n",
      " [0.         0.31034482 0.17064847 0.8689655 ]\n",
      " [0.87713313 0.50689656 0.9692833  0.5827586 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.6142709  0.3517992  0.71565765 0.54029036]\n",
      " [0.3222169  0.30243996 0.6827143  0.55132234]\n",
      " [0.6159872  0.34825724 0.68482476 0.4754695 ]\n",
      " [0.5691186  0.49657363 0.64365274 0.5792275 ]\n",
      " [0.5884925  0.53595126 0.61674696 0.56795394]\n",
      " [0.5057658  0.40294003 0.6172192  0.5542848 ]\n",
      " [0.29257575 0.30854172 0.298576   0.3137529 ]\n",
      " [0.3215344  0.5194517  0.39760214 0.67965615]\n",
      " [0.3189593  0.62109125 0.35506296 0.6631212 ]\n",
      " [0.3617494  0.3872813  0.53480625 0.71852195]\n",
      " [0.4463161  0.6682302  0.49975288 0.7381778 ]\n",
      " [0.28954935 0.35136333 0.40053678 0.62991637]\n",
      " [0.6540656  0.4757639  0.7077784  0.55189   ]\n",
      " [0.292574   0.3085318  0.2985699  0.3137394 ]\n",
      " [0.292574   0.3085318  0.2985699  0.3137394 ]\n",
      " [0.292574   0.3085318  0.2985699  0.3137394 ]]\n",
      "[[0.         0.23076923 0.2962963  0.7       ]\n",
      " [0.2361111  0.         0.8449074  0.75      ]\n",
      " [0.28935185 0.19615385 0.36342594 0.52692306]\n",
      " [0.42824075 0.4076923  0.5555556  1.        ]\n",
      " [0.4189815  0.76153845 0.5462963  1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.6018519  0.37692308 0.8287037  0.9115385 ]\n",
      " [0.5925926  0.7076923  0.7037037  0.9       ]\n",
      " [0.7175926  0.5846154  0.849537   0.9115385 ]\n",
      " [0.7175926  0.74615383 0.8078704  0.9269231 ]\n",
      " [0.8194444  0.11153846 1.         0.34615386]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.18119197 0.35237867 0.35792145 0.5813994 ]\n",
      " [0.35413152 0.3308494  0.72777146 0.5875243 ]\n",
      " [0.30396336 0.37708136 0.40782905 0.5686867 ]\n",
      " [0.45249355 0.47281897 0.5718318  0.6882797 ]\n",
      " [0.4483577  0.62922513 0.51612496 0.67910445]\n",
      " [0.18612583 0.3238334  0.19520746 0.32862952]\n",
      " [0.18612237 0.32382143 0.1951954  0.32861316]\n",
      " [0.62933505 0.46130124 0.7456879  0.64543664]\n",
      " [0.6347772  0.59363145 0.6911174  0.6484066 ]\n",
      " [0.6032136  0.5225127  0.66855985 0.6706022 ]\n",
      " [0.6002746  0.617336   0.64482296 0.6612367 ]\n",
      " [0.69681025 0.3598008  0.83827233 0.48929936]\n",
      " [0.18612862 0.32384425 0.19521801 0.32864437]\n",
      " [0.18612237 0.32382143 0.1951954  0.32861316]\n",
      " [0.18612237 0.32382143 0.1951954  0.32861316]\n",
      " [0.18612237 0.32382143 0.1951954  0.32861316]]\n",
      "[[0.         0.01020408 0.4723127  0.7397959 ]\n",
      " [0.2801303  0.         0.32899022 0.0867347 ]\n",
      " [0.09120521 0.03061225 0.15635179 0.10714286]\n",
      " [0.08794788 0.1122449  0.8436482  0.9897959 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.33876222 0.7091837  0.534202   1.        ]\n",
      " [0.3843648  0.8214286  0.5960912  0.97959185]\n",
      " [0.10749186 0.872449   0.21498372 0.99489796]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.78827363 0.3112245  0.87296414 0.5408163 ]\n",
      " [0.81433225 0.53571427 0.93485343 0.93877554]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.62540716 0.67346936 0.8078176  0.86734694]\n",
      " [0.7947883  0.63265306 1.         0.7602041 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.26162535 0.3485927  0.61299944 0.55897295]\n",
      " [0.4632519  0.38456813 0.48226896 0.41505826]\n",
      " [0.27204332 0.43632904 0.34316453 0.49016234]\n",
      " [0.35867444 0.36919245 0.75312126 0.6267797 ]\n",
      " [0.26550466 0.36037767 0.27228308 0.36417514]\n",
      " [0.49864668 0.5749685  0.588381   0.64001954]\n",
      " [0.4704627  0.6321497  0.5127597  0.6741428 ]\n",
      " [0.33124837 0.58747977 0.38048434 0.6325334 ]\n",
      " [0.26563787 0.36159128 0.2728415  0.3656655 ]\n",
      " [0.61863244 0.4904758  0.66291976 0.5371891 ]\n",
      " [0.5285629  0.5244111  0.5513442  0.5590724 ]\n",
      " [0.2655184  0.3604264  0.2723287  0.36424103]\n",
      " [0.5719033  0.60159403 0.6099793  0.6406114 ]\n",
      " [0.59591335 0.5459901  0.6385838  0.6083354 ]\n",
      " [0.2655048  0.36037824 0.2722836  0.36417595]\n",
      " [0.26550466 0.36037764 0.27228305 0.3641751 ]]\n",
      "[[0.36734694 0.         1.         0.22885571]\n",
      " [0.0952381  0.13930348 0.9047619  0.6965174 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.04081633 0.2636816  0.27210885 0.52238804]\n",
      " [0.39455783 0.6467662  0.56462586 1.        ]\n",
      " [0.3605442  0.7761194  0.5442177  0.9900498 ]\n",
      " [0.00680272 0.48258707 0.2993197  0.86567163]\n",
      " [0.         0.6666667  0.14285715 0.8557214 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.50066864 0.34438446 0.6096464  0.43393803]\n",
      " [0.39669916 0.3799291  0.6044525  0.5808322 ]\n",
      " [0.38771752 0.35681826 0.39096397 0.36071348]\n",
      " [0.38771877 0.35682872 0.39096847 0.3607276 ]\n",
      " [0.3891754  0.413648   0.4197906  0.49348837]\n",
      " [0.47886068 0.5855889  0.55292046 0.65456903]\n",
      " [0.4471224  0.60927665 0.4987668  0.66494274]\n",
      " [0.3932874  0.5071354  0.44692495 0.58636546]\n",
      " [0.39703786 0.5910641  0.4471716  0.63371754]\n",
      " [0.38771743 0.35681763 0.39096367 0.36071262]\n",
      " [0.38771722 0.35681581 0.3909629  0.36071017]\n",
      " [0.38771722 0.35681581 0.3909629  0.36071017]\n",
      " [0.38771722 0.35681581 0.3909629  0.36071017]\n",
      " [0.38771722 0.35681581 0.3909629  0.36071017]\n",
      " [0.38771722 0.35681581 0.3909629  0.36071017]\n",
      " [0.38771722 0.35681581 0.3909629  0.36071017]]\n",
      "[[0.47419354 0.         1.         0.60559005]\n",
      " [0.         0.34472048 0.66129035 0.7670807 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.516129   0.66459626 0.683871   0.9720497 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.22580644 0.6459627  0.38387096 0.99378884]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.01935484 0.55590063 0.24193548 1.        ]\n",
      " [0.13548388 0.8664596  0.2451613  1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.5096774  0.40062112 0.7612903  0.61490685]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.46176165 0.24489304 0.7502807  0.55119777]\n",
      " [0.27438533 0.43910313 0.60694945 0.65021485]\n",
      " [0.25743788 0.26502576 0.26444945 0.27141666]\n",
      " [0.55511105 0.6235583  0.6639813  0.7305577 ]\n",
      " [0.25744197 0.26505032 0.26446372 0.27144998]\n",
      " [0.32210708 0.6176718  0.45368016 0.73032665]\n",
      " [0.25743836 0.2650288  0.2644512  0.27142078]\n",
      " [0.25743788 0.26502585 0.2644495  0.27141678]\n",
      " [0.25743788 0.26502576 0.26444945 0.27141663]\n",
      " [0.260134   0.5563801  0.41037127 0.8015754 ]\n",
      " [0.2706494  0.7123115  0.32576576 0.76610535]\n",
      " [0.25743887 0.26503184 0.264453   0.27142492]\n",
      " [0.4957373  0.40808707 0.6042299  0.5475517 ]\n",
      " [0.25743786 0.26502573 0.26444945 0.2714166 ]\n",
      " [0.25743786 0.26502573 0.26444945 0.2714166 ]\n",
      " [0.25743786 0.26502573 0.26444945 0.2714166 ]]\n",
      "[[0.         0.         0.52156055 0.5514874 ]\n",
      " [0.18685831 0.13501143 0.8131417  0.6361556 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.32443532 0.5652174  0.50513345 1.        ]\n",
      " [0.3100616  0.8741419  0.43942505 1.        ]\n",
      " [0.23613963 0.597254   0.35318276 0.8672769 ]\n",
      " [0.21765913 0.8077803  0.3429158  0.8672769 ]\n",
      " [0.61601645 0.4256293  0.7823409  0.8993135 ]\n",
      " [0.67351127 0.7459954  0.7967146  0.8924485 ]\n",
      " [0.3429158  0.48283753 0.5379877  0.7643021 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.7227926  0.14874142 1.         0.5286041 ]\n",
      " [0.23613963 0.3501144  0.38398358 0.47139588]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.15822768 0.19240367 0.5226719  0.56357217]\n",
      " [0.29192019 0.26727945 0.67324775 0.5618187 ]\n",
      " [0.15523608 0.21148857 0.16520752 0.21934003]\n",
      " [0.48043144 0.5544647  0.6039815  0.83094895]\n",
      " [0.40156016 0.7220279  0.48839983 0.8188062 ]\n",
      " [0.26478428 0.5418344  0.41473663 0.7684989 ]\n",
      " [0.27414817 0.66341007 0.3654477  0.7477144 ]\n",
      " [0.5677991  0.42850706 0.78797543 0.77780247]\n",
      " [0.62244606 0.6456617  0.6969575  0.7127278 ]\n",
      " [0.38809404 0.46020406 0.5090556  0.620379  ]\n",
      " [0.15541086 0.21202196 0.1655887  0.22004701]\n",
      " [0.63763255 0.26256943 0.82345355 0.38593405]\n",
      " [0.3376177  0.38995254 0.5209273  0.53919387]\n",
      " [0.15523389 0.21147719 0.1651998  0.21932448]\n",
      " [0.15523389 0.21147719 0.1651998  0.21932448]\n",
      " [0.15523389 0.21147719 0.1651998  0.21932448]]\n",
      "[[0.30769232 0.         0.9692308  0.38235295]\n",
      " [0.         0.04901961 1.         0.7058824 ]\n",
      " [0.24615385 0.14705883 0.9692308  0.46078432]\n",
      " [0.5846154  0.6372549  0.86153847 1.        ]\n",
      " [0.6769231  0.92156863 0.83076924 0.99019605]\n",
      " [0.2769231  0.6666667  0.5538462  0.9705882 ]\n",
      " [0.33846155 0.88235295 0.5538462  0.98039216]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.04615385 0.50980395 0.23076923 0.78431374]\n",
      " [0.06153846 0.71568626 0.21538462 0.78431374]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.4826047  0.42798617 0.5468096  0.48651242]\n",
      " [0.45436165 0.43590724 0.541676   0.5345927 ]\n",
      " [0.48027402 0.4510089  0.53750795 0.49347743]\n",
      " [0.51316684 0.5239207  0.53807086 0.56871843]\n",
      " [0.5143013  0.55212265 0.5331094  0.5664283 ]\n",
      " [0.48060906 0.5214586  0.50682694 0.5768182 ]\n",
      " [0.48539224 0.55832565 0.50515187 0.57300675]\n",
      " [0.45398408 0.43265682 0.45531446 0.43448874]\n",
      " [0.45398405 0.43265647 0.45531434 0.43448827]\n",
      " [0.45632827 0.50092876 0.47499084 0.5461991 ]\n",
      " [0.46015275 0.5318891  0.47835344 0.5465216 ]\n",
      " [0.4539842  0.43265784 0.45531487 0.43449014]\n",
      " [0.45398402 0.43265614 0.4553142  0.43448782]\n",
      " [0.453984   0.43265602 0.45531416 0.43448764]\n",
      " [0.453984   0.43265602 0.45531416 0.43448764]\n",
      " [0.453984   0.43265602 0.45531416 0.43448764]]\n",
      "[[0.08675799 0.         0.4520548  0.5254237 ]\n",
      " [0.12785389 0.31355932 0.9543379  0.9322034 ]\n",
      " [0.1369863  0.37288135 0.37899545 0.7033898 ]\n",
      " [0.21917808 0.7118644  0.5251142  1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.7288136  0.21917808 0.94067794]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.5936073  0.7627119  0.9315069  0.9915254 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.9315069  0.5762712  1.         0.7457627 ]\n",
      " [0.11415525 0.16101696 0.3196347  0.5254237 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.33017033 0.40616927 0.4952913  0.50629544]\n",
      " [0.3664348  0.45989573 0.6528748  0.5912374 ]\n",
      " [0.37622032 0.47417372 0.47675526 0.5382773 ]\n",
      " [0.3993035  0.52961254 0.5093284  0.5793539 ]\n",
      " [0.32864168 0.4138918  0.3335953  0.4162339 ]\n",
      " [0.34422716 0.53786814 0.39628428 0.5754559 ]\n",
      " [0.3286456  0.4139046  0.3336093  0.41625133]\n",
      " [0.54030746 0.5260686  0.6362486  0.57925856]\n",
      " [0.32864326 0.41389662 0.33360073 0.4162405 ]\n",
      " [0.3286417  0.41389185 0.33359537 0.416234  ]\n",
      " [0.3286416  0.41389143 0.33359492 0.41623345]\n",
      " [0.5938394  0.53135574 0.6481573  0.5660149 ]\n",
      " [0.34785664 0.44031528 0.40535006 0.48861903]\n",
      " [0.3286416  0.41389143 0.33359492 0.41623345]\n",
      " [0.3286416  0.41389143 0.33359492 0.41623345]\n",
      " [0.3286416  0.41389143 0.33359492 0.41623345]]\n",
      "[[0.68123394 0.16867469 0.9974293  0.53313255]\n",
      " [0.14652957 0.31325302 0.9151671  0.9096386 ]\n",
      " [0.62982005 0.31024095 0.8971722  0.67168677]\n",
      " [0.6709511  0.78614455 0.7917738  0.9789157 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.46272492 0.6987952  0.6143959  1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.24935733 0.64759034 0.38303342 0.96385545]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.5873494  0.2570694  0.8313253 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.05655527 0.         0.34190232 0.4246988 ]\n",
      " [0.8071979  0.36746988 1.         0.52710843]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.61880326 0.30110556 0.8158691  0.54576623]\n",
      " [0.29332274 0.39272052 0.7398405  0.6753708 ]\n",
      " [0.60252416 0.42693192 0.70888984 0.53420424]\n",
      " [0.64772725 0.65460086 0.7294074  0.73749906]\n",
      " [0.20287076 0.26349702 0.21145979 0.26992956]\n",
      " [0.49722734 0.63436043 0.6335093  0.7839895 ]\n",
      " [0.20287086 0.26349753 0.21146014 0.26993024]\n",
      " [0.33122557 0.572894   0.45123515 0.7479724 ]\n",
      " [0.20287073 0.26349682 0.21145965 0.2699293 ]\n",
      " [0.22749789 0.4740836  0.34686685 0.6957303 ]\n",
      " [0.20287086 0.26349753 0.21146014 0.26993027]\n",
      " [0.20017566 0.24845703 0.4216648  0.44685638]\n",
      " [0.7170284  0.41989022 0.83496773 0.50928664]\n",
      " [0.20287071 0.2634968  0.21145962 0.26992923]\n",
      " [0.20287071 0.2634968  0.21145962 0.26992923]\n",
      " [0.20287071 0.2634968  0.21145962 0.26992923]]\n",
      "[[0.7291667  0.         1.         0.20588236]\n",
      " [0.27083334 0.11764706 0.9375     0.6764706 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.625      0.7058824  0.6666667  0.9705882 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.5416667  0.7058824  0.6041667  1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.4117647  0.27083334 0.5882353 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.5179163  0.47364503 0.5369775  0.48441792]\n",
      " [0.48417473 0.47998077 0.53419656 0.5106224 ]\n",
      " [0.4633362  0.4757798  0.46439603 0.47643855]\n",
      " [0.46333623 0.47577986 0.4643961  0.47643864]\n",
      " [0.4633362  0.4757798  0.46439603 0.47643855]\n",
      " [0.50745916 0.5107449  0.5156509  0.5238052 ]\n",
      " [0.46333635 0.47578034 0.46439648 0.47643927]\n",
      " [0.49912977 0.51142234 0.50980645 0.526174  ]\n",
      " [0.46333623 0.47577983 0.46439606 0.47643858]\n",
      " [0.46320495 0.49215457 0.48045495 0.5039878 ]\n",
      " [0.4633362  0.47577977 0.46439603 0.47643852]\n",
      " [0.4633362  0.47577977 0.46439603 0.47643852]\n",
      " [0.4633362  0.47577977 0.46439603 0.47643852]\n",
      " [0.4633362  0.47577977 0.46439603 0.47643852]\n",
      " [0.4633362  0.47577977 0.46439603 0.47643852]\n",
      " [0.4633362  0.47577977 0.46439603 0.47643852]]\n",
      "[[0.        0.        1.        0.8443983]\n",
      " [0.        0.        0.        0.       ]\n",
      " [0.579288  0.5435685 0.9579288 0.9439834]\n",
      " [0.        0.        0.        0.       ]\n",
      " [0.        0.        0.        0.       ]\n",
      " [0.7022654 0.8651452 0.9288026 1.       ]\n",
      " [0.        0.        0.        0.       ]\n",
      " [0.        0.        0.        0.       ]\n",
      " [0.        0.        0.        0.       ]\n",
      " [0.        0.        0.        0.       ]\n",
      " [0.        0.        0.        0.       ]\n",
      " [0.        0.        0.        0.       ]\n",
      " [0.        0.        0.        0.       ]\n",
      " [0.        0.        0.        0.       ]\n",
      " [0.        0.        0.        0.       ]\n",
      " [0.        0.        0.        0.       ]] [[0.27511647 0.1449217  0.70660126 0.72612613]\n",
      " [0.3458518  0.20018625 0.49367464 0.39963514]\n",
      " [0.57588637 0.43522704 0.6938518  0.5921867 ]\n",
      " [0.2760038  0.17609428 0.28356713 0.18604808]\n",
      " [0.27549094 0.17339782 0.28198227 0.18228307]\n",
      " [0.5761733  0.7576048  0.66291296 0.8798728 ]\n",
      " [0.27549034 0.17339253 0.28198022 0.18227586]\n",
      " [0.2755126  0.17359686 0.2820797  0.18257782]\n",
      " [0.2754908  0.17339665 0.28198183 0.18228146]\n",
      " [0.2754903  0.17339204 0.28198004 0.18227519]\n",
      " [0.2754903  0.17339204 0.28198004 0.18227519]\n",
      " [0.2754903  0.17339204 0.28198004 0.18227519]\n",
      " [0.2754903  0.17339206 0.28198004 0.1822752 ]\n",
      " [0.2754903  0.17339204 0.28198004 0.18227519]\n",
      " [0.2754903  0.17339204 0.28198004 0.18227519]\n",
      " [0.2754903  0.17339204 0.28198004 0.18227519]]\n",
      "[[0.30530974 0.         0.99557525 0.3340708 ]\n",
      " [0.05752213 0.29646018 1.         0.88053095]\n",
      " [0.34513274 0.26327434 0.94690263 0.38053098]\n",
      " [0.48230088 0.50442475 0.86725664 0.99336284]\n",
      " [0.53539824 0.92477876 0.73893803 0.9712389 ]\n",
      " [0.29646018 0.4800885  0.579646   1.        ]\n",
      " [0.33628318 0.9159292  0.539823   0.9712389 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.7123894  0.8495575  0.95132744]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.42554754 0.15764448 0.6558948  0.37017053]\n",
      " [0.3435632  0.36298037 0.667866   0.78942454]\n",
      " [0.44467598 0.321043   0.66261226 0.42955342]\n",
      " [0.49574685 0.48520353 0.6364367  0.8197234 ]\n",
      " [0.511435   0.7449069  0.57536256 0.8208302 ]\n",
      " [0.42553985 0.46312946 0.57369256 0.8300303 ]\n",
      " [0.4563579  0.75602466 0.5219541  0.8465727 ]\n",
      " [0.33158526 0.18586858 0.33645383 0.19441299]\n",
      " [0.3315852  0.18586788 0.33645362 0.19441205]\n",
      " [0.3315854  0.18587042 0.3364544  0.19441551]\n",
      " [0.33158532 0.1858693  0.33645403 0.19441397]\n",
      " [0.34143087 0.6816386  0.5942452  0.8448566 ]\n",
      " [0.3315851  0.18586706 0.33645338 0.19441092]\n",
      " [0.3315851  0.18586703 0.33645335 0.19441088]\n",
      " [0.3315851  0.18586703 0.33645335 0.19441088]\n",
      " [0.3315851  0.18586703 0.33645335 0.19441088]]\n",
      "[[0.38644066 0.         0.9728814  0.43811882]\n",
      " [0.01016949 0.37623763 1.         0.86633664]\n",
      " [0.41694915 0.32673267 0.9898305  0.54950494]\n",
      " [0.6576271  0.64356434 0.9627119  1.        ]\n",
      " [0.67118645 0.92574257 0.8305085  0.99752474]\n",
      " [0.38305086 0.52970296 0.69491524 0.92574257]\n",
      " [0.49830508 0.7846535  0.69491524 0.9331683 ]\n",
      " [0.47457626 0.8613861  0.6745763  0.9727723 ]\n",
      " [0.46779662 0.85643566 0.6644068  0.9628713 ]\n",
      " [0.16271186 0.86633664 0.4        0.980198  ]\n",
      " [0.13220339 0.8688119  0.36949152 0.96039605]\n",
      " [0.         0.7747525  0.579661   0.980198  ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.4363081  0.19532281 0.71962976 0.44682378]\n",
      " [0.29474205 0.4194947  0.6957104  0.73670185]\n",
      " [0.44972992 0.35895583 0.6805104  0.45559323]\n",
      " [0.587721   0.61449325 0.7063662  0.7943316 ]\n",
      " [0.56968224 0.7359135  0.63417286 0.81292576]\n",
      " [0.44021642 0.5247994  0.60425603 0.77822936]\n",
      " [0.50523055 0.70614374 0.6010057  0.7772297 ]\n",
      " [0.46440992 0.7081888  0.5871045  0.80896425]\n",
      " [0.50299525 0.706669   0.5906755  0.77568555]\n",
      " [0.35442415 0.6870683  0.4835863  0.7783454 ]\n",
      " [0.3657002  0.72515744 0.4557296  0.78526485]\n",
      " [0.29558274 0.6633736  0.56534815 0.7896554 ]\n",
      " [0.28016642 0.21922645 0.28652105 0.22686306]\n",
      " [0.28016642 0.21922626 0.286521   0.22686279]\n",
      " [0.28016642 0.21922626 0.286521   0.22686279]\n",
      " [0.28016642 0.21922626 0.286521   0.22686279]]\n",
      "[[0.         0.         0.8093525  0.5218391 ]\n",
      " [0.30935252 0.14022988 1.         0.99310344]\n",
      " [0.73741007 0.12183908 0.9496403  0.41609195]\n",
      " [0.72302157 0.7218391  0.9352518  1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.26618704 0.7770115  0.4532374  0.97471267]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.00719424 0.10804598 0.28417265 0.35172415]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.29278347 0.17046495 0.6398616  0.5029134 ]\n",
      " [0.4577732  0.23266807 0.7269175  0.79030126]\n",
      " [0.52968127 0.25048366 0.70691544 0.45078087]\n",
      " [0.59819597 0.6351533  0.6724553  0.8578854 ]\n",
      " [0.2928349  0.19768262 0.29882354 0.20590544]\n",
      " [0.46155408 0.6425203  0.5130155  0.7953778 ]\n",
      " [0.29283488 0.19768214 0.29882336 0.2059048 ]\n",
      " [0.29283482 0.19768178 0.2988232  0.2059043 ]\n",
      " [0.29283482 0.19768175 0.2988232  0.20590426]\n",
      " [0.29283625 0.19769488 0.2988282  0.2059221 ]\n",
      " [0.29283482 0.19768177 0.2988232  0.20590429]\n",
      " [0.29283485 0.19768189 0.29882324 0.20590445]\n",
      " [0.32463926 0.23962097 0.4462871  0.43496358]\n",
      " [0.29283482 0.19768175 0.2988232  0.20590426]\n",
      " [0.29283482 0.19768175 0.2988232  0.20590426]\n",
      " [0.29283482 0.19768175 0.2988232  0.20590426]]\n",
      "[[0.04724409 0.15445027 1.         0.7513089 ]\n",
      " [0.07480315 0.46335077 0.88188976 0.87434554]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.53937006 0.8376963  0.79527557 1.        ]\n",
      " [0.54330707 0.87172776 0.77165353 0.9973822 ]\n",
      " [0.01968504 0.66492146 0.26377952 0.9685864 ]\n",
      " [0.01574803 0.85863876 0.25984251 0.9685864 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.32984293 0.08267716 0.5445026 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.10236221 0.         0.46456692 0.20418848]\n",
      " [0.2913386  0.473822   0.86614174 0.7486911 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.33874255 0.25625908 0.68606806 0.6633791 ]\n",
      " [0.31885442 0.5225659  0.6769519  0.78770334]\n",
      " [0.30598754 0.22787885 0.31159574 0.23528007]\n",
      " [0.5510836  0.672342   0.658082   0.76921326]\n",
      " [0.53478706 0.70762503 0.6229806  0.776653  ]\n",
      " [0.32254335 0.62998664 0.41917983 0.7848002 ]\n",
      " [0.33263865 0.741768   0.41045707 0.8097262 ]\n",
      " [0.30601728 0.22816354 0.31170896 0.23566142]\n",
      " [0.3059876  0.22787929 0.3115959  0.23528065]\n",
      " [0.30521262 0.27839184 0.32464293 0.31288564]\n",
      " [0.305991   0.22791016 0.3116081  0.23532265]\n",
      " [0.30594358 0.208958   0.4165241  0.27027816]\n",
      " [0.49161908 0.43414396 0.6693952  0.59291756]\n",
      " [0.30598754 0.22787882 0.3115957  0.23528004]\n",
      " [0.30598754 0.22787882 0.3115957  0.23528004]\n",
      " [0.30598754 0.22787882 0.3115957  0.23528004]]\n",
      "[[0.         0.5882353  0.26940638 1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.12785389 0.         0.96803653 0.7205882 ]\n",
      " [0.11872146 0.31617647 0.31506848 0.6764706 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.94520545 0.125      1.         0.27941176]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.32998243 0.512701   0.39413    0.6008824 ]\n",
      " [0.33272487 0.40312976 0.33756766 0.4057686 ]\n",
      " [0.3327216  0.40311927 0.33755708 0.4057543 ]\n",
      " [0.3532401  0.39618176 0.6688308  0.5639287 ]\n",
      " [0.36312696 0.44899583 0.43793878 0.54045844]\n",
      " [0.33272156 0.40311915 0.33755696 0.40575412]\n",
      " [0.33272156 0.40311915 0.33755696 0.40575412]\n",
      " [0.33272156 0.40311915 0.33755696 0.40575412]\n",
      " [0.33272156 0.40311918 0.337557   0.40575415]\n",
      " [0.33272156 0.40311915 0.33755696 0.40575412]\n",
      " [0.33272156 0.40311915 0.33755696 0.40575412]\n",
      " [0.33272156 0.4031192  0.337557   0.4057542 ]\n",
      " [0.33272156 0.40311915 0.33755696 0.40575415]\n",
      " [0.6289164  0.4164081  0.67874914 0.44708952]\n",
      " [0.33272156 0.40311915 0.33755696 0.40575412]\n",
      " [0.33272156 0.40311915 0.33755696 0.40575412]]\n",
      "[[0.5089286  0.2256532  0.98214287 0.5914489 ]\n",
      " [0.         0.         0.83035713 0.9976247 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.34375    0.41805226 0.97321427 0.74821854]\n",
      " [0.8125     0.6745843  0.96428573 0.7553444 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.12053572 0.72921616 1.         1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.65625    0.4513064  0.9910714  0.57719713]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.49251133 0.21450764 0.63838863 0.5643155 ]\n",
      " [0.3425573  0.2367788  0.6255609  0.6690455 ]\n",
      " [0.34142193 0.2220432  0.34600636 0.229604  ]\n",
      " [0.34142172 0.22204098 0.34600565 0.22960098]\n",
      " [0.34142172 0.22204095 0.34600565 0.22960094]\n",
      " [0.47162277 0.50023156 0.64162195 0.71074533]\n",
      " [0.56644696 0.592518   0.6332238  0.6988491 ]\n",
      " [0.34147137 0.22261351 0.34621233 0.23038465]\n",
      " [0.34142172 0.22204095 0.34600565 0.22960094]\n",
      " [0.39269227 0.5411323  0.5621514  0.973459  ]\n",
      " [0.3414436  0.22221345 0.34606463 0.22983174]\n",
      " [0.34142202 0.22204433 0.34600672 0.22960556]\n",
      " [0.6102904  0.4924639  0.68796015 0.61589694]\n",
      " [0.34142172 0.22204094 0.34600562 0.22960092]\n",
      " [0.34142172 0.22204094 0.34600562 0.22960092]\n",
      " [0.34142172 0.22204094 0.34600562 0.22960092]]\n",
      "[[0.58943087 0.         1.         0.16285715]\n",
      " [0.06910569 0.05428571 0.6910569  0.73142856]\n",
      " [0.4796748  0.01428571 0.6869919  0.4457143 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.07723577 0.73714286 0.23170732 1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.33739838 0.66571426 0.51626015 0.9942857 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.6257143  0.23170732 0.82285714]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.54099596 0.2283957  0.6845032  0.332739  ]\n",
      " [0.33430448 0.25588322 0.5734184  0.64363766]\n",
      " [0.47977024 0.24669787 0.5732987  0.4776239 ]\n",
      " [0.31209818 0.25067437 0.31752974 0.25745562]\n",
      " [0.31209946 0.2506852  0.31753436 0.25747046]\n",
      " [0.34889483 0.60885674 0.39927167 0.7870581 ]\n",
      " [0.31209823 0.2506749  0.31752995 0.25745633]\n",
      " [0.45969826 0.599344   0.51843226 0.7977257 ]\n",
      " [0.31209826 0.25067517 0.31753007 0.25745672]\n",
      " [0.3129833  0.5808971  0.4019365  0.6761062 ]\n",
      " [0.31209815 0.2506743  0.3175297  0.2574555 ]\n",
      " [0.31209815 0.2506743  0.3175297  0.2574555 ]\n",
      " [0.31209815 0.2506743  0.3175297  0.2574555 ]\n",
      " [0.31209815 0.2506743  0.3175297  0.2574555 ]\n",
      " [0.31209815 0.2506743  0.3175297  0.2574555 ]\n",
      " [0.31209815 0.2506743  0.3175297  0.2574555 ]]\n",
      "[[0.         0.         0.7619048  0.3372781 ]\n",
      " [0.33333334 0.11242604 1.         0.65088755]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.4857143  0.56213015 0.84761906 1.        ]\n",
      " [0.4952381  0.8994083  0.6666667  0.99408287]\n",
      " [0.26666668 0.49704143 0.552381   0.8698225 ]\n",
      " [0.26666668 0.74556214 0.47619048 0.8639053 ]\n",
      " [0.7904762  0.6627219  0.96190476 0.9526627 ]\n",
      " [0.8        0.8579882  0.9238095  0.964497  ]\n",
      " [0.53333336 0.6745562  0.6857143  0.79881656]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.01904762 0.11242604 0.32380953 0.31360948]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.4249328  0.3783959  0.527627   0.4602451 ]\n",
      " [0.47278818 0.40490472 0.5702908  0.5360341 ]\n",
      " [0.42567796 0.38849562 0.4278529  0.39156222]\n",
      " [0.5022814  0.49054682 0.5451496  0.6238951 ]\n",
      " [0.49898952 0.58295375 0.5263984  0.61764413]\n",
      " [0.47532654 0.4933248  0.5094739  0.5880637 ]\n",
      " [0.46918648 0.56483257 0.49188358 0.5906369 ]\n",
      " [0.54341197 0.5339706  0.57432044 0.6268235 ]\n",
      " [0.55086446 0.58829486 0.5676178  0.60812694]\n",
      " [0.49194348 0.5381862  0.51204604 0.5792212 ]\n",
      " [0.4256677  0.38843128 0.4278193  0.39146996]\n",
      " [0.42566723 0.3884274  0.4278179  0.3914648 ]\n",
      " [0.43777302 0.41210058 0.48027325 0.4546109 ]\n",
      " [0.42566645 0.38842022 0.42781517 0.391455  ]\n",
      " [0.42566645 0.38842022 0.42781517 0.391455  ]\n",
      " [0.42566645 0.38842022 0.42781517 0.391455  ]]\n",
      "[[0.2596154  0.         1.         0.500994  ]\n",
      " [0.03846154 0.2743539  0.7051282  0.8190855 ]\n",
      " [0.14423077 0.25646123 0.69871795 0.5924453 ]\n",
      " [0.45833334 0.6023857  0.7307692  0.99801195]\n",
      " [0.4903846  0.8807157  0.724359   1.        ]\n",
      " [0.02564103 0.6163022  0.14423077 0.972167  ]\n",
      " [0.         0.8489066  0.11538462 0.96819085]\n",
      " [0.6217949  0.77932405 0.7307692  0.85685885]\n",
      " [0.625      0.7673956  0.73397434 0.8667992 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.34020862 0.1095521  0.735253   0.5195257 ]\n",
      " [0.2762518  0.27968514 0.6532116  0.7099521 ]\n",
      " [0.3583961  0.30824673 0.6248046  0.4693125 ]\n",
      " [0.4522682  0.5867709  0.6072056  0.88821363]\n",
      " [0.49305025 0.79633486 0.60438836 0.87838113]\n",
      " [0.2714985  0.5066616  0.38033605 0.8843806 ]\n",
      " [0.27030057 0.80180526 0.34511483 0.88523304]\n",
      " [0.53373563 0.73886514 0.6039084  0.8425442 ]\n",
      " [0.54677963 0.7535777  0.6163016  0.8215754 ]\n",
      " [0.2616865  0.1416931  0.2685779  0.15144217]\n",
      " [0.2616855  0.14168337 0.2685743  0.15142894]\n",
      " [0.26168555 0.14168376 0.26857442 0.15142946]\n",
      " [0.2616855  0.14168338 0.2685743  0.15142897]\n",
      " [0.2616855  0.14168335 0.26857427 0.15142892]\n",
      " [0.2616855  0.14168335 0.26857427 0.15142892]\n",
      " [0.2616855  0.14168335 0.26857427 0.15142892]]\n",
      "[[0.01510574 0.         1.         0.876     ]\n",
      " [0.         0.444      0.5800604  0.98      ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.61027193 0.764      0.7824773  0.976     ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.30513597 0.848      0.652568   1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.3021148  0.472      0.5740181  0.84      ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.2695413  0.30123916 0.7411181  0.651948  ]\n",
      " [0.24646302 0.49804348 0.54462075 0.6693528 ]\n",
      " [0.24100626 0.31756657 0.2484928  0.32252845]\n",
      " [0.60261905 0.5867876  0.7179713  0.66898835]\n",
      " [0.2410063  0.31756678 0.24849296 0.32252872]\n",
      " [0.3942706  0.6175795  0.5279777  0.7052244 ]\n",
      " [0.2410097  0.31758198 0.24850498 0.32254943]\n",
      " [0.24100626 0.3175666  0.24849282 0.32252848]\n",
      " [0.24100626 0.31756657 0.2484928  0.32252842]\n",
      " [0.24100627 0.31756663 0.24849285 0.3225285 ]\n",
      " [0.24100626 0.31756657 0.2484928  0.32252842]\n",
      " [0.24100626 0.31756657 0.2484928  0.32252842]\n",
      " [0.42476714 0.45664722 0.58616644 0.6101682 ]\n",
      " [0.24100626 0.31756657 0.2484928  0.32252842]\n",
      " [0.24100626 0.31756657 0.2484928  0.32252842]\n",
      " [0.24100626 0.31756657 0.2484928  0.32252842]]\n",
      "[[0.2777778  0.         0.6736111  0.23972602]\n",
      " [0.08333334 0.21232876 0.9236111  0.8938356 ]\n",
      " [0.35416666 0.21232876 0.5833333  0.30136988]\n",
      " [0.5625     0.57191783 0.9652778  0.7979452 ]\n",
      " [0.7708333  0.28082192 1.         0.6130137 ]\n",
      " [0.3402778  0.7431507  0.6597222  0.85958904]\n",
      " [0.00694444 0.5753425  0.4513889  0.78424656]\n",
      " [0.         0.28082192 0.19444445 0.6232877 ]\n",
      " [0.4097222  0.739726   0.7222222  0.8664383 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.45833334 0.8458904  0.9027778  1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.09722222 0.8390411  0.5138889  0.989726  ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.46694785 0.26788336 0.55100155 0.37883976]\n",
      " [0.4027241  0.371052   0.59380937 0.67305696]\n",
      " [0.48087275 0.3593676  0.53984535 0.40213406]\n",
      " [0.5214821  0.542238   0.61639106 0.64327055]\n",
      " [0.5510925  0.4128238  0.6049213  0.552433  ]\n",
      " [0.4665408  0.5533639  0.5281243  0.6290699 ]\n",
      " [0.38845408 0.5528302  0.48337257 0.671155  ]\n",
      " [0.39487335 0.4237644  0.43568355 0.5804479 ]\n",
      " [0.45526248 0.5794056  0.5154542  0.647913  ]\n",
      " [0.38732603 0.2869183  0.39058313 0.292714  ]\n",
      " [0.4779057  0.6162786  0.58194757 0.7393925 ]\n",
      " [0.38732597 0.28691775 0.39058295 0.2927132 ]\n",
      " [0.38732636 0.28692245 0.39058435 0.2927196 ]\n",
      " [0.39590004 0.6209049  0.4943418  0.70354694]\n",
      " [0.38732597 0.28691775 0.39058295 0.2927132 ]\n",
      " [0.38732597 0.28691775 0.39058295 0.2927132 ]]\n",
      "[[0.6413044  0.         1.         0.22818792]\n",
      " [0.         0.2147651  0.84782606 1.        ]\n",
      " [0.61413044 0.12751678 0.82608694 0.2885906 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.55370265 0.38149205 0.64220834 0.44269207]\n",
      " [0.3561334  0.4298954  0.6027271  0.6194966 ]\n",
      " [0.52116334 0.41808242 0.6059238  0.46413633]\n",
      " [0.35602766 0.39126965 0.36018938 0.39422694]\n",
      " [0.35602766 0.39126968 0.36018938 0.39422697]\n",
      " [0.35602766 0.39126965 0.36018938 0.39422694]\n",
      " [0.35602766 0.39126965 0.36018938 0.39422694]\n",
      " [0.35602766 0.39126965 0.36018938 0.39422694]\n",
      " [0.35602766 0.39126968 0.36018938 0.39422694]\n",
      " [0.3560279  0.39127088 0.36019027 0.3942286 ]\n",
      " [0.35602766 0.39126965 0.36018938 0.39422694]\n",
      " [0.35602766 0.39126965 0.36018938 0.39422694]\n",
      " [0.35602766 0.39126968 0.36018938 0.39422694]\n",
      " [0.35602766 0.39126965 0.36018938 0.39422694]\n",
      " [0.35602766 0.39126965 0.36018938 0.39422694]\n",
      " [0.35602766 0.39126965 0.36018938 0.39422694]]\n",
      "[[0.46706587 0.         0.8023952  0.23579545]\n",
      " [0.23353294 0.1590909  0.8622754  0.87215906]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.83233535 0.4943182  1.         0.6818182 ]\n",
      " [0.8023952  0.26988637 0.9700599  0.54829544]\n",
      " [0.8203593  0.65340906 0.98203593 0.82102275]\n",
      " [0.         0.53409094 0.28742516 0.74715906]\n",
      " [0.03592815 0.2528409  0.4011976  0.5823864 ]\n",
      " [0.07185629 0.69602275 0.2994012  0.86647725]\n",
      " [0.44311377 0.92045456 0.61676645 1.        ]\n",
      " [0.4790419  0.85227275 0.6706587  0.9375    ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.28143713 0.9375     0.49101797 0.99715906]\n",
      " [0.28742516 0.8380682  0.52095807 0.9630682 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.5084094  0.2335268  0.5792935  0.35291246]\n",
      " [0.40881026 0.303035   0.6088612  0.6418507 ]\n",
      " [0.37555268 0.25537556 0.37915212 0.26203287]\n",
      " [0.5775632  0.45664307 0.6234671  0.5637866 ]\n",
      " [0.5676527  0.38817778 0.6132262  0.5324646 ]\n",
      " [0.59034646 0.5662898  0.641049   0.6057632 ]\n",
      " [0.38865057 0.4847392  0.43572563 0.6199155 ]\n",
      " [0.3915713  0.37393457 0.46458998 0.5192982 ]\n",
      " [0.39150637 0.6166502  0.42716286 0.6913332 ]\n",
      " [0.50744706 0.7055106  0.55998236 0.7895002 ]\n",
      " [0.48558196 0.64444697 0.5479085  0.7768798 ]\n",
      " [0.37555218 0.25536978 0.37915042 0.26202497]\n",
      " [0.44056344 0.6992419  0.4944229  0.77609736]\n",
      " [0.40889207 0.6273894  0.5092804  0.7321408 ]\n",
      " [0.3755529  0.2553788  0.37915295 0.26203722]\n",
      " [0.37555185 0.25536546 0.37914917 0.2620191 ]]\n",
      "[[0.13783784 0.         0.8918919  0.70542634]\n",
      " [0.         0.04909561 1.         1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.68378377 0.7906977  0.8810811  0.95348835]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.56216216 0.83979326 0.68378377 0.96640825]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.28583783 0.20282178 0.7342025  0.6279199 ]\n",
      " [0.22184896 0.24355258 0.7557999  0.7763171 ]\n",
      " [0.21738346 0.22431707 0.22555287 0.23181517]\n",
      " [0.61969686 0.6656051  0.7667988  0.80726194]\n",
      " [0.21738356 0.22431764 0.22555321 0.23181595]\n",
      " [0.5457468  0.71272206 0.7028133  0.79779845]\n",
      " [0.21738358 0.2243178  0.2255533  0.23181617]\n",
      " [0.21738346 0.22431703 0.22555286 0.2318151 ]\n",
      " [0.21738346 0.22431701 0.22555284 0.2318151 ]\n",
      " [0.21738346 0.22431706 0.22555287 0.23181516]\n",
      " [0.21738346 0.22431704 0.22555286 0.23181513]\n",
      " [0.21738346 0.22431706 0.22555287 0.23181516]\n",
      " [0.21738347 0.22431715 0.22555292 0.23181528]\n",
      " [0.21738346 0.22431701 0.22555284 0.2318151 ]\n",
      " [0.21738346 0.22431701 0.22555284 0.2318151 ]\n",
      " [0.21738346 0.22431701 0.22555284 0.2318151 ]]\n",
      "[[0.21374045 0.         1.         0.52140075]\n",
      " [0.         0.2782101  0.78625953 0.81322956]\n",
      " [0.03816794 0.2859922  0.7442748  0.5856031 ]\n",
      " [0.5038168  0.5933852  0.9045802  0.9766537 ]\n",
      " [0.63740456 0.8599222  0.9351145  0.9747082 ]\n",
      " [0.01526718 0.651751   0.19847329 1.        ]\n",
      " [0.05343511 0.8832685  0.20229007 1.        ]\n",
      " [0.759542   0.7529183  0.8854962  0.82684827]\n",
      " [0.7633588  0.7392996  0.9045802  0.8326848 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.3620701  0.13162982 0.68108034 0.54358625]\n",
      " [0.31878394 0.30341858 0.64218783 0.7044278 ]\n",
      " [0.36132205 0.33435738 0.6154102  0.49282357]\n",
      " [0.5217656  0.5914757  0.6453817  0.8749071 ]\n",
      " [0.54939246 0.7778539  0.65952086 0.8490906 ]\n",
      " [0.32450914 0.54475045 0.41329986 0.875322  ]\n",
      " [0.325661   0.79688007 0.4023783  0.87972105]\n",
      " [0.5827421  0.6764163  0.64259887 0.7787133 ]\n",
      " [0.6146014  0.69958776 0.67711884 0.76660204]\n",
      " [0.31452164 0.16065733 0.3198872  0.1698939 ]\n",
      " [0.3145201  0.16063908 0.31988162 0.1698691 ]\n",
      " [0.31452018 0.1606403  0.319882   0.16987076]\n",
      " [0.31452012 0.1606395  0.31988174 0.16986966]\n",
      " [0.3145201  0.16063906 0.31988162 0.16986907]\n",
      " [0.3145201  0.16063906 0.31988162 0.16986907]\n",
      " [0.3145201  0.16063906 0.31988162 0.16986907]]\n",
      "[[0.10561798 0.         1.         0.85620916]\n",
      " [0.         0.5925926  0.71910113 1.        ]\n",
      " [0.06966292 0.5925926  0.71910113 1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.2049014  0.16097492 0.8308934  0.75209785]\n",
      " [0.1753767  0.56794375 0.6525347  0.83727556]\n",
      " [0.20596834 0.549549   0.639305   0.81570625]\n",
      " [0.17667702 0.1889773  0.18602318 0.19743665]\n",
      " [0.17667697 0.18897706 0.18602304 0.19743633]\n",
      " [0.17667699 0.18897714 0.18602309 0.19743642]\n",
      " [0.17667699 0.18897708 0.18602306 0.19743635]\n",
      " [0.17667697 0.18897706 0.18602304 0.19743633]\n",
      " [0.17667697 0.18897706 0.18602304 0.19743633]\n",
      " [0.17667699 0.18897708 0.18602306 0.19743633]\n",
      " [0.17667697 0.18897706 0.18602304 0.19743633]\n",
      " [0.17667697 0.18897706 0.18602304 0.19743633]\n",
      " [0.17667699 0.18897712 0.18602309 0.1974364 ]\n",
      " [0.17667697 0.18897706 0.18602304 0.19743633]\n",
      " [0.17667697 0.18897706 0.18602304 0.19743633]\n",
      " [0.17667697 0.18897706 0.18602304 0.19743633]]\n",
      "[[0.         0.         0.6885246  0.8235294 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.20491803 0.3529412  1.         1.        ]\n",
      " [0.20491803 0.38235295 0.78688526 1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.41231194 0.4510822  0.53559136 0.52934647]\n",
      " [0.41363162 0.45510432 0.41612846 0.45632553]\n",
      " [0.4136315  0.455104   0.4161281  0.45632508]\n",
      " [0.44931388 0.48377505 0.58911717 0.5467768 ]\n",
      " [0.44838327 0.4866752  0.54690576 0.54493725]\n",
      " [0.4136315  0.455104   0.4161281  0.45632508]\n",
      " [0.4136315  0.455104   0.4161281  0.45632508]\n",
      " [0.4136315  0.455104   0.4161281  0.45632508]\n",
      " [0.4136315  0.455104   0.4161281  0.45632508]\n",
      " [0.4136315  0.455104   0.4161281  0.45632508]\n",
      " [0.41363153 0.455104   0.4161281  0.45632508]\n",
      " [0.4136315  0.455104   0.4161281  0.45632508]\n",
      " [0.4136315  0.455104   0.4161281  0.45632508]\n",
      " [0.4136315  0.455104   0.4161281  0.45632508]\n",
      " [0.4136315  0.455104   0.4161281  0.45632508]\n",
      " [0.4136315  0.455104   0.4161281  0.45632508]]\n",
      "[[0.7234042  0.         1.         0.20588236]\n",
      " [0.27659574 0.11764706 0.9574468  0.7352941 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.65957445 0.7352941  0.7234042  1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.5744681  0.7352941  0.63829786 1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.5        0.29787233 0.7058824 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.51492643 0.47555912 0.5334108  0.4856437 ]\n",
      " [0.48565623 0.48172924 0.53328425 0.5133161 ]\n",
      " [0.4667269  0.47755203 0.4676887  0.4781626 ]\n",
      " [0.4667269  0.47755206 0.4676887  0.47816262]\n",
      " [0.4667269  0.47755203 0.4676887  0.4781626 ]\n",
      " [0.5091887  0.5123287  0.51702076 0.5234033 ]\n",
      " [0.466727   0.4775525  0.4676891  0.47816324]\n",
      " [0.5025664  0.5121617  0.5115781  0.5238173 ]\n",
      " [0.4667269  0.47755206 0.4676887  0.4781626 ]\n",
      " [0.46681717 0.49806437 0.48493013 0.5110179 ]\n",
      " [0.4667269  0.47755203 0.46768868 0.47816256]\n",
      " [0.4667269  0.47755203 0.46768868 0.47816256]\n",
      " [0.4667269  0.47755203 0.46768868 0.47816256]\n",
      " [0.4667269  0.47755203 0.46768868 0.47816256]\n",
      " [0.4667269  0.47755203 0.46768868 0.47816256]\n",
      " [0.4667269  0.47755203 0.46768868 0.47816256]]\n",
      "[[0.3004926  0.         0.72906405 0.3488372 ]\n",
      " [0.22660099 0.22674419 0.89162564 0.76744187]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.45320198 0.65697676 0.6551724  1.        ]\n",
      " [0.44334975 0.872093   0.56650245 0.99418604]\n",
      " [0.         0.5116279  0.2955665  0.68604654]\n",
      " [0.         0.5755814  0.20197044 0.6976744 ]\n",
      " [0.74384236 0.5813953  0.9310345  0.872093  ]\n",
      " [0.8325123  0.73255813 0.92610836 0.8604651 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.86206895 0.5813953  1.         0.75      ]\n",
      " [0.38916257 0.07558139 0.6403941  0.29069766]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.43191075 0.37305024 0.56916004 0.4687683 ]\n",
      " [0.41299957 0.4355631  0.609838   0.58516455]\n",
      " [0.35251    0.38346672 0.35678187 0.38664243]\n",
      " [0.4698293  0.54177386 0.54800916 0.6232115 ]\n",
      " [0.48657355 0.58627063 0.5258545  0.6145721 ]\n",
      " [0.36104327 0.50676835 0.4313102  0.55787957]\n",
      " [0.3604526  0.52821195 0.40261906 0.55945086]\n",
      " [0.56885564 0.53062105 0.656277   0.5943625 ]\n",
      " [0.57298297 0.57246697 0.61902595 0.5964543 ]\n",
      " [0.35250664 0.38345137 0.35677028 0.3866214 ]\n",
      " [0.35250703 0.3834533  0.35677165 0.38662404]\n",
      " [0.5916134  0.5412582  0.633746   0.57492816]\n",
      " [0.48103133 0.41356623 0.5339461  0.46346015]\n",
      " [0.35250658 0.3834511  0.35677007 0.38662103]\n",
      " [0.35250658 0.3834511  0.35677007 0.38662103]\n",
      " [0.35250658 0.3834511  0.35677007 0.38662103]]\n",
      "[[0.02074689 0.09164421 1.         0.7277628 ]\n",
      " [0.10373444 0.49056605 0.9834025  0.8679245 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.7095436  0.78975743 0.9626556  0.967655  ]\n",
      " [0.71369296 0.8436658  0.9502075  0.967655  ]\n",
      " [0.12033195 0.6981132  0.41908714 0.99730456]\n",
      " [0.14522822 0.8814016  0.41493776 1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.3638814  0.10373444 0.5795148 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.01244813 0.         0.406639   0.21563342]\n",
      " [0.3360996  0.44204852 0.9294606  0.7277628 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.3408532  0.23924294 0.67610055 0.641805  ]\n",
      " [0.3321266  0.53611374 0.6739904  0.7744165 ]\n",
      " [0.32040715 0.24216081 0.3255985  0.24917358]\n",
      " [0.5918932  0.6467327  0.6876205  0.73949486]\n",
      " [0.5862456  0.6874033  0.6704085  0.76065844]\n",
      " [0.36018783 0.64606035 0.46048516 0.7806832 ]\n",
      " [0.39183307 0.7262598  0.46967965 0.7918798 ]\n",
      " [0.32045636 0.24265674 0.3257841  0.24982211]\n",
      " [0.3204072  0.24216147 0.32559878 0.24917449]\n",
      " [0.3245538  0.4341843  0.36654204 0.5328343 ]\n",
      " [0.3204118  0.24220453 0.32561523 0.24923293]\n",
      " [0.31755763 0.22026035 0.3334229  0.23274384]\n",
      " [0.47383368 0.41126615 0.6605728  0.57849467]\n",
      " [0.32040715 0.24216077 0.3255985  0.24917354]\n",
      " [0.32040715 0.24216077 0.3255985  0.24917354]\n",
      " [0.32040715 0.24216077 0.3255985  0.24917354]]\n",
      "[[0.36263737 0.         1.         0.7147436 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.23076923 0.6769231  1.        ]\n",
      " [0.46593407 0.37179488 0.6901099  0.875     ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.44403848 0.26958215 0.8570462  0.6066416 ]\n",
      " [0.16941132 0.28858575 0.17896745 0.29433587]\n",
      " [0.1694113  0.2885857  0.1789674  0.29433578]\n",
      " [0.16884044 0.36789602 0.609974   0.74241054]\n",
      " [0.51838464 0.42244837 0.661646   0.66543496]\n",
      " [0.1694113  0.2885857  0.17896739 0.29433578]\n",
      " [0.1694113  0.2885857  0.17896739 0.29433578]\n",
      " [0.1694113  0.2885857  0.17896739 0.29433578]\n",
      " [0.1694113  0.2885857  0.1789674  0.29433578]\n",
      " [0.1694113  0.2885857  0.17896739 0.29433578]\n",
      " [0.1694113  0.28858572 0.1789674  0.2943358 ]\n",
      " [0.1694113  0.2885857  0.17896739 0.29433578]\n",
      " [0.16941132 0.28858575 0.17896743 0.29433584]\n",
      " [0.1694113  0.2885857  0.17896739 0.29433578]\n",
      " [0.1694113  0.2885857  0.17896739 0.29433578]\n",
      " [0.1694113  0.2885857  0.17896739 0.29433578]]\n",
      "[[0.         0.         0.12840466 0.37089202]\n",
      " [0.11673152 0.86384976 0.15953307 0.943662  ]\n",
      " [0.24124514 0.943662   0.29571983 0.98122066]\n",
      " [0.08949416 0.04225352 0.8249027  0.63849765]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.09338521 0.57746476 0.24513619 0.69014084]\n",
      " [0.08949416 0.6713615  0.15175097 0.9389671 ]\n",
      " [0.26848248 0.6760563  0.32684824 0.80751175]\n",
      " [0.23735408 0.7887324  0.33463034 0.98122066]\n",
      " [0.61478597 0.5023474  0.78210115 0.7699531 ]\n",
      " [0.66536963 0.7652582  0.7859922  1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.692607   0.7793427  0.74319065 0.9389671 ]\n",
      " [0.770428   0.31455398 1.         0.7605634 ]\n",
      " [0.6614786  0.9624413  0.71984434 1.        ]\n",
      " [0.7003891  0.9061033  0.7237354  0.9389671 ]] [[0.29563537 0.33112144 0.38317284 0.48602927]\n",
      " [0.34625882 0.6026684  0.363526   0.6334974 ]\n",
      " [0.3703175  0.6293257  0.39779752 0.6625228 ]\n",
      " [0.33869135 0.34426403 0.6302396  0.5448912 ]\n",
      " [0.29951033 0.34300995 0.31727627 0.36751965]\n",
      " [0.3452095  0.49877697 0.4005144  0.54931474]\n",
      " [0.33709708 0.57070136 0.3677893  0.6323014 ]\n",
      " [0.39455628 0.5270179  0.43126118 0.58444357]\n",
      " [0.37932935 0.5785963  0.41384336 0.6396533 ]\n",
      " [0.5423003  0.5300664  0.6105702  0.60001266]\n",
      " [0.5493963  0.60014784 0.5978639  0.6671539 ]\n",
      " [0.29890868 0.3445692  0.3047228  0.34879762]\n",
      " [0.5646948  0.58342576 0.6096437  0.65250134]\n",
      " [0.625842   0.40392894 0.7127978  0.5171395 ]\n",
      " [0.5459442  0.64941716 0.56820494 0.66790605]\n",
      " [0.58838505 0.6269959  0.61496395 0.6467962 ]]\n",
      "[[0.         0.         0.12840466 0.37089202]\n",
      " [0.11673152 0.86384976 0.15953307 0.943662  ]\n",
      " [0.24124514 0.943662   0.29571983 0.98122066]\n",
      " [0.08949416 0.04225352 0.8249027  0.63849765]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.09338521 0.57746476 0.24513619 0.69014084]\n",
      " [0.08949416 0.6713615  0.15175097 0.9389671 ]\n",
      " [0.26848248 0.6760563  0.32684824 0.80751175]\n",
      " [0.23735408 0.7887324  0.33463034 0.98122066]\n",
      " [0.61478597 0.5023474  0.78210115 0.7699531 ]\n",
      " [0.66536963 0.7652582  0.7859922  1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.692607   0.7793427  0.74319065 0.9389671 ]\n",
      " [0.770428   0.31455398 1.         0.7605634 ]\n",
      " [0.6614786  0.9624413  0.71984434 1.        ]\n",
      " [0.7003891  0.9061033  0.7237354  0.9389671 ]] [[0.31023318 0.34318835 0.3915968  0.48690778]\n",
      " [0.35732886 0.5951916  0.37337774 0.62373424]\n",
      " [0.37972212 0.6200954  0.40527853 0.6508478 ]\n",
      " [0.35006338 0.35538185 0.62107277 0.54130876]\n",
      " [0.31379834 0.3543107  0.32995167 0.37640566]\n",
      " [0.35626712 0.4990434  0.40755224 0.5459483 ]\n",
      " [0.3487294  0.56576824 0.377231   0.62301344]\n",
      " [0.4019778  0.5251807  0.43598467 0.57853794]\n",
      " [0.38788158 0.57312953 0.419936   0.62987435]\n",
      " [0.53903687 0.52805865 0.60233474 0.59284455]\n",
      " [0.5454614  0.593081   0.59044296 0.6553211 ]\n",
      " [0.31327236 0.3556714  0.3186712  0.3595978 ]\n",
      " [0.56004447 0.5774539  0.6018369  0.6416198 ]\n",
      " [0.6166256  0.4105353  0.697562   0.5154162 ]\n",
      " [0.5422246  0.6387973  0.5629038  0.6559925 ]\n",
      " [0.58168435 0.61805165 0.6064023  0.6364323 ]]\n",
      "[[0.06896552 0.         0.9396552  0.5824561 ]\n",
      " [0.03448276 0.39298245 0.9741379  0.79298246]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.54310346 0.5614035  1.         0.9157895 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.6421053  0.57758623 1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.3616883  0.2871913  0.6282768  0.5048957 ]\n",
      " [0.34039676 0.4665559  0.6694275  0.68480307]\n",
      " [0.32711408 0.30193055 0.33211195 0.3073181 ]\n",
      " [0.5529498  0.56533986 0.6811571  0.6941346 ]\n",
      " [0.32711416 0.30193114 0.33211225 0.30731893]\n",
      " [0.36941022 0.56815565 0.52715766 0.7236443 ]\n",
      " [0.32711896 0.30196786 0.33212972 0.30736867]\n",
      " [0.32711393 0.3019295  0.33211145 0.3073167 ]\n",
      " [0.32711393 0.30192944 0.33211142 0.3073166 ]\n",
      " [0.32711524 0.3019389  0.33211607 0.30732948]\n",
      " [0.32711393 0.30192944 0.33211142 0.3073166 ]\n",
      " [0.32711396 0.30192962 0.3321115  0.30731687]\n",
      " [0.327114   0.30192977 0.3321116  0.30731708]\n",
      " [0.32711393 0.30192944 0.33211142 0.3073166 ]\n",
      " [0.32711393 0.30192944 0.33211142 0.3073166 ]\n",
      " [0.32711393 0.30192944 0.33211142 0.3073166 ]]\n",
      "[[0.         0.         0.30493274 0.352     ]\n",
      " [0.13452914 0.016      0.30044842 0.352     ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.11659193 0.08       1.         1.        ]\n",
      " [0.12107623 0.088      0.41255605 0.752     ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.33123684 0.40535033 0.4163675  0.46650556]\n",
      " [0.3411314  0.4074375  0.3676139  0.43491286]\n",
      " [0.3338208  0.41312715 0.3386246  0.41549003]\n",
      " [0.37470052 0.4258714  0.6603861  0.6026166 ]\n",
      " [0.364515   0.43278974 0.4404864  0.5423009 ]\n",
      " [0.3338207  0.41312692 0.33862436 0.4154897 ]\n",
      " [0.3338207  0.41312692 0.33862433 0.4154897 ]\n",
      " [0.3338207  0.41312692 0.33862433 0.4154897 ]\n",
      " [0.3338207  0.41312692 0.33862433 0.4154897 ]\n",
      " [0.3338207  0.41312692 0.33862433 0.4154897 ]\n",
      " [0.3338207  0.41312692 0.33862433 0.4154897 ]\n",
      " [0.3338207  0.41312692 0.33862433 0.4154897 ]\n",
      " [0.3338207  0.41312692 0.33862433 0.4154897 ]\n",
      " [0.3338207  0.41312692 0.33862433 0.4154897 ]\n",
      " [0.3338207  0.41312692 0.33862433 0.4154897 ]\n",
      " [0.3338207  0.41312692 0.33862433 0.4154897 ]]\n",
      "[[0.00677966 0.         0.43728814 0.43421054]\n",
      " [0.         0.14144737 0.99661016 0.67763156]\n",
      " [0.00338983 0.35855263 0.30847457 0.5065789 ]\n",
      " [0.4779661  0.47697368 0.66101694 0.7730263 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.25762713 0.6381579  0.43728814 0.875     ]\n",
      " [0.32542372 0.7763158  0.43728814 0.875     ]\n",
      " [0.45762712 0.57236844 0.9050847  0.8618421 ]\n",
      " [0.45762712 0.7894737  0.52881354 0.8618421 ]\n",
      " [0.33559322 0.6940789  0.5423729  0.7927632 ]\n",
      " [0.33559322 0.6940789  0.4779661  0.7927632 ]\n",
      " [0.6576271  0.50986844 1.         1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.2822403  0.28565365 0.45451647 0.47218174]\n",
      " [0.29282656 0.32845217 0.68371665 0.6076311 ]\n",
      " [0.31471094 0.38641563 0.42880017 0.5022894 ]\n",
      " [0.45316777 0.49351084 0.5540318  0.62380373]\n",
      " [0.28566426 0.29401857 0.291865   0.29962558]\n",
      " [0.40382555 0.5415562  0.48607105 0.6589472 ]\n",
      " [0.45249093 0.6154593  0.5002452  0.6666064 ]\n",
      " [0.48025084 0.4843784  0.64680547 0.6459716 ]\n",
      " [0.5219905  0.60533965 0.57064426 0.6625445 ]\n",
      " [0.39389417 0.564671   0.48848963 0.65062237]\n",
      " [0.39230615 0.55315727 0.43004236 0.602291  ]\n",
      " [0.6021038  0.5152917  0.7713369  0.67832744]\n",
      " [0.28566366 0.29401496 0.2918629  0.29962066]\n",
      " [0.2856623  0.29400656 0.291858   0.2996092 ]\n",
      " [0.2856623  0.29400656 0.291858   0.2996092 ]\n",
      " [0.2856623  0.29400656 0.291858   0.2996092 ]]\n",
      "[[0.         0.         0.90756303 0.6568915 ]\n",
      " [0.7478992  0.62463343 0.92436975 0.71260995]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.4369748  0.61876833 1.         1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.04201681 0.6979472  0.5294118  0.8797654 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.41651666 0.254586   0.5623154  0.6077249 ]\n",
      " [0.48119414 0.645447   0.56115806 0.7502756 ]\n",
      " [0.4158349  0.27571848 0.41856945 0.28218094]\n",
      " [0.4157553  0.27485985 0.4181905  0.28098327]\n",
      " [0.4157553  0.27485988 0.4181905  0.28098333]\n",
      " [0.49933946 0.5865666  0.5847814  0.70072424]\n",
      " [0.4550911  0.5656141  0.5131845  0.6562884 ]\n",
      " [0.41575643 0.27487943 0.41819477 0.28101018]\n",
      " [0.41578522 0.5980772  0.49857923 0.70092726]\n",
      " [0.4157553  0.27485976 0.4181905  0.28098318]\n",
      " [0.4157553  0.27485976 0.4181905  0.28098318]\n",
      " [0.4157553  0.27485976 0.4181905  0.28098318]\n",
      " [0.4157553  0.27485976 0.4181905  0.28098318]\n",
      " [0.4157553  0.27485976 0.4181905  0.28098318]\n",
      " [0.4157553  0.27485976 0.4181905  0.28098318]\n",
      " [0.4157553  0.27485976 0.4181905  0.28098318]]\n",
      "[[0.30316743 0.         1.         0.42238268]\n",
      " [0.05882353 0.15884477 0.7918552  0.7761733 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.23465703 0.19457014 0.62815887]\n",
      " [0.479638   0.6895307  0.7239819  0.9494585 ]\n",
      " [0.48868778 0.80144405 0.71945703 0.94584835]\n",
      " [0.09049774 0.6209386  0.26244345 1.        ]\n",
      " [0.1040724  0.80866426 0.2850679  1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.46914136 0.29653186 0.667776   0.48698407]\n",
      " [0.3473689  0.34237757 0.6094581  0.62324256]\n",
      " [0.33942842 0.3123027  0.34407017 0.317408  ]\n",
      " [0.33943486 0.31235436 0.3440934  0.3174771 ]\n",
      " [0.34097776 0.44161004 0.3855827  0.5534668 ]\n",
      " [0.49473456 0.61833596 0.59715474 0.7047808 ]\n",
      " [0.5021798  0.643785   0.5682771  0.7157966 ]\n",
      " [0.3568353  0.5883085  0.44502607 0.70183676]\n",
      " [0.35836345 0.66080326 0.43496832 0.7188055 ]\n",
      " [0.33942908 0.31230766 0.34407255 0.31741473]\n",
      " [0.33942834 0.31230208 0.34406987 0.31740713]\n",
      " [0.33942834 0.31230205 0.34406987 0.3174071 ]\n",
      " [0.33942834 0.31230205 0.34406987 0.3174071 ]\n",
      " [0.33942834 0.31230205 0.34406987 0.3174071 ]\n",
      " [0.33942834 0.31230205 0.34406987 0.3174071 ]\n",
      " [0.33942834 0.31230205 0.34406987 0.3174071 ]]\n",
      "[[0.         0.         0.22352941 0.20975609]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.10980392 0.12195122 0.75686276 0.63414633]\n",
      " [0.10196079 0.11707317 0.32941177 0.41951218]\n",
      " [0.21568628 0.5902439  0.27450982 0.76585364]\n",
      " [0.23921569 0.7512195  0.37254903 0.8780488 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.30588236 0.8829268  0.3647059  1.        ]\n",
      " [0.5529412  0.55609757 0.77254903 0.6926829 ]\n",
      " [0.6156863  0.66341466 0.77254903 0.83902436]\n",
      " [0.60784316 0.64878047 0.6745098  0.75609756]\n",
      " [0.50980395 0.74634147 0.64705884 0.9512195 ]\n",
      " [0.7372549  0.25853658 1.         0.5902439 ]\n",
      " [0.6156863  0.7902439  0.6901961  0.84390247]\n",
      " [0.50980395 0.91219515 0.56078434 0.9512195 ]] [[0.30261245 0.34105036 0.39355543 0.4214462 ]\n",
      " [0.30522463 0.35397053 0.3108571  0.3579439 ]\n",
      " [0.3052304  0.3539967  0.3108765  0.3579791 ]\n",
      " [0.32782006 0.35106215 0.62671995 0.5247623 ]\n",
      " [0.33111978 0.35602415 0.42997578 0.43467158]\n",
      " [0.39364582 0.5346131  0.43681967 0.5964861 ]\n",
      " [0.41786692 0.5853193  0.4628281  0.63758564]\n",
      " [0.31038818 0.3717103  0.3235942  0.38173765]\n",
      " [0.37709484 0.58667463 0.41512805 0.63801944]\n",
      " [0.4707272  0.5022367  0.55996966 0.57480246]\n",
      " [0.55279946 0.57588816 0.61689687 0.6425483 ]\n",
      " [0.517867   0.52890813 0.5684105  0.5721008 ]\n",
      " [0.48776203 0.5540427  0.5382009  0.6233218 ]\n",
      " [0.57075316 0.442697   0.66628075 0.57894576]\n",
      " [0.5884155  0.6310861  0.6241259  0.65023196]\n",
      " [0.49571025 0.62201774 0.5279607  0.6349484 ]]\n",
      "[[0.10106383 0.         0.5531915  0.41960785]\n",
      " [0.         0.34117648 0.7180851  0.98039216]\n",
      " [0.15957446 0.32156864 0.22340426 0.39215687]\n",
      " [0.69148934 0.84705883 1.         1.        ]\n",
      " [0.5744681  0.5568628  0.88829786 0.94509804]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.3594366  0.29721433 0.4945709  0.4441267 ]\n",
      " [0.35958394 0.47593868 0.58436626 0.7402856 ]\n",
      " [0.39397252 0.42846015 0.45180538 0.48162526]\n",
      " [0.5486417  0.6470588  0.6302266  0.7001777 ]\n",
      " [0.54752445 0.55953467 0.61848176 0.70747584]\n",
      " [0.35289785 0.31391805 0.35715008 0.3189792 ]\n",
      " [0.3528979  0.31391862 0.35715032 0.31897998]\n",
      " [0.35289848 0.31392312 0.3571523  0.31898612]\n",
      " [0.35289782 0.31391796 0.35715005 0.31897908]\n",
      " [0.35289782 0.3139179  0.35715002 0.31897902]\n",
      " [0.35289782 0.3139179  0.35715002 0.318979  ]\n",
      " [0.35289782 0.3139179  0.35715002 0.318979  ]\n",
      " [0.35289782 0.3139179  0.35715002 0.318979  ]\n",
      " [0.35289782 0.3139179  0.35715002 0.318979  ]\n",
      " [0.35289782 0.3139179  0.35715002 0.318979  ]\n",
      " [0.35289782 0.3139179  0.35715002 0.318979  ]]\n",
      "[[0.         0.         0.47058824 0.33333334]\n",
      " [0.09803922 0.02666667 0.9411765  0.58666664]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.47058824 0.56       0.60784316 1.        ]\n",
      " [0.49019608 0.96       0.60784316 1.        ]\n",
      " [0.21568628 0.56       0.37254903 1.        ]\n",
      " [0.21568628 0.9066667  0.39215687 1.        ]\n",
      " [0.84313726 0.36       1.         0.81333333]\n",
      " [0.9019608  0.74666667 1.         0.81333333]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.4598673  0.44334564 0.502039   0.48499513]\n",
      " [0.46808955 0.448233   0.5398641  0.50768876]\n",
      " [0.4600948  0.44527143 0.46124876 0.4467605 ]\n",
      " [0.49342299 0.5089273  0.5091136  0.55415195]\n",
      " [0.5023899  0.5453057  0.51216084 0.5623545 ]\n",
      " [0.47651133 0.50098217 0.49325618 0.55575967]\n",
      " [0.4780959  0.54311514 0.48898646 0.55901134]\n",
      " [0.52359635 0.49679384 0.5372983  0.53825456]\n",
      " [0.5272374  0.5241498  0.53649944 0.53626525]\n",
      " [0.4600947  0.44527048 0.46124834 0.44675922]\n",
      " [0.46009463 0.44526997 0.46124813 0.44675854]\n",
      " [0.46009463 0.44527    0.46124816 0.4467586 ]\n",
      " [0.46009463 0.44527012 0.4612482  0.44675872]\n",
      " [0.46009463 0.44526997 0.46124813 0.44675854]\n",
      " [0.46009463 0.44526997 0.46124813 0.44675854]\n",
      " [0.46009463 0.44526997 0.46124813 0.44675854]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ww/3nytfk1917b8nz7dchtckxnm0000gn/T/ipykernel_70094/2805332933.py:93: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  result['obj_class'].replace(class_dict, inplace=True)\n",
      "/var/folders/ww/3nytfk1917b8nz7dchtckxnm0000gn/T/ipykernel_70094/2805332933.py:119: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['head' 'torso' 'neck' ... 'rleg' 'rfoot' 'tail']' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  result.loc[result['obj_class']==k, ['part_labels']] = result.loc[result['obj_class']==k,['part_labels']].replace(v).copy()\n"
     ]
    }
   ],
   "source": [
    "#testing loop\n",
    "model_path = ('/Users/amrutamuthal/Documents/training_runs/model/'+run_prefix+'/Obj-Box-'\n",
    "                        +str(learning_rate)\n",
    "                        +'-batch-'+str(batch_size)\n",
    "                        +'-h1-'+str(hidden1)\n",
    "                        +'-h2-'+str(hidden2)\n",
    "                        +'-h3-'+str(hidden3)+'-test')\n",
    "summary_path = ('/Users/amrutamuthal/Documents/training_runs/run/'+run_prefix+'/Obj-Box-'\n",
    "                        +str(learning_rate)\n",
    "                        +'-batch-'+str(batch_size)\n",
    "                        +'-h1-'+str(hidden1)\n",
    "                        +'-h2-'+str(hidden2)\n",
    "                        +'-h3-'+str(hidden3)+'-test')\n",
    "\n",
    "\n",
    "write_tensorboard = False\n",
    "if write_tensorboard:\n",
    "    writer = SummaryWriter(summary_path)\n",
    "\n",
    "vae = TwoStageAutoEncoder(latent_dims,\n",
    "                          num_nodes,\n",
    "                          bbx_size,\n",
    "                          num_classes,\n",
    "                          label_shape,\n",
    "                          hidden1,\n",
    "                          hidden2,\n",
    "                          hidden3,\n",
    "                          dense_hidden1,\n",
    "                          dense_hidden2,\n",
    "                          adaptive_margin,\n",
    "                          output_log,\n",
    "                          area_encoding,\n",
    "                          coupling,\n",
    "                          obj_bbx_conditioning,\n",
    "                          use_fft_on_bbx,\n",
    "                          use_gcn_in_decoder,\n",
    "                        )\n",
    "\n",
    "vae.load_state_dict(torch.load(model_path+ '/model_weights.pth'))\n",
    "\n",
    "# decoder = vae.decoder\n",
    "image_shape = [num_nodes, bbx_size]\n",
    "global_step = 250000\n",
    "class_dict = {0:'cow', 1:'sheep', 2:'bird', 3:'person', 4:'cat', 5:'dog', 6:'horse'}\n",
    "res_dfs = []\n",
    "for i, val_data in enumerate(batch_val_loader, 0):\n",
    "    \n",
    "    val_data\n",
    "    node_data_true = val_data.x\n",
    "    label_true = node_data_true[:,:1]\n",
    "    y_true = val_data.y\n",
    "    class_true = y_true[:, :num_classes]\n",
    "    X_obj_true = y_true[:, num_classes:]\n",
    "    X_obj_true_transformed = X_obj_true[:,2:]-X_obj_true[:,:2]\n",
    "    node_data_transformed = torch.cat(\n",
    "            [node_data_true[:, :-2],\n",
    "             node_data_true[:, -2:] -  node_data_true[:, -4:-2]], axis=-1)\n",
    "    adj_true = val_data.edge_index\n",
    "    class_true  = torch.flatten(class_true)\n",
    "    \n",
    "    output = vae(adj_true, node_data_transformed, X_obj_true_transformed, label_true , class_true, variational, coupling)\n",
    "    node_data_pred_test = output[0]\n",
    "    node_data_pred_test_refined = output[8]\n",
    "    node_data_pred_test_refined_new = torch.cat(\n",
    "            [node_data_pred_test_refined[:, :, :-2],\n",
    "             node_data_pred_test_refined[:, :, -2:] + node_data_pred_test_refined[:, :, -4:-2]], axis=-1)\n",
    "    X_obj_pred_test = output[1]\n",
    "#     node_data_pred_test, X_obj_pred_test, label_pred, z_mean, z_logvar, margin = output\n",
    "    X_obj_pred_test = torch.cat(((torch.tensor([1.0])-X_obj_pred_test)*torch.tensor([0.5]),\n",
    "                                (torch.tensor([1.0])+X_obj_pred_test)*torch.tensor([0.5])),\n",
    "                              -1)\n",
    "    res_dfs.append(metrics.get_metrics(node_data_true, X_obj_true, node_data_pred_test_refined_new,\n",
    "                               X_obj_pred_test,\n",
    "                               label_true,\n",
    "                               class_true,\n",
    "                               num_nodes,\n",
    "                               num_classes))\n",
    "    \n",
    "    if write_tensorboard:\n",
    "        \n",
    "        for j in range(int(len(node_data_true)/num_nodes)):\n",
    "            \n",
    "            image = plot_bbx(node_data_true[j].detach().to(\"cpu\").numpy().astype(float))/255\n",
    "            pred_image = plot_bbx(node_data_pred_test[j].detach().to(\"cpu\").numpy()).astype(float)/255\n",
    "\n",
    "            writer.add_image('test_result/images/'+str(j)+'-input/', image, global_step, dataformats='HWC')  \n",
    "            writer.add_image('test_result/images/'+str(j)+'-generated/', pred_image, global_step, dataformats='HWC')\n",
    "            \n",
    "\n",
    "result = pd.concat(res_dfs)\n",
    "result['obj_class'] = np.where(result['obj_class'].isna(), 0, result['obj_class'])\n",
    "result['obj_class'] = result['obj_class'].astype('int')\n",
    "result['obj_class'].replace(class_dict, inplace=True)\n",
    "result.where(result['part_labels']!=0, np.NaN, inplace=True)\n",
    "result['part_labels'] = np.where(result['part_labels'].isna(), 0, result['part_labels'])\n",
    "result['part_labels'] = result['part_labels'].astype('int')\n",
    "result['id'] = np.repeat(np.array(list(range(int(len(result)/num_nodes)))), 16)\n",
    "\n",
    "if write_tensorboard:\n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "\n",
    "bird_labels = {'head':1 , 'torso':2, 'neck':3, 'lwing':4, 'rwing':5, 'lleg':6, 'lfoot':7, 'rleg':8, 'rfoot':9, 'tail':10}\n",
    "cat_labels = {'head':1, 'torso':2, 'neck':3, 'lfleg':4, 'lfpa':5, 'rfleg':6, 'rfpa':7, 'lbleg':8, 'lbpa':9, 'rbleg':10, 'rbpa':11, 'tail':12}\n",
    "cow_labels = {'head':1,'lhorn':2, 'rhorn':3, 'torso':4, 'neck':5, 'lfuleg':6, 'lflleg':7, 'rfuleg':8, 'rflleg':9, 'lbuleg':10, 'lblleg':11, 'rbuleg':12, 'rblleg':13, 'tail':14}\n",
    "dog_labels = {'head':1,'torso':2, 'neck':3, 'lfleg':4, 'lfpa':5, 'rfleg':6, 'rfpa':7, 'lbleg':8, 'lbpa':9, 'rbleg':10, 'rbpa':11, 'tail':12, 'muzzle':13}\n",
    "horse_labels = {'head':1,'lfho':2, 'rfho':3, 'torso':4, 'neck':5, 'lfuleg':6, 'lflleg':7, 'rfuleg':8, 'rflleg':9, 'lbuleg':10, 'lblleg':11, 'rbuleg':12, 'rblleg':13, 'tail':14, 'lbho':15, 'rbho':16}\n",
    "person_labels = {'head':1, 'torso':2, 'neck': 3, 'llarm': 4, 'luarm': 5, 'lhand': 6, 'rlarm':7, 'ruarm':8, 'rhand': 9, 'llleg': 10, 'luleg':11, 'lfoot':12, 'rlleg':13, 'ruleg':14, 'rfoot':15}\n",
    "sheep_labels = cow_labels\n",
    "part_labels_combined_parts = {'bird': bird_labels, 'cat': cat_labels, 'cow': cow_labels, 'dog': dog_labels, 'sheep': sheep_labels, 'horse':horse_labels,'person':person_labels}\n",
    "\n",
    "for k, v in part_labels_combined_parts.items():\n",
    "    new_map = {}\n",
    "    for pk, pv in v.items():\n",
    "        new_map[pv]=pk\n",
    "    part_labels_combined_parts[k] = new_map\n",
    "    \n",
    "for k, v in part_labels_combined_parts.items():\n",
    "    result.loc[result['obj_class']==k, ['part_labels']] = result.loc[result['obj_class']==k,['part_labels']].replace(v).copy()\n",
    "\n",
    "# rel_metrics.csv')sult.to_csv(model_path+'/raw_metrics.csv')\n",
    "# res_obj_level = result.groupby(['obj_class', 'id']).mean().reset_index()\n",
    "# res_obj_level.groupby(['obj_class']).mean().reset_index()[['obj_class', 'IOU', 'MSE']].to_csv(model_path+'/obj_level_metrics.csv')\n",
    "# result.groupby(['obj_class', 'part_labels']).mean().reset_index()[['obj_class', 'part_labels',  'IOU', 'MSE']].to_csv(model_path+'/part_leve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9eeb5151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0326, 0.0692, 0.1789, 0.3123],\n",
       "        [0.2018, 0.8220, 0.0474, 0.0461],\n",
       "        [0.2299, 0.7969, 0.0483, 0.0312],\n",
       "        [0.1337, 0.1705, 0.7212, 0.4065],\n",
       "        [0.1678, 0.1147, 0.2044, 0.2841],\n",
       "        [0.2097, 0.5183, 0.0977, 0.2169],\n",
       "        [0.2058, 0.7084, 0.0884, 0.1857],\n",
       "        [0.2449, 0.5426, 0.0893, 0.1976],\n",
       "        [0.2484, 0.6813, 0.0777, 0.1656],\n",
       "        [0.7232, 0.4989, 0.1412, 0.2142],\n",
       "        [0.7659, 0.6848, 0.1124, 0.1899],\n",
       "        [0.6906, 0.4840, 0.0836, 0.1716],\n",
       "        [0.6649, 0.6978, 0.1055, 0.1816],\n",
       "        [0.8333, 0.3005, 0.1759, 0.2734],\n",
       "        [0.7980, 0.8009, 0.0549, 0.0481],\n",
       "        [0.6304, 0.7935, 0.0458, 0.0480]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_data_pred_test_refined[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "be575bf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000,  0.4945,  0.1333,  0.6301,  0.6242],\n",
       "        [ 0.0000,  0.2697,  0.1333,  0.2697,  0.1333],\n",
       "        [ 0.0000,  0.2697,  0.1333,  0.2697,  0.1333],\n",
       "        [ 4.0000,  0.3551,  0.3712,  0.6425,  0.6894],\n",
       "        [ 0.0000,  0.2697,  0.1333,  0.2697,  0.1333],\n",
       "        [ 6.0000,  0.5884,  0.6242,  0.6330,  0.8030],\n",
       "        [ 7.0000,  0.5950,  0.8091,  0.6254,  0.8667],\n",
       "        [ 8.0000,  0.5087,  0.6652,  0.5647,  0.8500],\n",
       "        [ 9.0000,  0.5334,  0.8379,  0.5675,  0.8667],\n",
       "        [10.0000,  0.4272,  0.6470,  0.4575,  0.7485],\n",
       "        [11.0000,  0.4281,  0.7273,  0.4746,  0.8652],\n",
       "        [12.0000,  0.3560,  0.5167,  0.4082,  0.7318],\n",
       "        [13.0000,  0.3845,  0.7182,  0.4158,  0.8545],\n",
       "        [14.0000,  0.4053,  0.5894,  0.4253,  0.7394],\n",
       "        [15.0000,  0.4395,  0.8197,  0.4774,  0.8667],\n",
       "        [16.0000,  0.3873,  0.8197,  0.4158,  0.8576]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_data_true[:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "78220c71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>obj_class</th>\n",
       "      <th>part_labels</th>\n",
       "      <th>IOU</th>\n",
       "      <th>MSE</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bird</td>\n",
       "      <td>head</td>\n",
       "      <td>0.661019</td>\n",
       "      <td>0.000274</td>\n",
       "      <td>3402.995090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bird</td>\n",
       "      <td>lfoot</td>\n",
       "      <td>0.236245</td>\n",
       "      <td>0.001427</td>\n",
       "      <td>3511.627968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bird</td>\n",
       "      <td>lleg</td>\n",
       "      <td>0.382974</td>\n",
       "      <td>0.000362</td>\n",
       "      <td>3449.013722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bird</td>\n",
       "      <td>lwing</td>\n",
       "      <td>0.440634</td>\n",
       "      <td>0.001476</td>\n",
       "      <td>3514.273504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bird</td>\n",
       "      <td>neck</td>\n",
       "      <td>0.464578</td>\n",
       "      <td>0.000434</td>\n",
       "      <td>3388.062622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>sheep</td>\n",
       "      <td>lhorn</td>\n",
       "      <td>0.232146</td>\n",
       "      <td>0.001155</td>\n",
       "      <td>3424.169811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>sheep</td>\n",
       "      <td>neck</td>\n",
       "      <td>0.686244</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>3325.633333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>sheep</td>\n",
       "      <td>rhorn</td>\n",
       "      <td>0.270818</td>\n",
       "      <td>0.001003</td>\n",
       "      <td>3520.534722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>sheep</td>\n",
       "      <td>tail</td>\n",
       "      <td>0.293806</td>\n",
       "      <td>0.000642</td>\n",
       "      <td>3333.458015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>sheep</td>\n",
       "      <td>torso</td>\n",
       "      <td>0.882393</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>3290.846154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>86 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   obj_class part_labels       IOU       MSE           id\n",
       "0       bird        head  0.661019  0.000274  3402.995090\n",
       "1       bird       lfoot  0.236245  0.001427  3511.627968\n",
       "2       bird        lleg  0.382974  0.000362  3449.013722\n",
       "3       bird       lwing  0.440634  0.001476  3514.273504\n",
       "4       bird        neck  0.464578  0.000434  3388.062622\n",
       "..       ...         ...       ...       ...          ...\n",
       "81     sheep       lhorn  0.232146  0.001155  3424.169811\n",
       "82     sheep        neck  0.686244  0.000163  3325.633333\n",
       "83     sheep       rhorn  0.270818  0.001003  3520.534722\n",
       "84     sheep        tail  0.293806  0.000642  3333.458015\n",
       "85     sheep       torso  0.882393  0.000118  3290.846154\n",
       "\n",
       "[86 rows x 5 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[['obj_class', 'part_labels', 'IOU', 'MSE', 'id']].groupby(['obj_class', 'part_labels']).mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "02e6f6eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>obj_class</th>\n",
       "      <th>IOU</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bird</td>\n",
       "      <td>0.570665</td>\n",
       "      <td>0.000476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cat</td>\n",
       "      <td>0.612275</td>\n",
       "      <td>0.001287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cow</td>\n",
       "      <td>0.536254</td>\n",
       "      <td>0.000745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dog</td>\n",
       "      <td>0.503102</td>\n",
       "      <td>0.000873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>horse</td>\n",
       "      <td>0.434981</td>\n",
       "      <td>0.001420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>person</td>\n",
       "      <td>0.522078</td>\n",
       "      <td>0.001277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sheep</td>\n",
       "      <td>0.696612</td>\n",
       "      <td>0.000317</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  obj_class       IOU       MSE\n",
       "0      bird  0.570665  0.000476\n",
       "1       cat  0.612275  0.001287\n",
       "2       cow  0.536254  0.000745\n",
       "3       dog  0.503102  0.000873\n",
       "4     horse  0.434981  0.001420\n",
       "5    person  0.522078  0.001277\n",
       "6     sheep  0.696612  0.000317"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_obj_level = result[['obj_class', 'IOU', 'MSE', 'id']].groupby(['obj_class', 'id']).mean().reset_index()\n",
    "res_obj_level.groupby(['obj_class']).mean().reset_index()[['obj_class', 'IOU', 'MSE']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d9367e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(X_obj, part_decoder, obj_class, nodes, batch_size, latent_dims):\n",
    "    \n",
    "    nodes = torch.reshape(nodes,(batch_size,part_decoder.num_nodes))\n",
    "    obj_class = torch.reshape(obj_class, \n",
    "                              (batch_size, num_classes))\n",
    "    \n",
    "    # obj sampling\n",
    "    z_latent_obj = X_obj\n",
    "    \n",
    "    print(z_latent_obj.shape, obj_class.shape)\n",
    "    conditioned_obj_latent = torch.cat([obj_class, z_latent_obj],dim=-1)\n",
    "    conditioned_obj_latent = torch.cat([nodes, conditioned_obj_latent],dim=-1)\n",
    "\n",
    "    # part sampling\n",
    "    z_latent_part = torch.normal(torch.zeros([batch_size,latent_dims]))\n",
    "    conditioned_part_latent = torch.cat([conditioned_obj_latent, z_latent_part],dim=-1)\n",
    "    \n",
    "    x_bbx, _, _, _ = part_decoder(conditioned_part_latent)\n",
    "    \n",
    "    return x_bbx, X_obj\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c39d671",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#testing loop\n",
    "model_path = ('D:/meronym_data/model/'+run_prefix+'/Obj-Box-'\n",
    "                        +str(learning_rate)\n",
    "                        +'-batch-'+str(batch_size)\n",
    "                        +'-h1-'+str(hidden1)\n",
    "                        +'-h2-'+str(hidden2)\n",
    "                        +'-h3-'+str(hidden3)+'-test')\n",
    "summary_path = ('D:/meronym_data/runs/'+run_prefix+'/Obj-Box-'\n",
    "                        +str(learning_rate)\n",
    "                        +'-batch-'+str(batch_size)\n",
    "                        +'-h1-'+str(hidden1)\n",
    "                        +'-h2-'+str(hidden2)\n",
    "                        +'-h3-'+str(hidden3)+'-test')\n",
    "\n",
    "\n",
    "write_tensorboard = True\n",
    "if write_tensorboard:\n",
    "    writer = SummaryWriter(summary_path)\n",
    "\n",
    "vae = TwoStageAutoEncoder(latent_dims,\n",
    "                          num_nodes,\n",
    "                          bbx_size,\n",
    "                          num_classes,\n",
    "                          label_shape,\n",
    "                          hidden1,\n",
    "                          hidden2,\n",
    "                          hidden3,\n",
    "                          dense_hidden1,\n",
    "                          dense_hidden2,\n",
    "                          adaptive_margin,\n",
    "                          output_log,\n",
    "                          area_encoding,\n",
    "                          coupling,\n",
    "                          obj_bbx_conditioning,\n",
    "                        )\n",
    "\n",
    "\n",
    "vae.load_state_dict(torch.load(model_path+ '/model_weights.pth'))\n",
    "\n",
    "part_decoder = vae.gcn_decoder\n",
    "image_shape = [num_nodes, bbx_size]\n",
    "global_step = 250000\n",
    "class_dict = {0:'cow', 1:'sheep', 2:'bird', 3:'person', 4:'cat', 5:'dog', 6:'horse'}\n",
    "res_dfs = []\n",
    "for i, val_data in enumerate(batch_val_loader, 0):\n",
    "    \n",
    "    val_data\n",
    "    node_data_true = val_data.x\n",
    "    label_true = node_data_true[:,:1]\n",
    "    y_true = val_data.y\n",
    "    class_true = y_true[:, :num_classes]\n",
    "    X_obj_true = y_true[:, num_classes:]\n",
    "    adj_true = val_data.edge_index\n",
    "    class_true  = torch.flatten(class_true)\n",
    "    \n",
    "    output = inference(X_obj_true, part_decoder, class_true, label_true, batch_size, latent_dims)\n",
    "    node_data_pred_test = output[0]\n",
    "    X_obj_pred_test = output[1]\n",
    "#     node_data_pred_test, X_obj_pred_test, label_pred, z_mean, z_logvar, margin = output\n",
    "    \n",
    "#     res_dfs.append(metrics.get_metrics(node_data_true, X_obj_true, node_data_pred_test,\n",
    "#                                X_obj_pred_test,\n",
    "#                                label_true,\n",
    "#                                class_true,\n",
    "#                                num_nodes,\n",
    "#                                num_classes))\n",
    "    \n",
    "    if write_tensorboard:\n",
    "        \n",
    "        for j in range(int(len(node_data_true)/num_nodes)):\n",
    "            image = plot_utils.plot_bbx((node_data_true[num_nodes*j:num_nodes*(j+1)\n",
    "                                                       ,1:5]*label_true[num_nodes*j:num_nodes*(j+1)]).detach().to(\"cpu\").numpy().astype(float))/255\n",
    "            pred_image = plot_utils.plot_bbx((node_data_pred_test[j]*label_true[num_nodes*j:num_nodes*(j+1)]).detach().to(\"cpu\").numpy()).astype(float)/255\n",
    "\n",
    "            writer.add_image('test_result/images/'+str(j)+'-input/', image, global_step, dataformats='HWC')  \n",
    "            writer.add_image('test_result/images/'+str(j)+'-generated/', pred_image, global_step, dataformats='HWC')\n",
    "            \n",
    "\n",
    "result = pd.concat(res_dfs)\n",
    "result['obj_class'] = np.where(result['obj_class'].isna(), 0, result['obj_class'])\n",
    "result['obj_class'] = result['obj_class'].astype('int')\n",
    "result['obj_class'].replace(class_dict, inplace=True)\n",
    "result.where(result['part_labels']!=0, np.NaN, inplace=True)\n",
    "result['part_labels'] = np.where(result['part_labels'].isna(), 0, result['part_labels'])\n",
    "result['part_labels'] = result['part_labels'].astype('int')\n",
    "result['id'] = np.repeat(np.array(list(range(int(len(result)/num_nodes)))), 16)\n",
    "\n",
    "if write_tensorboard:\n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "\n",
    "bird_labels = {'head':1 , 'torso':2, 'neck':3, 'lwing':4, 'rwing':5, 'lleg':6, 'lfoot':7, 'rleg':8, 'rfoot':9, 'tail':10}\n",
    "cat_labels = {'head':1, 'torso':2, 'neck':3, 'lfleg':4, 'lfpa':5, 'rfleg':6, 'rfpa':7, 'lbleg':8, 'lbpa':9, 'rbleg':10, 'rbpa':11, 'tail':12}\n",
    "cow_labels = {'head':1,'lhorn':2, 'rhorn':3, 'torso':4, 'neck':5, 'lfuleg':6, 'lflleg':7, 'rfuleg':8, 'rflleg':9, 'lbuleg':10, 'lblleg':11, 'rbuleg':12, 'rblleg':13, 'tail':14}\n",
    "dog_labels = {'head':1,'torso':2, 'neck':3, 'lfleg':4, 'lfpa':5, 'rfleg':6, 'rfpa':7, 'lbleg':8, 'lbpa':9, 'rbleg':10, 'rbpa':11, 'tail':12, 'muzzle':13}\n",
    "horse_labels = {'head':1,'lfho':2, 'rfho':3, 'torso':4, 'neck':5, 'lfuleg':6, 'lflleg':7, 'rfuleg':8, 'rflleg':9, 'lbuleg':10, 'lblleg':11, 'rbuleg':12, 'rblleg':13, 'tail':14, 'lbho':15, 'rbho':16}\n",
    "person_labels = {'head':1, 'torso':2, 'neck': 3, 'llarm': 4, 'luarm': 5, 'lhand': 6, 'rlarm':7, 'ruarm':8, 'rhand': 9, 'llleg': 10, 'luleg':11, 'lfoot':12, 'rlleg':13, 'ruleg':14, 'rfoot':15}\n",
    "sheep_labels = cow_labels\n",
    "part_labels_combined_parts = {'bird': bird_labels, 'cat': cat_labels, 'cow': cow_labels, 'dog': dog_labels, 'sheep': sheep_labels, 'horse':horse_labels,'person':person_labels}\n",
    "\n",
    "for k, v in part_labels_combined_parts.items():\n",
    "    new_map = {}\n",
    "    for pk, pv in v.items():\n",
    "        new_map[pv]=pk\n",
    "    part_labels_combined_parts[k] = new_map\n",
    "    \n",
    "for k, v in part_labels_combined_parts.items():\n",
    "    result.loc[result['obj_class']==k, ['part_labels']] = result.loc[result['obj_class']==k,['part_labels']].replace(v).copy()\n",
    "\n",
    "result.to_csv(model_path+'/raw_metrics.csv')\n",
    "res_obj_level = result.groupby(['obj_class', 'id']).mean().reset_index()\n",
    "res_obj_level.groupby(['obj_class']).mean().reset_index()[['obj_class', 'IOU', 'MSE']].to_csv(model_path+'/obj_level_metrics.csv')\n",
    "result.groupby(['obj_class', 'part_labels']).mean().reset_index()[['obj_class', 'part_labels',  'IOU', 'MSE']].to_csv(model_path+'/part_level_metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a379c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = ('D:/meronym_data/model/'+run_prefix+'/Obj-Box-'\n",
    "                        +str(learning_rate)\n",
    "                        +'-batch-'+str(batch_size)\n",
    "                        +'-h1-'+str(hidden1)\n",
    "                        +'-h2-'+str(hidden2)\n",
    "                        +'-h3-'+str(hidden3)+'-test')\n",
    "summary_path = ('D:/meronym_data/runs/'+run_prefix+'/Obj-Box-'\n",
    "                        +str(learning_rate)\n",
    "                        +'-batch-'+str(batch_size)\n",
    "                        +'-h1-'+str(hidden1)\n",
    "                        +'-h2-'+str(hidden2)\n",
    "                        +'-h3-'+str(hidden3)+'-test')\n",
    "\n",
    "result = pd.read_csv(model_path+'/raw_metrics.csv')\n",
    "result.groupby(['obj_class', 'part_labels']).mean().reset_index()[['obj_class', 'part_labels',\n",
    "                                                                   'IOU', 'MSE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2d2cbb0",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "agg function failed [how->mean,dtype->object]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/groupby/groupby.py:1942\u001b[0m, in \u001b[0;36mGroupBy._agg_py_fallback\u001b[0;34m(self, how, values, ndim, alt)\u001b[0m\n\u001b[1;32m   1941\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1942\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_grouper\u001b[38;5;241m.\u001b[39magg_series(ser, alt, preserve_dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1943\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/groupby/ops.py:864\u001b[0m, in \u001b[0;36mBaseGrouper.agg_series\u001b[0;34m(self, obj, func, preserve_dtype)\u001b[0m\n\u001b[1;32m    862\u001b[0m     preserve_dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 864\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aggregate_series_pure_python(obj, func)\n\u001b[1;32m    866\u001b[0m npvalues \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mmaybe_convert_objects(result, try_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/groupby/ops.py:885\u001b[0m, in \u001b[0;36mBaseGrouper._aggregate_series_pure_python\u001b[0;34m(self, obj, func)\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(splitter):\n\u001b[0;32m--> 885\u001b[0m     res \u001b[38;5;241m=\u001b[39m func(group)\n\u001b[1;32m    886\u001b[0m     res \u001b[38;5;241m=\u001b[39m extract_result(res)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/groupby/groupby.py:2454\u001b[0m, in \u001b[0;36mGroupBy.mean.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   2451\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2452\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cython_agg_general(\n\u001b[1;32m   2453\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m-> 2454\u001b[0m         alt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: Series(x, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mmean(numeric_only\u001b[38;5;241m=\u001b[39mnumeric_only),\n\u001b[1;32m   2455\u001b[0m         numeric_only\u001b[38;5;241m=\u001b[39mnumeric_only,\n\u001b[1;32m   2456\u001b[0m     )\n\u001b[1;32m   2457\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroupby\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/series.py:6549\u001b[0m, in \u001b[0;36mSeries.mean\u001b[0;34m(self, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m   6541\u001b[0m \u001b[38;5;129m@doc\u001b[39m(make_doc(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m, ndim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m   6542\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmean\u001b[39m(\n\u001b[1;32m   6543\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   6547\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   6548\u001b[0m ):\n\u001b[0;32m-> 6549\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m NDFrame\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;28mself\u001b[39m, axis, skipna, numeric_only, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/generic.py:12420\u001b[0m, in \u001b[0;36mNDFrame.mean\u001b[0;34m(self, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m  12413\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmean\u001b[39m(\n\u001b[1;32m  12414\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m  12415\u001b[0m     axis: Axis \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  12418\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m  12419\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[0;32m> 12420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stat_function(\n\u001b[1;32m  12421\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m, nanops\u001b[38;5;241m.\u001b[39mnanmean, axis, skipna, numeric_only, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m  12422\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/generic.py:12377\u001b[0m, in \u001b[0;36mNDFrame._stat_function\u001b[0;34m(self, name, func, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m  12375\u001b[0m validate_bool_kwarg(skipna, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskipna\u001b[39m\u001b[38;5;124m\"\u001b[39m, none_allowed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m> 12377\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reduce(\n\u001b[1;32m  12378\u001b[0m     func, name\u001b[38;5;241m=\u001b[39mname, axis\u001b[38;5;241m=\u001b[39maxis, skipna\u001b[38;5;241m=\u001b[39mskipna, numeric_only\u001b[38;5;241m=\u001b[39mnumeric_only\n\u001b[1;32m  12379\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/series.py:6457\u001b[0m, in \u001b[0;36mSeries._reduce\u001b[0;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[0m\n\u001b[1;32m   6453\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   6454\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeries.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not allow \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwd_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnumeric_only\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   6455\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith non-numeric dtypes.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   6456\u001b[0m     )\n\u001b[0;32m-> 6457\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op(delegate, skipna\u001b[38;5;241m=\u001b[39mskipna, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/nanops.py:147\u001b[0m, in \u001b[0;36mbottleneck_switch.__call__.<locals>.f\u001b[0;34m(values, axis, skipna, **kwds)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 147\u001b[0m     result \u001b[38;5;241m=\u001b[39m alt(values, axis\u001b[38;5;241m=\u001b[39maxis, skipna\u001b[38;5;241m=\u001b[39mskipna, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/nanops.py:404\u001b[0m, in \u001b[0;36m_datetimelike_compat.<locals>.new_func\u001b[0;34m(values, axis, skipna, mask, **kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m     mask \u001b[38;5;241m=\u001b[39m isna(values)\n\u001b[0;32m--> 404\u001b[0m result \u001b[38;5;241m=\u001b[39m func(values, axis\u001b[38;5;241m=\u001b[39maxis, skipna\u001b[38;5;241m=\u001b[39mskipna, mask\u001b[38;5;241m=\u001b[39mmask, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m datetimelike:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/nanops.py:720\u001b[0m, in \u001b[0;36mnanmean\u001b[0;34m(values, axis, skipna, mask)\u001b[0m\n\u001b[1;32m    719\u001b[0m the_sum \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39msum(axis, dtype\u001b[38;5;241m=\u001b[39mdtype_sum)\n\u001b[0;32m--> 720\u001b[0m the_sum \u001b[38;5;241m=\u001b[39m _ensure_numeric(the_sum)\n\u001b[1;32m    722\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(the_sum, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/nanops.py:1701\u001b[0m, in \u001b[0;36m_ensure_numeric\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1699\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   1700\u001b[0m     \u001b[38;5;66;03m# GH#44008, GH#36703 avoid casting e.g. strings to numeric\u001b[39;00m\n\u001b[0;32m-> 1701\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not convert string \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to numeric\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1702\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: Could not convert string 'headtorsoneck' to numeric",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m res_obj_level \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mgroupby([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobj_class\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[1;32m      2\u001b[0m res_obj_level\u001b[38;5;241m.\u001b[39mgroupby([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobj_class\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mreset_index()[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobj_class\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIOU\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMSE\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/groupby/groupby.py:2452\u001b[0m, in \u001b[0;36mGroupBy.mean\u001b[0;34m(self, numeric_only, engine, engine_kwargs)\u001b[0m\n\u001b[1;32m   2445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_numba_agg_general(\n\u001b[1;32m   2446\u001b[0m         grouped_mean,\n\u001b[1;32m   2447\u001b[0m         executor\u001b[38;5;241m.\u001b[39mfloat_dtype_mapping,\n\u001b[1;32m   2448\u001b[0m         engine_kwargs,\n\u001b[1;32m   2449\u001b[0m         min_periods\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   2450\u001b[0m     )\n\u001b[1;32m   2451\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2452\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cython_agg_general(\n\u001b[1;32m   2453\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2454\u001b[0m         alt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: Series(x, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mmean(numeric_only\u001b[38;5;241m=\u001b[39mnumeric_only),\n\u001b[1;32m   2455\u001b[0m         numeric_only\u001b[38;5;241m=\u001b[39mnumeric_only,\n\u001b[1;32m   2456\u001b[0m     )\n\u001b[1;32m   2457\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroupby\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/groupby/groupby.py:1998\u001b[0m, in \u001b[0;36mGroupBy._cython_agg_general\u001b[0;34m(self, how, alt, numeric_only, min_count, **kwargs)\u001b[0m\n\u001b[1;32m   1995\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agg_py_fallback(how, values, ndim\u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39mndim, alt\u001b[38;5;241m=\u001b[39malt)\n\u001b[1;32m   1996\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m-> 1998\u001b[0m new_mgr \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mgrouped_reduce(array_func)\n\u001b[1;32m   1999\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_agged_manager(new_mgr)\n\u001b[1;32m   2000\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m how \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midxmin\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midxmax\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/internals/managers.py:1469\u001b[0m, in \u001b[0;36mBlockManager.grouped_reduce\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m   1465\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m blk\u001b[38;5;241m.\u001b[39mis_object:\n\u001b[1;32m   1466\u001b[0m     \u001b[38;5;66;03m# split on object-dtype blocks bc some columns may raise\u001b[39;00m\n\u001b[1;32m   1467\u001b[0m     \u001b[38;5;66;03m#  while others do not.\u001b[39;00m\n\u001b[1;32m   1468\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sb \u001b[38;5;129;01min\u001b[39;00m blk\u001b[38;5;241m.\u001b[39m_split():\n\u001b[0;32m-> 1469\u001b[0m         applied \u001b[38;5;241m=\u001b[39m sb\u001b[38;5;241m.\u001b[39mapply(func)\n\u001b[1;32m   1470\u001b[0m         result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[1;32m   1471\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/internals/blocks.py:393\u001b[0m, in \u001b[0;36mBlock.apply\u001b[0;34m(self, func, **kwargs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Block]:\n\u001b[1;32m    389\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;124;03m    apply the function to my values; return a block if we are not\u001b[39;00m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;124;03m    one\u001b[39;00m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 393\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    395\u001b[0m     result \u001b[38;5;241m=\u001b[39m maybe_coerce_values(result)\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split_op_result(result)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/groupby/groupby.py:1995\u001b[0m, in \u001b[0;36mGroupBy._cython_agg_general.<locals>.array_func\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m   1992\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m   1994\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m alt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1995\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agg_py_fallback(how, values, ndim\u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39mndim, alt\u001b[38;5;241m=\u001b[39malt)\n\u001b[1;32m   1996\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/groupby/groupby.py:1946\u001b[0m, in \u001b[0;36mGroupBy._agg_py_fallback\u001b[0;34m(self, how, values, ndim, alt)\u001b[0m\n\u001b[1;32m   1944\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magg function failed [how->\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhow\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,dtype->\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mser\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1945\u001b[0m     \u001b[38;5;66;03m# preserve the kind of exception that raised\u001b[39;00m\n\u001b[0;32m-> 1946\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(err)(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   1948\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ser\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[1;32m   1949\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m res_values\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: agg function failed [how->mean,dtype->object]"
     ]
    }
   ],
   "source": [
    "res_obj_level = result.groupby(['obj_class', 'id']).mean().reset_index()\n",
    "res_obj_level.groupby(['obj_class']).mean().reset_index()[['obj_class', 'IOU', 'MSE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2a0a03e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IOU    0.550569\n",
       "MSE    0.000943\n",
       "dtype: float64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_obj_level[['IOU', 'MSE']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e231a317",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_obj_pred[:10,2:]-X_obj_pred[:10,:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90832eb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_obj_true[:10, 2:]-X_obj_true[:10, :2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b789243a",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_obj_level.to_csv(model_path+'/obj_metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838534e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7e5b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = 'D:/meronym_data/generate_boxes.npy'\n",
    "with open(outfile, 'wb') as pickle_file:\n",
    "    pred_boxes = np.concatenate(pred_boxes)\n",
    "    pickle.dump(pred_boxes, pickle_file)\n",
    "outfile = 'D:/meronym_data/generate_boxesobj_class.npy'\n",
    "with open(outfile, 'wb') as pickle_file:\n",
    "    pickle.dump(classes,pickle_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
