{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cfd1439",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5671166e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_LAUNCH_BLOCKING=1\n"
     ]
    }
   ],
   "source": [
    "%set_env CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21434d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import collections\n",
    "import cv2\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e88c7fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "from torch_geometric.data import DataLoader\n",
    "import torch.utils.data as data_utils\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e46b5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from losses import BoxVAE_losses as loss\n",
    "from evaluation import metrics\n",
    "from utils import plot_utils\n",
    "from utils import data_utils as data_loading\n",
    "from components.DenseAutoencoder import DenseAutoencoder\n",
    "from components.DenseAutoencoder import Decoder\n",
    "from components.TwoStageGCNAutoEncoder import TwoStageAutoEncoder\n",
    "from components.Decoder import GCNDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d19349cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b59f5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mask_generation import masked_sketch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cd8d4a6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "latent_dims = 64\n",
    "num_nodes = 16\n",
    "bbx_size = 4\n",
    "num_classes = 7\n",
    "label_shape = 1\n",
    "nb_epochs = 500\n",
    "klw = loss.frange_cycle_linear(nb_epochs)\n",
    "learning_rate = 0.00003\n",
    "hidden1 = 32\n",
    "hidden2 = 16\n",
    "hidden3 = 128\n",
    "dense_hidden1=8\n",
    "dense_hidden2=4\n",
    "adaptive_margin = True\n",
    "fine_tune_box = False\n",
    "output_log = False\n",
    "area_encoding = False\n",
    "run_prefix = \"two_stage_small_obj_conditioning_sqr_fft_gcn_decoder\"\n",
    "variational=False\n",
    "coupling = True\n",
    "obj_bbx_conditioning = True\n",
    "use_fft_on_bbx = True\n",
    "use_gcn_in_decoder = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7f3f906",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "batch_train_loader, batch_val_loader = data_loading.load_data(obj_data_postfix = '_obj_boundary_sqr',\n",
    "                                                              part_data_post_fix = '_scaled_sqr',\n",
    "                                                              file_postfix = '_combined',\n",
    "                                                              seed=345,\n",
    "                                                              batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0df33149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50944"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch_train_loader)*128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a319fcb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "361b90d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73f4529b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d41ca05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "[1,   399] loss: 109.485\n",
      "200\n",
      "[2,   399] loss: 97.748\n",
      "200\n",
      "[3,   399] loss: 96.814\n",
      "200\n",
      "[4,   399] loss: 96.378\n",
      "200\n",
      "[5,   399] loss: 95.884\n",
      "200\n",
      "[6,   399] loss: 95.294\n",
      "200\n",
      "[7,   399] loss: 94.619\n",
      "200\n",
      "[8,   399] loss: 93.858\n",
      "200\n",
      "[9,   399] loss: 93.009\n",
      "200\n",
      "[10,   399] loss: 92.151\n",
      "200\n",
      "[11,   399] loss: 91.277\n",
      "200\n",
      "[12,   399] loss: 90.390\n",
      "200\n",
      "[13,   399] loss: 89.558\n",
      "200\n",
      "[14,   399] loss: 88.754\n",
      "200\n",
      "[15,   399] loss: 87.988\n",
      "200\n",
      "[16,   399] loss: 87.211\n",
      "200\n",
      "[17,   399] loss: 86.653\n",
      "200\n",
      "[18,   399] loss: 85.925\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 85\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m vae\u001b[38;5;241m.\u001b[39mparameters():\n\u001b[1;32m     83\u001b[0m     param\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m output \u001b[38;5;241m=\u001b[39m vae(adj_true, node_data_true, X_obj_true_transformed, label_true , class_true, variational, coupling)\n\u001b[1;32m     87\u001b[0m node_data_pred \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     88\u001b[0m X_obj_pred \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/VSWorkspaces/MeronymNet-PyTorch/src/models/../components/TwoStageGCNAutoEncoder.py:92\u001b[0m, in \u001b[0;36mTwoStageAutoEncoder.forward\u001b[0;34m(self, E, X_part, X_obj, nodes, obj_class, variational, coupling, obj_bx_conditioning)\u001b[0m\n\u001b[1;32m     90\u001b[0m     X_label, X_box \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msplit(X_part, (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     91\u001b[0m     X_part_fft \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([torch\u001b[38;5;241m.\u001b[39msin(X_box), torch\u001b[38;5;241m.\u001b[39mcos(X_box), X_label], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 92\u001b[0m     z_mean_part, z_logvar_part \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgcn_encoder(\n\u001b[1;32m     93\u001b[0m         E, X_part_fft, obj_class)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     95\u001b[0m     z_mean_part, z_logvar_part \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgcn_encoder(E, X_part, obj_class)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/VSWorkspaces/MeronymNet-PyTorch/src/models/../components/Encoder.py:44\u001b[0m, in \u001b[0;36mGCNEncoder.forward\u001b[0;34m(self, E, X_data, class_labels)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, E, X_data,class_labels):\n\u001b[1;32m     43\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgconv1(X_data,E)\n\u001b[0;32m---> 44\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgconv2(x,E)\n\u001b[1;32m     46\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_nodes)\n\u001b[1;32m     48\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mreshape(x,(batch_size,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_nodes\u001b[38;5;241m*\u001b[39mx\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch_geometric/nn/conv/gcn_conv.py:265\u001b[0m, in \u001b[0;36mGCNConv.forward\u001b[0;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m# propagate_type: (x: Tensor, edge_weight: OptTensor)\u001b[39;00m\n\u001b[1;32m    263\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpropagate(edge_index, x\u001b[38;5;241m=\u001b[39mx, edge_weight\u001b[38;5;241m=\u001b[39medge_weight)\n\u001b[0;32m--> 265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    266\u001b[0m     out \u001b[38;5;241m=\u001b[39m out \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1716\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[1;32m   1709\u001b[0m \u001b[38;5;66;03m# On the return type:\u001b[39;00m\n\u001b[1;32m   1710\u001b[0m \u001b[38;5;66;03m# We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\u001b[39;00m\n\u001b[1;32m   1711\u001b[0m \u001b[38;5;66;03m# This is done for better interop with various type checkers for the end users.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1714\u001b[0m \u001b[38;5;66;03m# See full discussion on the problems with returning `Union` here\u001b[39;00m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;66;03m# https://github.com/microsoft/pyright/issues/4213\u001b[39;00m\n\u001b[0;32m-> 1716\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m   1717\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1718\u001b[0m         _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "reconstruction_loss_arr = []\n",
    "kl_loss_obj_arr = []\n",
    "kl_loss_part_arr = []\n",
    "bbox_loss_arr = []\n",
    "refined_bbox_loss_arr = []\n",
    "adj_loss_arr = []\n",
    "node_loss_arr = []\n",
    "\n",
    "reconstruction_loss_val_arr = []\n",
    "kl_loss_val_arr = []\n",
    "bbox_loss_val_arr = []\n",
    "refined_bbox_loss_val_arr = []\n",
    "adj_loss_val_arr = []\n",
    "node_loss_val_arr = []\n",
    "\n",
    "bbox_loss_threshold = 1.0\n",
    "vae = TwoStageAutoEncoder(latent_dims,\n",
    "                          num_nodes,\n",
    "                          bbx_size,\n",
    "                          num_classes,\n",
    "                          label_shape,\n",
    "                          hidden1,\n",
    "                          hidden2,\n",
    "                          hidden3,\n",
    "                          dense_hidden1,\n",
    "                          dense_hidden2,\n",
    "                          adaptive_margin,\n",
    "                          output_log,\n",
    "                          area_encoding,\n",
    "                          coupling,\n",
    "                          obj_bbx_conditioning,\n",
    "                          use_fft_on_bbx,\n",
    "                        )\n",
    "vae.to(device)\n",
    "optimizer = optim.Adam(vae.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[200,400], gamma=0.85)\n",
    "model_path = ('/Users/amrutamuthal/Documents/training_runs/model/'+run_prefix+'/Obj-Box-'\n",
    "                        +str(learning_rate)\n",
    "                        +'-batch-'+str(batch_size)\n",
    "                        +'-h1-'+str(hidden1)\n",
    "                        +'-h2-'+str(hidden2)\n",
    "                        +'-h3-'+str(hidden3)+'-test')\n",
    "summary_path = ('/Users/amrutamuthal/Documents/training_runs/runs/'+run_prefix+'/Obj-Box-'\n",
    "                        +str(learning_rate)\n",
    "                        +'-batch-'+str(batch_size)\n",
    "                        +'-h1-'+str(hidden1)\n",
    "                        +'-h2-'+str(hidden2)\n",
    "                        +'-h3-'+str(hidden3)+'-test')\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "writer = SummaryWriter(summary_path)\n",
    "icoef = 0\n",
    "\n",
    "for epoch in range(nb_epochs):\n",
    "    \n",
    "    batch_loss = torch.tensor([0.0])\n",
    "    batch_kl_loss_part = torch.tensor([0.0])\n",
    "    batch_kl_loss_obj = torch.tensor([0.0])\n",
    "    batch_bbox_loss = torch.tensor([0.0])\n",
    "    batch_refined_bbox_loss = torch.tensor([0.0])\n",
    "    batch_obj_bbox_loss = torch.tensor([0.0])\n",
    "    batch_node_loss = torch.tensor([0.0])\n",
    "    IOU_weight_delta = torch.tensor([(1+epoch)/nb_epochs])\n",
    "    images = []\n",
    "    \n",
    "    vae.train()\n",
    "    i=0\n",
    "    for train_data in batch_train_loader:\n",
    "        \n",
    "        node_data_true = train_data.x\n",
    "        label_true = node_data_true[:,:1]\n",
    "        y_true = train_data.y\n",
    "        class_true = y_true[:, :num_classes]\n",
    "        X_obj_true = y_true[:, num_classes:]\n",
    "        X_obj_true_transformed = X_obj_true[:,2:]-X_obj_true[:,:2]\n",
    "        adj_true = train_data.edge_index\n",
    "        batch = train_data.batch\n",
    "        \n",
    "        class_true  = torch.flatten(class_true)\n",
    "        \n",
    "\n",
    "        for param in vae.parameters():\n",
    "            param.grad=None\n",
    "        \n",
    "        output = vae(adj_true, node_data_true, X_obj_true_transformed, label_true , class_true, variational, coupling)\n",
    "        \n",
    "        node_data_pred = output[0]\n",
    "        X_obj_pred = output[1]\n",
    "        X_obj_pred = torch.cat(((torch.tensor([1.0])-X_obj_pred)*torch.tensor([0.5]),\n",
    "                                (torch.tensor([1.0])+X_obj_pred)*torch.tensor([0.5])),\n",
    "                              -1)\n",
    "        label_pred = output[2]\n",
    "        z_mean_part = output[3]\n",
    "        z_logvar_part = output[4]\n",
    "        margin = output[5]\n",
    "        z_mean_obj = output[6]\n",
    "        z_logvar_obj = output[7]\n",
    "        \n",
    "        obj_bbox_loss = loss.obj_bbox_loss(pred_box=X_obj_pred, true_box=X_obj_true, weight=IOU_weight_delta, has_mse=False)\n",
    "        kl_loss_obj = loss.kl_loss(z_mean_obj, z_logvar_obj)\n",
    "        kl_loss_part = loss.kl_loss(z_mean_part, z_logvar_part)\n",
    "        bbox_loss = loss.weighted_bbox_loss(pred_box=node_data_pred, true_box=node_data_true[:,1:], weight=IOU_weight_delta, margin=margin)\n",
    "        node_loss = loss.node_loss(label_pred,label_true)\n",
    "        \n",
    "        reconstruction_loss = (bbox_loss + node_loss)*num_nodes\n",
    "        \n",
    "        kl_weight = klw[icoef]\n",
    "        if variational and (kl_weight>0):\n",
    "            reconstruction_loss += kl_loss_part*kl_weight   \n",
    "\n",
    "        reconstruction_loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        i+=1\n",
    "      \n",
    "        batch_loss += reconstruction_loss\n",
    "        batch_kl_loss_part += kl_loss_part\n",
    "        batch_kl_loss_obj += kl_loss_obj\n",
    "        batch_bbox_loss += bbox_loss\n",
    "        batch_obj_bbox_loss += obj_bbox_loss\n",
    "        batch_node_loss += node_loss\n",
    "    \n",
    "        if i%200==0:\n",
    "            print(i)\n",
    "            global_step = epoch*len(batch_train_loader)+i\n",
    "            \n",
    "            writer.add_scalar(\"Loss/train/reconstruction_loss\", batch_loss.item()/(i+1), global_step)\n",
    "            writer.add_scalar(\"Loss/train/kl_loss_part\", batch_kl_loss_part.item()/(i+1), global_step)\n",
    "            writer.add_scalar(\"Loss/train/kl_loss_obj\", batch_kl_loss_obj.item()/(i+1), global_step)\n",
    "            writer.add_scalar(\"Loss/train/bbox_loss\", batch_bbox_loss.item()/(i+1), global_step)\n",
    "            writer.add_scalar(\"Loss/train/obj_bbox_loss\", batch_obj_bbox_loss.item()/(i+1), global_step)\n",
    "            writer.add_scalar(\"Loss/train/node_loss\", batch_node_loss.item()/(i+1), global_step)\n",
    "            \n",
    "            \n",
    "#     scheduler.step()\n",
    "    global_step = epoch*len(batch_train_loader)+i\n",
    "    image_shape = [num_nodes, bbx_size]\n",
    "\n",
    "    image = plot_utils.plot_bbx(np.reshape((node_data_true[:num_nodes,1:5]*label_true[:num_nodes]).detach().to(\"cpu\").numpy(),\n",
    "                                image_shape)).astype(float)/255\n",
    "    writer.add_image('train/images/input', image, global_step, dataformats='HWC')\n",
    "    image = plot_utils.plot_bbx((node_data_pred[0]*label_true[:num_nodes]).detach().to(\"cpu\").numpy()).astype(float)/255\n",
    "    writer.add_image('train/images/generated', image, global_step, dataformats='HWC')\n",
    "    \n",
    "    reconstruction_loss_arr.append(batch_loss.detach().item()/(i+1))\n",
    "    kl_loss_obj_arr.append(batch_kl_loss_obj.detach().item()/(i+1))\n",
    "    kl_loss_part_arr.append(batch_kl_loss_part.detach().item()/(i+1))\n",
    "    bbox_loss_arr.append(batch_bbox_loss.detach().item()/(i+1))\n",
    "    node_loss_arr.append(batch_node_loss.detach().item()/(i+1))\n",
    "    \n",
    "    print('[%d, %5d] loss: %.3f' %\n",
    "              (epoch + 1, i + 1, batch_loss/(i+1) ))\n",
    "    \n",
    "    \n",
    "    batch_loss = torch.tensor([0.0])\n",
    "    batch_kl_loss_part = torch.tensor([0.0])\n",
    "    batch_kl_loss_obj = torch.tensor([0.0])\n",
    "    batch_bbox_loss = torch.tensor([0.0])\n",
    "    batch_obj_bbox_loss = torch.tensor([0.0])\n",
    "    batch_node_loss = torch.tensor([0.0])\n",
    "    images = []\n",
    "    vae.eval()\n",
    "    for i, val_data in enumerate(batch_val_loader, 0):\n",
    "        \n",
    "        node_data_true = val_data.x\n",
    "        label_true = node_data_true[:,:1]\n",
    "        y_true = val_data.y\n",
    "        class_true = torch.flatten(y_true[:, :num_classes])\n",
    "        X_obj_true = y_true[:, num_classes:]\n",
    "        X_obj_true_transformed = X_obj_true[:,2:]-X_obj_true[:,:2]\n",
    "        adj_true = val_data.edge_index\n",
    "        batch = val_data.batch\n",
    "        \n",
    "        class_true  = torch.flatten(class_true)\n",
    "        \n",
    "        output = vae(adj_true, node_data_true, X_obj_true_transformed, label_true , class_true, variational, coupling)\n",
    "        \n",
    "        node_data_pred = output[0]\n",
    "        X_obj_pred = output[1]\n",
    "        X_obj_pred = torch.cat(((torch.tensor([1.0])-X_obj_pred)*torch.tensor([0.5]),\n",
    "                                (torch.tensor([1.0])+X_obj_pred)*torch.tensor([0.5])),\n",
    "                              -1)\n",
    "        label_pred = output[2]\n",
    "        z_mean_part = output[3]\n",
    "        z_logvar_part = output[4]\n",
    "        margin = output[5]\n",
    "        z_mean_obj = output[6]\n",
    "        z_logvar_obj = output[7]\n",
    "        \n",
    "        \n",
    "        obj_bbox_loss = loss.obj_bbox_loss(pred_box=X_obj_pred, true_box=X_obj_true, weight=IOU_weight_delta, has_mse=False)\n",
    "        kl_loss_obj = loss.kl_loss(z_mean_obj, z_logvar_obj)\n",
    "        kl_loss_part = loss.kl_loss(z_mean_part, z_logvar_part)\n",
    "        bbox_loss = loss.weighted_bbox_loss(\n",
    "            pred_box=node_data_pred, true_box=node_data_true[:,1:],\n",
    "            weight=IOU_weight_delta, margin=margin\n",
    "        )\n",
    "        node_loss = loss.node_loss(label_pred,label_true)\n",
    "        \n",
    "        if kl_weight>0:\n",
    "            reconstruction_loss = kl_loss_part*kl_weight + (bbox_loss + node_loss)*num_nodes\n",
    "        else:\n",
    "            reconstruction_loss = (bbox_loss + node_loss)*num_nodes\n",
    "            \n",
    "        batch_loss += reconstruction_loss\n",
    "        batch_kl_loss_part += kl_loss_part\n",
    "        batch_kl_loss_obj += kl_loss_obj\n",
    "        batch_bbox_loss += bbox_loss\n",
    "        batch_obj_bbox_loss += obj_bbox_loss\n",
    "        batch_node_loss += node_loss\n",
    "    \n",
    "    image = plot_utils.plot_bbx(np.reshape((node_data_true[:num_nodes,1:5]*label_true[:num_nodes]).detach().to(\"cpu\").numpy(),\n",
    "                                image_shape)).astype(float)/255\n",
    "    writer.add_image('val/images/input', image, global_step, dataformats='HWC')\n",
    "    image = plot_utils.plot_bbx((node_data_pred[0]*label_true[:num_nodes]).detach().to(\"cpu\").numpy()).astype(float)/255\n",
    "    writer.add_image('val/images/generated', image, global_step, dataformats='HWC')\n",
    "    \n",
    "    reconstruction_loss_val_arr.append(batch_loss.detach().item()/(i+1))\n",
    "    bbox_loss_val_arr.append(batch_bbox_loss.detach().item()/(i+1))\n",
    "    node_loss_val_arr.append(batch_node_loss.detach().item()/(i+1))\n",
    "    \n",
    "    writer.add_scalar(\"Loss/val/reconstruction_loss\", batch_loss.detach()/(i+1), global_step)\n",
    "    writer.add_scalar(\"Loss/train/kl_loss_part\", batch_kl_loss_part.item()/(i+1), global_step)\n",
    "    writer.add_scalar(\"Loss/train/kl_loss_obj\", batch_kl_loss_obj.item()/(i+1), global_step)\n",
    "    writer.add_scalar(\"Loss/val/bbox_loss\", batch_bbox_loss.item()/(i+1), global_step)\n",
    "    writer.add_scalar(\"Loss/val/obj_bbox_loss\", batch_obj_bbox_loss.item()/(i+1), global_step)\n",
    "    writer.add_scalar(\"Loss/val/node_loss\", batch_node_loss.item()/(i+1), global_step)\n",
    "       \n",
    "    if epoch%50 == 0:\n",
    "        torch.save(vae.state_dict(), model_path + '/model_weights.pth')\n",
    "        \n",
    "    if ((kl_loss_part_arr[-1]>0.5) and \n",
    "        (abs(bbox_loss_arr[-1] - bbox_loss_val_arr[-1]) < 0.07) and \n",
    "        (bbox_loss_arr[-1]<bbox_loss_threshold) and (epoch>300)):\n",
    "        \n",
    "        icoef = icoef + 1\n",
    "        bbox_loss_threshold*=0.9\n",
    "\n",
    "torch.save(vae.state_dict(),model_path + '/model_weights.pth')\n",
    "\n",
    "for i in range(min(100,int(len(node_data_true)/num_nodes))):    \n",
    "    image = plot_utils.plot_bbx(np.reshape((node_data_true[num_nodes*i:num_nodes*(i+1),1:5]).detach().to(\"cpu\").numpy(),\n",
    "                                    image_shape)).astype(float)/255\n",
    "    writer.add_image('result/images/'+str(i)+'-input', image, global_step, dataformats='HWC')\n",
    "    image = plot_utils.plot_bbx((node_data_pred[i]*label_true[num_nodes*i:num_nodes*(i+1)]).detach().to(\"cpu\").numpy()).astype(float)/255\n",
    "    writer.add_image('result/images/'+str(i)+'-generated', image, global_step, dataformats='HWC')\n",
    "    \n",
    "writer.flush()\n",
    "writer.close()\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "127746fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TwoStageAutoEncoder(\n",
       "  (gcn_encoder): GCNEncoder(\n",
       "    (gconv1): GCNConv(9, 32)\n",
       "    (gconv2): GCNConv(32, 16)\n",
       "    (dense_boxes): Linear(in_features=8, out_features=16, bias=True)\n",
       "    (dense_labels): Linear(in_features=1, out_features=16, bias=True)\n",
       "    (act): ReLU()\n",
       "    (dense1): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (dense2): Linear(in_features=263, out_features=128, bias=True)\n",
       "    (dense3): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (latent): Linear(in_features=128, out_features=64, bias=True)\n",
       "  )\n",
       "  (margin_layer): Linear(in_features=8, out_features=2, bias=True)\n",
       "  (margin_activation): Sigmoid()\n",
       "  (gcn_decoder): GCNDecoder(\n",
       "    (dense1): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (dense_bbx): Linear(in_features=64, out_features=4, bias=True)\n",
       "    (dense_bbx_refine): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (gconv_bbx_1): GCNConv(77, 128)\n",
       "    (gconv_bbx_2): GCNConv(128, 64)\n",
       "    (dense_lbl): Linear(in_features=64, out_features=1, bias=True)\n",
       "    (dense_edge): Linear(in_features=64, out_features=16, bias=True)\n",
       "    (dense_cls): Linear(in_features=64, out_features=7, bias=True)\n",
       "    (act1): Sigmoid()\n",
       "    (act2): Softmax(dim=None)\n",
       "    (act3): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6b72cd01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ww/3nytfk1917b8nz7dchtckxnm0000gn/T/ipykernel_2905/1910956142.py:38: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  vae.load_state_dict(torch.load(model_path+ '/model_weights.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.0813253  0.36144578 0.53614455]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.29819277 0.12048193 1.         0.7319277 ]\n",
      " [0.28915662 0.12048193 0.5572289  0.56626505]\n",
      " [0.57831323 0.68373495 0.68373495 0.90361446]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.44879517 0.7198795  0.5391566  0.8915663 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.7801205  0.71686745 0.81626505 0.8313253 ]\n",
      " [0.7680723  0.7981928  0.810241   0.9126506 ]\n",
      " [0.8072289  0.7319277  0.8674699  0.9186747 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.42388976 0.42486605 0.63427234 0.5890634 ]\n",
      " [0.39537886 0.28947705 0.54075146 0.28957057]\n",
      " [0.39537886 0.28947705 0.54075146 0.28957057]\n",
      " [0.30659488 0.32101607 0.70909154 0.5891483 ]\n",
      " [0.42388976 0.42486605 0.63427234 0.5890634 ]\n",
      " [0.42388976 0.42486605 0.63427234 0.5890634 ]\n",
      " [0.39537886 0.28947705 0.54075146 0.28957057]\n",
      " [0.42388976 0.42486605 0.63427234 0.5890634 ]\n",
      " [0.39537886 0.28947705 0.54075146 0.28957057]\n",
      " [0.39537886 0.28947705 0.54075146 0.28957057]\n",
      " [0.39537886 0.28947705 0.54075146 0.28957057]\n",
      " [0.4443867  0.440315   0.63333327 0.5877276 ]\n",
      " [0.45950693 0.5447494  0.60786617 0.6453128 ]\n",
      " [0.388213   0.4113179  0.65072113 0.6138502 ]\n",
      " [0.39537886 0.28947705 0.54075146 0.28957057]\n",
      " [0.39537886 0.28947705 0.54075146 0.28957057]]\n",
      "[[0.20619658 0.13596492 0.44523168 0.71929824]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.27417904 0.00219298 0.79610884 0.6118421 ]\n",
      " [0.403565   0.         0.6447931  0.4868421 ]\n",
      " [0.6645299  0.5        0.75882816 0.7763158 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.51102114 0.5504386  0.62724924 0.7653509 ]\n",
      " [0.53295094 0.75       0.6097053  1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.46283397 0.28836778 0.5577141  0.625885  ]\n",
      " [0.41831633 0.1633037  0.52586293 0.16346876]\n",
      " [0.41831633 0.1633037  0.52586293 0.16346876]\n",
      " [0.42670524 0.21737094 0.5970286  0.6118631 ]\n",
      " [0.46283397 0.28836778 0.5577141  0.625885  ]\n",
      " [0.48592374 0.32984722 0.54597664 0.6315909 ]\n",
      " [0.41831633 0.1633037  0.52586293 0.16346876]\n",
      " [0.42327756 0.22049442 0.6002989  0.61932147]\n",
      " [0.46688026 0.44238517 0.55837405 0.75120044]\n",
      " [0.41831633 0.1633037  0.52586293 0.16346876]\n",
      " [0.41831633 0.1633037  0.52586293 0.16346876]\n",
      " [0.41831633 0.1633037  0.52586293 0.16346876]\n",
      " [0.41831633 0.1633037  0.52586293 0.16346876]\n",
      " [0.41831633 0.1633037  0.52586293 0.16346876]\n",
      " [0.41831633 0.1633037  0.52586293 0.16346876]\n",
      " [0.41831633 0.1633037  0.52586293 0.16346876]]\n",
      "[[0.05248619 0.         0.9475138  1.        ]\n",
      " [0.09392265 0.1878453  0.18232045 0.29281768]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.2960731  0.29235798 0.60333747 0.61800754]\n",
      " [0.2960731  0.29235798 0.60333747 0.61800754]\n",
      " [0.4018548  0.23955981 0.53111935 0.23968498]\n",
      " [0.4018548  0.23955981 0.53111935 0.23968498]\n",
      " [0.4018548  0.23955981 0.53111935 0.23968498]\n",
      " [0.4018548  0.23955981 0.53111935 0.23968498]\n",
      " [0.4018548  0.23955981 0.53111935 0.23968498]\n",
      " [0.4018548  0.23955981 0.53111935 0.23968498]\n",
      " [0.4018548  0.23955981 0.53111935 0.23968498]\n",
      " [0.4018548  0.23955981 0.53111935 0.23968498]\n",
      " [0.4018548  0.23955981 0.53111935 0.23968498]\n",
      " [0.4018548  0.23955981 0.53111935 0.23968498]\n",
      " [0.4018548  0.23955981 0.53111935 0.23968498]\n",
      " [0.4018548  0.23955981 0.53111935 0.23968498]\n",
      " [0.4018548  0.23955981 0.53111935 0.23968498]\n",
      " [0.4018548  0.23955981 0.53111935 0.23968498]]\n",
      "[[0.01098901 0.21978022 0.7582418  0.7307692 ]\n",
      " [0.43406594 0.26923078 0.76373625 0.64835167]\n",
      " [0.         0.22527473 0.4010989  0.5769231 ]\n",
      " [0.03846154 0.06043956 1.         0.9395604 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.88461536 0.510989   0.99450547 0.72527474]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.38218173 0.44424143 0.6146347  0.59981596]\n",
      " [0.40471798 0.48362693 0.58731556 0.58885646]\n",
      " [0.40471798 0.48362693 0.58731556 0.58885646]\n",
      " [0.39531472 0.453415   0.5929235  0.5760809 ]\n",
      " [0.44334447 0.38186142 0.5243262  0.38191327]\n",
      " [0.44334447 0.38186142 0.5243262  0.38191327]\n",
      " [0.44334447 0.38186142 0.5243262  0.38191327]\n",
      " [0.44334447 0.38186142 0.5243262  0.38191327]\n",
      " [0.44334447 0.38186142 0.5243262  0.38191327]\n",
      " [0.44334447 0.38186142 0.5243262  0.38191327]\n",
      " [0.44334447 0.38186142 0.5243262  0.38191327]\n",
      " [0.44334447 0.38186142 0.5243262  0.38191327]\n",
      " [0.44334447 0.38186142 0.5243262  0.38191327]\n",
      " [0.40399748 0.551798   0.5746479  0.605399  ]\n",
      " [0.44334447 0.38186142 0.5243262  0.38191327]\n",
      " [0.44334447 0.38186142 0.5243262  0.38191327]]\n",
      "[[0.79545456 0.         0.969697   0.40151516]\n",
      " [0.42424244 0.8939394  0.48863637 0.9583333 ]\n",
      " [0.27272728 0.95454544 0.34469697 1.        ]\n",
      " [0.03030303 0.03030303 0.8787879  0.5984849 ]\n",
      " [0.48863637 0.03030303 0.81060606 0.38636363]\n",
      " [0.42424244 0.5378788  0.530303   0.6818182 ]\n",
      " [0.41287878 0.67424244 0.4848485  0.9583333 ]\n",
      " [0.31439394 0.5416667  0.3939394  0.78409094]\n",
      " [0.27272728 0.7878788  0.34469697 1.        ]\n",
      " [0.09848485 0.45075756 0.19318181 0.594697  ]\n",
      " [0.09469697 0.5681818  0.1590909  0.78409094]\n",
      " [0.04924243 0.40151516 0.125      0.56439394]\n",
      " [0.03787879 0.54924244 0.09469697 0.8068182 ]\n",
      " [0.06818182 0.56060606 0.18939394 0.7689394 ]\n",
      " [0.10227273 0.7462121  0.15151516 0.7878788 ]\n",
      " [0.03787879 0.7613636  0.08333334 0.8068182 ]] [[0.36245662 0.34906822 0.5492792  0.46163255]\n",
      " [0.3537338  0.5966727  0.44651678 0.6400257 ]\n",
      " [0.3537338  0.5966727  0.44651678 0.6400257 ]\n",
      " [0.32960418 0.3240169  0.6270477  0.5037634 ]\n",
      " [0.36245662 0.34906822 0.5492792  0.46163255]\n",
      " [0.35443008 0.4495108  0.4635049  0.52597386]\n",
      " [0.34396392 0.56255645 0.45971763 0.62847203]\n",
      " [0.35443008 0.4495108  0.4635049  0.52597386]\n",
      " [0.34396392 0.5625565  0.45971763 0.6284721 ]\n",
      " [0.35443008 0.4495108  0.4635049  0.52597386]\n",
      " [0.34396392 0.56255645 0.45971763 0.62847203]\n",
      " [0.35443008 0.4495108  0.4635049  0.52597386]\n",
      " [0.34396392 0.5625565  0.45971763 0.6284721 ]\n",
      " [0.36295623 0.33274007 0.58816445 0.47043625]\n",
      " [0.3537338  0.5966727  0.44651678 0.6400257 ]\n",
      " [0.3537338  0.5966727  0.44651678 0.6400257 ]]\n",
      "[[0.5479452  0.22819342 0.913242   0.5112984 ]\n",
      " [0.0456621  0.3971432  0.8721461  0.7304765 ]\n",
      " [0.6210046  0.42910665 0.8630137  0.6071889 ]\n",
      " [0.7808219  0.62088746 1.         0.73504275]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.47488585 0.6117551  0.7808219  0.7670062 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.06849315 0.63915235 0.4063927  0.76243997]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.5386957  0.06849315 0.6300199 ]\n",
      " [0.6803653  0.31495142 0.88584477 0.5112984 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.43763453 0.4573016  0.6284852  0.49715257]\n",
      " [0.37119153 0.45589817 0.64294535 0.5497602 ]\n",
      " [0.4612118  0.46457914 0.6169614  0.4927593 ]\n",
      " [0.44218686 0.5159497  0.604491   0.5497562 ]\n",
      " [0.49189982 0.41287196 0.5918484  0.41287455]\n",
      " [0.44218686 0.5159497  0.604491   0.5497562 ]\n",
      " [0.49189982 0.41287196 0.5918484  0.41287455]\n",
      " [0.49189982 0.41287196 0.5918484  0.41287455]\n",
      " [0.49189982 0.41287196 0.5918484  0.41287455]\n",
      " [0.44218686 0.5159497  0.604491   0.5497562 ]\n",
      " [0.49189982 0.41287196 0.5918484  0.41287455]\n",
      " [0.44218686 0.5159497  0.604491   0.5497562 ]\n",
      " [0.47515061 0.49117136 0.6077194  0.50377184]\n",
      " [0.49189982 0.41287196 0.5918484  0.41287455]\n",
      " [0.49189982 0.41287196 0.5918484  0.41287455]\n",
      " [0.49189982 0.41287196 0.5918484  0.41287455]]\n",
      "[[0.         0.         0.40425533 0.3723404 ]\n",
      " [0.14893617 0.09574468 0.9893617  0.9680851 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.11702128 0.63829786 0.39361703 1.        ]\n",
      " [0.12765957 0.9148936  0.27659574 1.        ]\n",
      " [0.07446808 0.62765956 0.31914893 0.9361702 ]\n",
      " [0.07446808 0.88297874 0.21276596 0.9468085 ]\n",
      " [0.26595744 0.88297874 0.5        0.9787234 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.21276596 0.18085106 0.3617021 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.4401897  0.49128833 0.49454474 0.54770064]\n",
      " [0.43692103 0.457914   0.5191695  0.54154146]\n",
      " [0.4692404  0.43237153 0.51226556 0.43240637]\n",
      " [0.4401897  0.49128833 0.49454474 0.54770064]\n",
      " [0.44150195 0.54077137 0.4769782  0.56049037]\n",
      " [0.4401897  0.49128833 0.49454474 0.54770064]\n",
      " [0.44150195 0.54077137 0.4769782  0.56049037]\n",
      " [0.4454207  0.47400036 0.50760305 0.53694236]\n",
      " [0.4692404  0.43237153 0.51226556 0.43240637]\n",
      " [0.4692404  0.43237153 0.51226556 0.43240637]\n",
      " [0.4692404  0.43237153 0.51226556 0.43240637]\n",
      " [0.4692404  0.43237153 0.51226556 0.43240637]\n",
      " [0.44150195 0.54077137 0.4769782  0.56049037]\n",
      " [0.4692404  0.43237153 0.51226556 0.43240637]\n",
      " [0.4692404  0.43237153 0.51226556 0.43240637]\n",
      " [0.4692404  0.43237153 0.51226556 0.43240637]]\n",
      "[[0.         0.07366375 0.23265307 0.2410107 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.10204082 0.1797862  0.75510204 0.6124393 ]\n",
      " [0.09795918 0.17570457 0.33061224 0.42468417]\n",
      " [0.18775511 0.5716229  0.24081632 0.7185617 ]\n",
      " [0.2        0.71039844 0.3265306  0.82468414]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.25714287 0.82468414 0.31428573 0.92264336]\n",
      " [0.5346939  0.58386785 0.75918365 0.7063168 ]\n",
      " [0.5836735  0.681827   0.75918365 0.82060254]\n",
      " [0.5877551  0.66141886 0.6571429  0.74713314]\n",
      " [0.46938777 0.73896986 0.6204082  0.89815354]\n",
      " [0.74693877 0.3552964  1.         0.63692904]\n",
      " [0.5836735  0.77570456 0.6612245  0.82060254]\n",
      " [0.46530613 0.8695821  0.51836735 0.89815354]] [[0.4051367  0.3869782  0.58268636 0.4697249 ]\n",
      " [0.41132402 0.33461252 0.5366765  0.3346921 ]\n",
      " [0.41132402 0.33461252 0.5366765  0.3346921 ]\n",
      " [0.34771138 0.36425927 0.64695466 0.51282495]\n",
      " [0.4051367  0.3869782  0.58268636 0.4697249 ]\n",
      " [0.4110416  0.3994469  0.5811121  0.4801327 ]\n",
      " [0.42339545 0.5396812  0.55344313 0.59685516]\n",
      " [0.41132402 0.33461252 0.5366765  0.3346921 ]\n",
      " [0.4449926  0.61757386 0.55596846 0.63475156]\n",
      " [0.43575746 0.48790982 0.57619095 0.5535499 ]\n",
      " [0.4206184  0.5674181  0.56697524 0.615962  ]\n",
      " [0.43575746 0.4879098  0.57619095 0.5535499 ]\n",
      " [0.42061839 0.5674181  0.56697524 0.615962  ]\n",
      " [0.38643867 0.37238556 0.6070724  0.4825552 ]\n",
      " [0.43698213 0.58712924 0.5476893  0.6190193 ]\n",
      " [0.43698213 0.58712924 0.5476893  0.6190193 ]]\n",
      "[[0.18237083 0.         0.6139818  0.45288754]\n",
      " [0.24012157 0.3799392  0.81155014 0.9574468 ]\n",
      " [0.27659574 0.38905776 0.60790277 0.5744681 ]\n",
      " [0.41945288 0.6018237  0.65045595 1.        ]\n",
      " [0.42553192 0.87841946 0.59574467 0.993921  ]\n",
      " [0.337386   0.6899696  0.4924012  0.9300912 ]\n",
      " [0.34346506 0.85106385 0.4650456  0.9240122 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.56231004 0.9179331  0.81762916 1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.440316   0.49136794 0.54280853 0.65747046]\n",
      " [0.40066427 0.40645576 0.594851   0.6905469 ]\n",
      " [0.440316   0.49136794 0.54280853 0.65747046]\n",
      " [0.44011903 0.5416645  0.54577464 0.6866128 ]\n",
      " [0.4568667  0.6689952  0.51420486 0.71986914]\n",
      " [0.44011903 0.5416645  0.54577464 0.6866128 ]\n",
      " [0.4568667  0.6689952  0.51420486 0.71986914]\n",
      " [0.4387304  0.24462213 0.5292369  0.24471626]\n",
      " [0.4387304  0.24462213 0.5292369  0.24471626]\n",
      " [0.4387304  0.24462213 0.5292369  0.24471626]\n",
      " [0.4387304  0.24462213 0.5292369  0.24471626]\n",
      " [0.42594022 0.42006698 0.56706554 0.6448717 ]\n",
      " [0.4387304  0.24462213 0.5292369  0.24471626]\n",
      " [0.4387304  0.24462213 0.5292369  0.24471626]\n",
      " [0.4387304  0.24462213 0.5292369  0.24471626]\n",
      " [0.4387304  0.24462213 0.5292369  0.24471626]]\n",
      "[[0.17344753 0.         0.8286938  0.84368306]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.18201284 0.53319055 0.42184153 0.95717347]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.17987151 0.86509633 0.32119915 1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.32689303 0.54874694 0.60497224 0.8487024 ]\n",
      " [0.3978684  0.13749509 0.5352094  0.13768816]\n",
      " [0.32689303 0.54874694 0.60497224 0.8487024 ]\n",
      " [0.3978684  0.13749509 0.5352094  0.13768816]\n",
      " [0.3978684  0.13749509 0.5352094  0.13768816]\n",
      " [0.3978684  0.13749509 0.5352094  0.13768816]\n",
      " [0.3978684  0.13749509 0.5352094  0.13768816]\n",
      " [0.3978684  0.13749509 0.5352094  0.13768816]\n",
      " [0.326893   0.54874676 0.60497224 0.8487024 ]\n",
      " [0.3978684  0.13749509 0.5352094  0.13768816]\n",
      " [0.3978684  0.13749509 0.5352094  0.13768816]\n",
      " [0.3978684  0.13749509 0.5352094  0.13768816]\n",
      " [0.3978684  0.13749509 0.5352094  0.13768816]\n",
      " [0.3978684  0.13749509 0.5352094  0.13768816]\n",
      " [0.3978684  0.13749509 0.5352094  0.13768816]\n",
      " [0.3978684  0.13749509 0.5352094  0.13768816]]\n",
      "[[0.61377245 0.13473053 1.         0.5449102 ]\n",
      " [0.30538923 0.49401197 0.93413174 0.8622754 ]\n",
      " [0.7814371  0.46107784 0.8712575  0.52395207]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.7185629  0.16467066 0.8083832 ]\n",
      " [0.2005988  0.7694611  0.2994012  0.8293413 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.00299401 0.66167665 0.2245509  0.8143712 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.3532248  0.55477095 0.560567   0.6411184 ]\n",
      " [0.3532248  0.55477095 0.560567   0.6411184 ]\n",
      " [0.3532248  0.55477095 0.560567   0.6411184 ]\n",
      " [0.40154952 0.32517734 0.5387112  0.3252504 ]\n",
      " [0.40154952 0.32517734 0.5387112  0.3252504 ]\n",
      " [0.35322484 0.55477095 0.560567   0.6411184 ]\n",
      " [0.35322487 0.5547709  0.56056714 0.64111835]\n",
      " [0.40154952 0.32517734 0.5387112  0.3252504 ]\n",
      " [0.35322487 0.5547709  0.56056714 0.64111835]\n",
      " [0.40154952 0.32517734 0.5387112  0.3252504 ]\n",
      " [0.40154952 0.32517734 0.5387112  0.3252504 ]\n",
      " [0.40154952 0.32517734 0.5387112  0.3252504 ]\n",
      " [0.40154952 0.32517734 0.5387112  0.3252504 ]\n",
      " [0.40154952 0.32517734 0.5387112  0.3252504 ]\n",
      " [0.40154952 0.32517734 0.5387112  0.3252504 ]\n",
      " [0.40154952 0.32517734 0.5387112  0.3252504 ]]\n",
      "[[0.38414633 0.         0.85365856 0.35365853]\n",
      " [0.16463415 0.09756097 0.60365856 0.6402439 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.43292683 0.5        0.5792683  0.89634144]\n",
      " [0.4512195  0.7621951  0.5731707  0.8902439 ]\n",
      " [0.25       0.5365854  0.41463414 1.        ]\n",
      " [0.30487806 0.89634144 0.40853658 1.        ]\n",
      " [0.33536586 0.6585366  0.41463414 0.7987805 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.1402439  0.61585367 0.22560975 0.9268293 ]\n",
      " [0.14634146 0.8231707  0.2195122  0.93292683]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.63414633 0.14634146 0.85365856 0.34756097]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.45466307 0.47440806 0.51402575 0.5685125 ]\n",
      " [0.44011045 0.4013477  0.5430027  0.5300576 ]\n",
      " [0.4660009  0.37580025 0.51748246 0.3758473 ]\n",
      " [0.45466307 0.47440806 0.51402575 0.5685125 ]\n",
      " [0.46231556 0.55147463 0.49618158 0.5926358 ]\n",
      " [0.45466307 0.47440806 0.51402575 0.5685125 ]\n",
      " [0.46231556 0.55147463 0.49618158 0.5926358 ]\n",
      " [0.46184918 0.44091237 0.5152666  0.52904284]\n",
      " [0.4660009  0.37580025 0.51748246 0.3758473 ]\n",
      " [0.45466307 0.47440806 0.51402575 0.5685125 ]\n",
      " [0.46231556 0.55147463 0.49618158 0.5926358 ]\n",
      " [0.4660009  0.37580025 0.51748246 0.3758473 ]\n",
      " [0.46231556 0.55147463 0.49618158 0.5926358 ]\n",
      " [0.4660009  0.37580025 0.51748246 0.3758473 ]\n",
      " [0.4660009  0.37580025 0.51748246 0.3758473 ]\n",
      " [0.4660009  0.37580025 0.51748246 0.3758473 ]]\n",
      "[[0.01587302 0.08163265 0.6825397  0.5442177 ]\n",
      " [0.16099773 0.49433106 0.5260771  0.7755102 ]\n",
      " [0.16780046 0.50340134 0.47619048 0.63038546]\n",
      " [0.45578232 0.27891156 1.         0.7256236 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.6281179  0.3356009  0.9206349 ]\n",
      " [0.00680272 0.723356   0.27210885 0.9206349 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.23613141 0.41511434 0.5256174  0.61039263]\n",
      " [0.20204803 0.3241474  0.65222836 0.6256399 ]\n",
      " [0.23613141 0.41511434 0.5256174  0.61039263]\n",
      " [0.25195485 0.33240628 0.61575186 0.571074  ]\n",
      " [0.3544026  0.21279158 0.55004513 0.21291095]\n",
      " [0.224103   0.43745136 0.527778   0.6407745 ]\n",
      " [0.2383864  0.6208157  0.44781214 0.7114352 ]\n",
      " [0.3544026  0.21279158 0.55004513 0.21291095]\n",
      " [0.3544026  0.21279158 0.55004513 0.21291095]\n",
      " [0.3544026  0.21279158 0.55004513 0.21291095]\n",
      " [0.3544026  0.21279158 0.55004513 0.21291095]\n",
      " [0.3544026  0.21279158 0.55004513 0.21291095]\n",
      " [0.3544026  0.21279158 0.55004513 0.21291095]\n",
      " [0.3544026  0.21279158 0.55004513 0.21291095]\n",
      " [0.3544026  0.21279158 0.55004513 0.21291095]\n",
      " [0.3544026  0.21279158 0.55004513 0.21291095]]\n",
      "[[3.1733118e-02 4.0683482e-04 3.8079739e-01 2.3840521e-01]\n",
      " [0.0000000e+00 1.7493898e-01 9.8372662e-01 1.0000000e+00]\n",
      " [1.1106591e-01 1.9080554e-01 3.8079739e-01 5.5573636e-01]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]] [[0.45914385 0.4641498  0.5008299  0.5105237 ]\n",
      " [0.45914385 0.4641498  0.5008299  0.5105237 ]\n",
      " [0.45914385 0.4641498  0.5008299  0.5105237 ]\n",
      " [0.48178723 0.45348114 0.50535965 0.45350227]\n",
      " [0.48178723 0.45348114 0.50535965 0.45350227]\n",
      " [0.48178723 0.45348114 0.50535965 0.45350227]\n",
      " [0.48178723 0.45348114 0.50535965 0.45350227]\n",
      " [0.48178723 0.45348114 0.50535965 0.45350227]\n",
      " [0.48178723 0.45348114 0.50535965 0.45350227]\n",
      " [0.48178723 0.45348114 0.50535965 0.45350227]\n",
      " [0.48178723 0.45348114 0.50535965 0.45350227]\n",
      " [0.48178723 0.45348114 0.50535965 0.45350227]\n",
      " [0.48178723 0.45348114 0.50535965 0.45350227]\n",
      " [0.48178723 0.45348114 0.50535965 0.45350227]\n",
      " [0.48178723 0.45348114 0.50535965 0.45350227]\n",
      " [0.48178723 0.45348114 0.50535965 0.45350227]]\n",
      "[[0.         0.07451923 0.7548077  0.77884614]\n",
      " [0.25240386 0.40144232 1.         0.9254808 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.22232914 0.3212664  0.7667559  0.70090806]\n",
      " [0.22232914 0.3212664  0.7667559  0.70090806]\n",
      " [0.35828716 0.23191728 0.5605569  0.23205   ]\n",
      " [0.35828716 0.23191728 0.5605569  0.23205   ]\n",
      " [0.35828716 0.23191728 0.5605569  0.23205   ]\n",
      " [0.35828716 0.23191728 0.5605569  0.23205   ]\n",
      " [0.35828716 0.23191728 0.5605569  0.23205   ]\n",
      " [0.35828716 0.23191728 0.5605569  0.23205   ]\n",
      " [0.35828716 0.23191728 0.5605569  0.23205   ]\n",
      " [0.35828716 0.23191728 0.5605569  0.23205   ]\n",
      " [0.35828716 0.23191728 0.5605569  0.23205   ]\n",
      " [0.35828716 0.23191728 0.5605569  0.23205   ]\n",
      " [0.35828716 0.23191728 0.5605569  0.23205   ]\n",
      " [0.35828716 0.23191728 0.5605569  0.23205   ]\n",
      " [0.35828716 0.23191728 0.5605569  0.23205   ]\n",
      " [0.35828716 0.23191728 0.5605569  0.23205   ]]\n",
      "[[0.         0.04712042 0.28010473 0.2434555 ]\n",
      " [0.21204188 0.13612565 0.8900524  0.5497382 ]\n",
      " [0.19895288 0.12827225 0.36125654 0.35078534]\n",
      " [0.39790577 0.539267   0.60209423 0.90052354]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.28010473 0.552356   0.42670158 0.9528796 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.7198953  0.36910996 0.8507853  0.85340315]\n",
      " [0.71727747 0.7801047  0.7905759  0.85863876]\n",
      " [0.6492147  0.41361257 0.7722513  0.75654453]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.81413615 0.18062827 1.         0.28010473]\n",
      " [0.0026178  0.13612565 0.10471204 0.2356021 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.36109328 0.26838192 0.6131283  0.41815925]\n",
      " [0.26791498 0.23970464 0.7302721  0.5127475 ]\n",
      " [0.39919323 0.30560902 0.5937035  0.45113623]\n",
      " [0.37687802 0.40627378 0.6125195  0.62603676]\n",
      " [0.43495566 0.22485586 0.5974564  0.22491466]\n",
      " [0.37687802 0.40627378 0.6125195  0.62603676]\n",
      " [0.43495566 0.22485586 0.5974564  0.22491466]\n",
      " [0.40048492 0.34390548 0.5851666  0.5039313 ]\n",
      " [0.42054942 0.4352075  0.5247077  0.53822684]\n",
      " [0.37687802 0.40627378 0.6125195  0.62603676]\n",
      " [0.43495566 0.22485586 0.5974564  0.22491466]\n",
      " [0.37687802 0.40627378 0.6125195  0.62603676]\n",
      " [0.41186965 0.3285716  0.54387236 0.43782377]\n",
      " [0.43495566 0.22485586 0.5974564  0.22491466]\n",
      " [0.43495566 0.22485586 0.5974564  0.22491466]\n",
      " [0.43495566 0.22485586 0.5974564  0.22491466]]\n",
      "[[0.34284532 0.         0.8025227  0.22580644]\n",
      " [0.39929694 0.16935484 0.87107116 0.516129  ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.6694582  0.5282258  0.7783292  0.75      ]\n",
      " [0.6694582  0.58870965 0.7944582  0.74596775]\n",
      " [0.13316791 0.35887095 0.67349046 1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.47468236 0.42057016 0.5781073  0.5660472 ]\n",
      " [0.43638426 0.3572213  0.6014215  0.5540614 ]\n",
      " [0.45486832 0.31688663 0.52743006 0.31695038]\n",
      " [0.45486832 0.31688663 0.52743006 0.31695038]\n",
      " [0.45486832 0.31688663 0.52743006 0.31695038]\n",
      " [0.45486832 0.31688663 0.52743006 0.31695038]\n",
      " [0.45486832 0.31688663 0.52743006 0.31695038]\n",
      " [0.4925577  0.43982682 0.59353817 0.58481485]\n",
      " [0.508047   0.5082655  0.5800484  0.6059263 ]\n",
      " [0.47468236 0.42057016 0.5781073  0.5660472 ]\n",
      " [0.45486832 0.31688663 0.52743006 0.31695038]\n",
      " [0.45486832 0.31688663 0.52743006 0.31695038]\n",
      " [0.45486832 0.31688663 0.52743006 0.31695038]\n",
      " [0.45486832 0.31688663 0.52743006 0.31695038]\n",
      " [0.45486832 0.31688663 0.52743006 0.31695038]\n",
      " [0.45486832 0.31688663 0.52743006 0.31695038]]\n",
      "[[0.         0.29333332 0.28666666 0.56      ]\n",
      " [0.13       0.22333333 0.88       0.6666667 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.40333334 0.63       0.5133333  0.7733333 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.16333333 0.62       0.29333332 0.75      ]\n",
      " [0.22333333 0.69666666 0.29333332 0.75333333]\n",
      " [0.48       0.56       0.6433333  0.75666666]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.28       0.67333335 0.42       0.77666664]\n",
      " [0.27666667 0.73333335 0.38333333 0.77666664]\n",
      " [0.83       0.22333333 1.         0.51666665]\n",
      " [0.03       0.35       0.14       0.47666666]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.33451217 0.50078356 0.5081935  0.5617187 ]\n",
      " [0.30786005 0.41344303 0.6334498  0.5372504 ]\n",
      " [0.40707248 0.38057083 0.5440674  0.3806134 ]\n",
      " [0.35637367 0.4854583  0.5401901  0.5566232 ]\n",
      " [0.40707248 0.38057083 0.5440674  0.3806134 ]\n",
      " [0.33451217 0.50078356 0.5081935  0.5617187 ]\n",
      " [0.33717433 0.53721976 0.45538884 0.56594765]\n",
      " [0.35637367 0.4854583  0.5401901  0.5566232 ]\n",
      " [0.40707248 0.38057083 0.5440674  0.3806134 ]\n",
      " [0.33451217 0.50078356 0.5081935  0.5617187 ]\n",
      " [0.33717433 0.53721976 0.45538884 0.56594765]\n",
      " [0.35637367 0.4854583  0.5401901  0.5566232 ]\n",
      " [0.33717433 0.53721976 0.45538884 0.56594765]\n",
      " [0.40707248 0.38057083 0.5440674  0.3806134 ]\n",
      " [0.40707248 0.38057083 0.5440674  0.3806134 ]\n",
      " [0.40707248 0.38057083 0.5440674  0.3806134 ]]\n",
      "[[0.82417583 0.48131868 1.         0.73846155]\n",
      " [0.0967033  0.36043957 0.9164835  0.63076925]\n",
      " [0.72527474 0.4021978  0.9142857  0.62197804]\n",
      " [0.11208791 0.26373628 0.33846155 0.31648353]\n",
      " [0.12967034 0.25494507 0.6967033  0.47032967]\n",
      " [0.3802198  0.5824176  0.53846157 0.6879121 ]\n",
      " [0.3802198  0.60879123 0.53846157 0.6835165 ]\n",
      " [0.45274726 0.5978022  0.6769231  0.74505496]\n",
      " [0.52307695 0.6769231  0.7010989  0.74505496]\n",
      " [0.         0.33186814 0.23956044 0.45934066]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.3318019  0.46313724 0.68555725 0.5234273 ]\n",
      " [0.24536267 0.4298818  0.75385004 0.5507655 ]\n",
      " [0.3318019  0.46313724 0.68555725 0.5234273 ]\n",
      " [0.3318019  0.46313724 0.68555725 0.5234273 ]\n",
      " [0.3318019  0.46313724 0.68555725 0.5234273 ]\n",
      " [0.3295552  0.49767542 0.7072059  0.55772126]\n",
      " [0.35607868 0.546414   0.6708182  0.5639826 ]\n",
      " [0.3295552  0.49767542 0.7072059  0.55772126]\n",
      " [0.35607868 0.546414   0.6708182  0.5639826 ]\n",
      " [0.31169653 0.4411068  0.6863838  0.53196   ]\n",
      " [0.38565513 0.33112466 0.63009614 0.33115432]\n",
      " [0.38565513 0.33112466 0.63009614 0.33115432]\n",
      " [0.38565513 0.33112466 0.63009614 0.33115432]\n",
      " [0.38565513 0.33112466 0.63009614 0.33115432]\n",
      " [0.38565513 0.33112466 0.63009614 0.33115432]\n",
      " [0.38565513 0.33112466 0.63009614 0.33115432]]\n",
      "[[0.         0.200409   1.         0.799591  ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.42740285 0.3599182  1.         0.797546  ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.1984706  0.35141295 0.85399723 0.6421783 ]\n",
      " [0.31074357 0.2670244  0.5948239  0.26714984]\n",
      " [0.31074357 0.2670244  0.5948239  0.26714984]\n",
      " [0.31074357 0.2670244  0.5948239  0.26714984]\n",
      " [0.31074357 0.2670244  0.5948239  0.26714984]\n",
      " [0.31074357 0.2670244  0.5948239  0.26714984]\n",
      " [0.31074357 0.2670244  0.5948239  0.26714984]\n",
      " [0.31074357 0.2670244  0.5948239  0.26714984]\n",
      " [0.31074357 0.2670244  0.5948239  0.26714984]\n",
      " [0.31074357 0.2670244  0.5948239  0.26714984]\n",
      " [0.31074357 0.2670244  0.5948239  0.26714984]\n",
      " [0.31074357 0.2670244  0.5948239  0.26714984]\n",
      " [0.1984706  0.35141295 0.85399723 0.6421783 ]\n",
      " [0.31074357 0.2670244  0.5948239  0.26714984]\n",
      " [0.31074357 0.2670244  0.5948239  0.26714984]\n",
      " [0.31074357 0.2670244  0.5948239  0.26714984]]\n",
      "[[0.10944027 0.         0.52610695 0.27631578]\n",
      " [0.28487885 0.1491228  0.7804929  0.7368421 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.39014202 0.6447368  0.45154554 0.7236842 ]\n",
      " [0.39452797 0.67105263 0.4427736  0.7236842 ]\n",
      " [0.3682122  0.63596493 0.39452797 0.6666667 ]\n",
      " [0.3682122  0.63596493 0.38575605 0.65789473]\n",
      " [0.5304929  0.7017544  0.894528   1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.43954253 0.45017177 0.53263915 0.56604123]\n",
      " [0.41302818 0.38967186 0.56659615 0.55992186]\n",
      " [0.4449336  0.31870344 0.5184999  0.3187724 ]\n",
      " [0.4449336  0.31870344 0.5184999  0.3187724 ]\n",
      " [0.4449336  0.31870344 0.5184999  0.3187724 ]\n",
      " [0.44017714 0.50942695 0.50751054 0.58863103]\n",
      " [0.45231405 0.5523282  0.48742333 0.5943691 ]\n",
      " [0.44017714 0.50942695 0.50751054 0.58863103]\n",
      " [0.45231405 0.5523282  0.48742333 0.5943691 ]\n",
      " [0.43954253 0.45017177 0.53263915 0.56604123]\n",
      " [0.4449336  0.31870344 0.5184999  0.3187724 ]\n",
      " [0.4449336  0.31870344 0.5184999  0.3187724 ]\n",
      " [0.4449336  0.31870344 0.5184999  0.3187724 ]\n",
      " [0.4449336  0.31870344 0.5184999  0.3187724 ]\n",
      " [0.4449336  0.31870344 0.5184999  0.3187724 ]\n",
      " [0.4449336  0.31870344 0.5184999  0.3187724 ]]\n",
      "[[0.71794873 0.16239317 1.         0.47863248]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.75213677 0.15384616 0.8803419  0.30769232]\n",
      " [0.         0.18803419 0.9059829  0.7692308 ]\n",
      " [0.60683763 0.1965812  0.85470086 0.50427353]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.04273504 0.61538464 0.62393165 0.85470086]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.44203746 0.46180424 0.58027536 0.5308893 ]\n",
      " [0.46364132 0.43635038 0.51923066 0.4363759 ]\n",
      " [0.4601557  0.49231339 0.56949407 0.54037714]\n",
      " [0.44203746 0.46180424 0.58027536 0.5308893 ]\n",
      " [0.4787975  0.48307955 0.57470685 0.52180487]\n",
      " [0.46364132 0.43635038 0.51923066 0.4363759 ]\n",
      " [0.46364132 0.43635038 0.51923066 0.4363759 ]\n",
      " [0.46364132 0.43635038 0.51923066 0.4363759 ]\n",
      " [0.46364132 0.43635038 0.51923066 0.4363759 ]\n",
      " [0.46364132 0.43635038 0.51923066 0.4363759 ]\n",
      " [0.46364132 0.43635038 0.51923066 0.4363759 ]\n",
      " [0.46364132 0.43635038 0.51923066 0.4363759 ]\n",
      " [0.46364132 0.43635038 0.51923066 0.4363759 ]\n",
      " [0.4601557  0.49231339 0.56949407 0.54037714]\n",
      " [0.46364132 0.43635038 0.51923066 0.4363759 ]\n",
      " [0.46364132 0.43635038 0.51923066 0.4363759 ]]\n",
      "[[0.2997988  0.23133674 1.         0.8691637 ]\n",
      " [0.         0.12872104 0.9919517  0.71020997]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.19913988 0.29016376 0.8389743  0.6551948 ]\n",
      " [0.19913988 0.29016376 0.8389743  0.6551948 ]\n",
      " [0.33681643 0.22828527 0.5814506  0.22841522]\n",
      " [0.33681643 0.22828527 0.5814506  0.22841522]\n",
      " [0.33681643 0.22828527 0.5814506  0.22841522]\n",
      " [0.33681643 0.22828527 0.5814506  0.22841522]\n",
      " [0.33681643 0.22828527 0.5814506  0.22841522]\n",
      " [0.33681643 0.22828527 0.5814506  0.22841522]\n",
      " [0.33681643 0.22828527 0.5814506  0.22841522]\n",
      " [0.33681643 0.22828527 0.5814506  0.22841522]\n",
      " [0.33681643 0.22828527 0.5814506  0.22841522]\n",
      " [0.33681643 0.22828527 0.5814506  0.22841522]\n",
      " [0.33681643 0.22828527 0.5814506  0.22841522]\n",
      " [0.33681643 0.22828527 0.5814506  0.22841522]\n",
      " [0.33681643 0.22828527 0.5814506  0.22841522]\n",
      " [0.33681643 0.22828527 0.5814506  0.22841522]]\n",
      "[[0.2983871  0.5282258  0.41532257 0.63709676]\n",
      " [0.3669355  0.41129032 0.6854839  0.62096775]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.5241935  0.48790324 1.         0.8346774 ]\n",
      " [0.         0.16129032 0.55241936 0.5       ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.6330645  0.3669355  0.8266129  0.45564517]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.4295957  0.46197516 0.58052015 0.5222152 ]\n",
      " [0.3839577  0.42928696 0.6178556  0.5325302 ]\n",
      " [0.463794   0.37036666 0.5801438  0.37038347]\n",
      " [0.43351975 0.4757546  0.59252894 0.5298506 ]\n",
      " [0.43351975 0.4757546  0.59252894 0.5298506 ]\n",
      " [0.463794   0.37036666 0.5801438  0.37038347]\n",
      " [0.463794   0.37036666 0.5801438  0.37038347]\n",
      " [0.463794   0.37036666 0.5801438  0.37038347]\n",
      " [0.463794   0.37036666 0.5801438  0.37038347]\n",
      " [0.4295957  0.46197516 0.58052015 0.5222152 ]\n",
      " [0.463794   0.37036666 0.5801438  0.37038347]\n",
      " [0.463794   0.37036666 0.5801438  0.37038347]\n",
      " [0.463794   0.37036666 0.5801438  0.37038347]\n",
      " [0.463794   0.37036666 0.5801438  0.37038347]\n",
      " [0.463794   0.37036666 0.5801438  0.37038347]\n",
      " [0.463794   0.37036666 0.5801438  0.37038347]]\n",
      "[[0.31374422 0.         0.75473803 0.33850932]\n",
      " [0.24852684 0.28881988 0.72989327 0.7795031 ]\n",
      " [0.48455167 0.28881988 0.71747094 0.39130434]\n",
      " [0.6025641  0.5031056  0.75473803 0.83229816]\n",
      " [0.6367256  0.76086956 0.75163245 0.8291925 ]\n",
      " [0.33548334 0.636646   0.46902373 0.9968944 ]\n",
      " [0.36964485 0.91925466 0.46591815 1.        ]\n",
      " [0.40070075 0.66459626 0.53113556 0.73602486]\n",
      " [0.43796784 0.66770184 0.5280299  0.71118015]\n",
      " [0.24852684 0.7826087  0.2671604  0.8416149 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.2920051  0.71428573 0.3541169  0.7919255 ]\n",
      " [0.3199554  0.1583851  0.5746138  0.33229813]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.4501419  0.38111758 0.54978067 0.5552347 ]\n",
      " [0.4222786  0.3350285  0.57899594 0.6077794 ]\n",
      " [0.46532762 0.4283234  0.53878355 0.5622567 ]\n",
      " [0.46838942 0.5476638  0.53795886 0.6463429 ]\n",
      " [0.48197824 0.65009224 0.5169963  0.67559445]\n",
      " [0.46838942 0.5476638  0.53795886 0.6463429 ]\n",
      " [0.48197824 0.65009224 0.5169963  0.67559445]\n",
      " [0.46838942 0.5476638  0.53795886 0.6463429 ]\n",
      " [0.48197824 0.65009224 0.5169963  0.67559445]\n",
      " [0.4536263  0.42775035 0.5447462  0.5978762 ]\n",
      " [0.46903846 0.2622384  0.5392358  0.262289  ]\n",
      " [0.4536263  0.42775035 0.5447462  0.5978762 ]\n",
      " [0.47390902 0.5065896  0.5252219  0.5904431 ]\n",
      " [0.46903846 0.2622384  0.5392358  0.262289  ]\n",
      " [0.46903846 0.2622384  0.5392358  0.262289  ]\n",
      " [0.46903846 0.2622384  0.5392358  0.262289  ]]\n",
      "[[0.         0.27440476 0.1875     0.43690476]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.1375     0.31190476 0.925      0.73690474]\n",
      " [0.1375     0.33690476 0.2875     0.53690475]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.8875     0.44940478 1.         0.5744048 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.45392188 0.49130583 0.5125322  0.5031738 ]\n",
      " [0.47250736 0.47057918 0.5097756  0.4705915 ]\n",
      " [0.47250736 0.47057918 0.5097756  0.4705915 ]\n",
      " [0.448013   0.48119137 0.53378963 0.50243545]\n",
      " [0.45392188 0.49130583 0.5125322  0.5031738 ]\n",
      " [0.47250736 0.47057918 0.5097756  0.4705915 ]\n",
      " [0.47250736 0.47057918 0.5097756  0.4705915 ]\n",
      " [0.47250736 0.47057918 0.5097756  0.4705915 ]\n",
      " [0.47250736 0.47057918 0.5097756  0.4705915 ]\n",
      " [0.47250736 0.47057918 0.5097756  0.4705915 ]\n",
      " [0.47250736 0.47057918 0.5097756  0.4705915 ]\n",
      " [0.47250736 0.47057918 0.5097756  0.4705915 ]\n",
      " [0.47250736 0.47057918 0.5097756  0.4705915 ]\n",
      " [0.45556563 0.4884881  0.5234217  0.50606537]\n",
      " [0.47250736 0.47057918 0.5097756  0.4705915 ]\n",
      " [0.47250736 0.47057918 0.5097756  0.4705915 ]]\n",
      "[[0.82844245 0.50790066 1.         0.7765237 ]\n",
      " [0.09255079 0.3589165  0.9322799  0.6433409 ]\n",
      " [0.738149   0.4108352  0.9300226  0.63656884]\n",
      " [0.1241535  0.22347629 0.3589165  0.27539504]\n",
      " [0.13995485 0.23250565 0.717833   0.44695258]\n",
      " [0.36794582 0.56207675 0.53047407 0.67945826]\n",
      " [0.36794582 0.5891648  0.53047407 0.67494357]\n",
      " [0.44469526 0.58690745 0.66591424 0.751693  ]\n",
      " [0.50790066 0.67494357 0.69074494 0.751693  ]\n",
      " [0.         0.28216705 0.24604966 0.4108352 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.33386803 0.4546522  0.6824123  0.5247796 ]\n",
      " [0.24985155 0.4172746  0.74946165 0.5554209 ]\n",
      " [0.33386803 0.4546522  0.6824123  0.5247796 ]\n",
      " [0.33386803 0.4546522  0.6824123  0.5247796 ]\n",
      " [0.33386803 0.4546522  0.6824123  0.5247796 ]\n",
      " [0.33115846 0.4919212  0.70323586 0.5622328 ]\n",
      " [0.35658985 0.54595226 0.6677374  0.56941926]\n",
      " [0.33115846 0.4919212  0.70323586 0.5622328 ]\n",
      " [0.35658985 0.54595226 0.6677374  0.56941926]\n",
      " [0.31430048 0.43287817 0.6838548  0.53660005]\n",
      " [0.38687992 0.31446385 0.62476856 0.3144978 ]\n",
      " [0.38687992 0.31446385 0.62476856 0.3144978 ]\n",
      " [0.38687992 0.31446385 0.62476856 0.3144978 ]\n",
      " [0.38687992 0.31446385 0.62476856 0.3144978 ]\n",
      " [0.38687992 0.31446385 0.62476856 0.3144978 ]\n",
      " [0.38687992 0.31446385 0.62476856 0.3144978 ]]\n",
      "[[0.13917525 0.         0.69072163 0.6030928 ]\n",
      " [0.12886597 0.52061856 0.87113404 1.        ]\n",
      " [0.36082473 0.4742268  0.5979381  0.6958763 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.43310696 0.4551345  0.5482523  0.59210104]\n",
      " [0.43310696 0.4551345  0.5482523  0.59210104]\n",
      " [0.43310696 0.4551345  0.5482523  0.59210104]\n",
      " [0.46034026 0.35675538 0.52115476 0.3568105 ]\n",
      " [0.46034026 0.35675538 0.52115476 0.3568105 ]\n",
      " [0.46034026 0.35675538 0.52115476 0.3568105 ]\n",
      " [0.46034026 0.35675538 0.52115476 0.3568105 ]\n",
      " [0.46034026 0.35675538 0.52115476 0.3568105 ]\n",
      " [0.46034026 0.35675538 0.52115476 0.3568105 ]\n",
      " [0.46034026 0.35675538 0.52115476 0.3568105 ]\n",
      " [0.46034026 0.35675538 0.52115476 0.3568105 ]\n",
      " [0.46034026 0.35675538 0.52115476 0.3568105 ]\n",
      " [0.46034026 0.35675538 0.52115476 0.3568105 ]\n",
      " [0.46034026 0.35675538 0.52115476 0.3568105 ]\n",
      " [0.46034026 0.35675538 0.52115476 0.3568105 ]\n",
      " [0.46034026 0.35675538 0.52115476 0.3568105 ]]\n",
      "[[0.72289157 0.30522087 1.         0.61445785]\n",
      " [0.01606426 0.26907632 0.78313255 0.6827309 ]\n",
      " [0.6506024  0.32128513 0.78313255 0.5582329 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.5261044  0.53815264 0.8995984  0.6506024 ]\n",
      " [0.81124496 0.5582329  0.939759   0.6184739 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.5421687  0.6706827  0.7309237 ]\n",
      " [0.875502   0.502008   0.99196786 0.6184739 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.47246894 0.4571664  0.6716081  0.50198543]\n",
      " [0.45400363 0.4558251  0.6767195  0.5112308 ]\n",
      " [0.5184897  0.48563296 0.66270596 0.5120635 ]\n",
      " [0.4850413  0.41073567 0.596588   0.41074252]\n",
      " [0.4850413  0.41073567 0.596588   0.41074252]\n",
      " [0.5286617  0.4989976  0.6704947  0.5231144 ]\n",
      " [0.5599123  0.53483766 0.662678   0.5388752 ]\n",
      " [0.4850413  0.41073567 0.596588   0.41074252]\n",
      " [0.4850413  0.41073567 0.596588   0.41074252]\n",
      " [0.4850413  0.41073567 0.596588   0.41074252]\n",
      " [0.4850413  0.41073567 0.596588   0.41074252]\n",
      " [0.47883344 0.48000002 0.64789784 0.51854527]\n",
      " [0.51789004 0.49936625 0.65526855 0.52196   ]\n",
      " [0.4850413  0.41073567 0.596588   0.41074252]\n",
      " [0.4850413  0.41073567 0.596588   0.41074252]\n",
      " [0.4850413  0.41073567 0.596588   0.41074252]]\n",
      "[[0.06682578 0.         0.70167065 0.55369925]\n",
      " [0.02625298 0.44152746 0.973747   1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.23120056 0.2598549  0.7381601  0.7347237 ]\n",
      " [0.23120056 0.2598549  0.7381601  0.7347237 ]\n",
      " [0.3659901  0.18269001 0.5499207  0.1828502 ]\n",
      " [0.3659901  0.18269001 0.5499207  0.1828502 ]\n",
      " [0.3659901  0.18269001 0.5499207  0.1828502 ]\n",
      " [0.3659901  0.18269001 0.5499207  0.1828502 ]\n",
      " [0.3659901  0.18269001 0.5499207  0.1828502 ]\n",
      " [0.3659901  0.18269001 0.5499207  0.1828502 ]\n",
      " [0.3659901  0.18269001 0.5499207  0.1828502 ]\n",
      " [0.3659901  0.18269001 0.5499207  0.1828502 ]\n",
      " [0.3659901  0.18269001 0.5499207  0.1828502 ]\n",
      " [0.3659901  0.18269001 0.5499207  0.1828502 ]\n",
      " [0.3659901  0.18269001 0.5499207  0.1828502 ]\n",
      " [0.3659901  0.18269001 0.5499207  0.1828502 ]\n",
      " [0.3659901  0.18269001 0.5499207  0.1828502 ]\n",
      " [0.3659901  0.18269001 0.5499207  0.1828502 ]]\n",
      "[[0.24821669 0.         0.5677656  0.22180451]\n",
      " [0.39859262 0.13909775 0.62039715 0.33834586]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.35723925 0.22932331 0.6730287  0.87218046]\n",
      " [0.571525   0.17293233 0.7557355  0.8120301 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.5076152  0.7293233  0.6542317  1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.48832074 0.3917629  0.53296566 0.5470096 ]\n",
      " [0.46551982 0.33798006 0.5537877  0.55570686]\n",
      " [0.48484004 0.30358747 0.5339276  0.3036251 ]\n",
      " [0.50213313 0.40014398 0.53874683 0.5484855 ]\n",
      " [0.50213313 0.40014398 0.53874683 0.5484855 ]\n",
      " [0.48484004 0.30358747 0.5339276  0.3036251 ]\n",
      " [0.48484004 0.30358747 0.5339276  0.3036251 ]\n",
      " [0.48484004 0.30358747 0.5339276  0.3036251 ]\n",
      " [0.48484004 0.30358747 0.5339276  0.3036251 ]\n",
      " [0.48832074 0.3917629  0.53296566 0.5470096 ]\n",
      " [0.48484004 0.30358747 0.5339276  0.3036251 ]\n",
      " [0.48484004 0.30358747 0.5339276  0.3036251 ]\n",
      " [0.48484004 0.30358747 0.5339276  0.3036251 ]\n",
      " [0.48484004 0.30358747 0.5339276  0.3036251 ]\n",
      " [0.48484004 0.30358747 0.5339276  0.3036251 ]\n",
      " [0.48484004 0.30358747 0.5339276  0.3036251 ]]\n",
      "[[0.5988372  0.         0.96511626 0.21511628]\n",
      " [0.03488372 0.21511628 0.872093   1.        ]\n",
      " [0.5813953  0.13953489 0.8023256  0.26744187]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.4830199  0.40194994 0.5904191  0.51255125]\n",
      " [0.4830199  0.40194994 0.5904191  0.51255125]\n",
      " [0.4830199  0.40194994 0.5904191  0.51255125]\n",
      " [0.46142754 0.37299955 0.5245754  0.37304622]\n",
      " [0.46142754 0.37299955 0.5245754  0.37304622]\n",
      " [0.46142754 0.37299955 0.5245754  0.37304622]\n",
      " [0.46142754 0.37299955 0.5245754  0.37304622]\n",
      " [0.46142754 0.37299955 0.5245754  0.37304622]\n",
      " [0.46142754 0.37299955 0.5245754  0.37304622]\n",
      " [0.46142754 0.37299955 0.5245754  0.37304622]\n",
      " [0.46142754 0.37299955 0.5245754  0.37304622]\n",
      " [0.46142754 0.37299955 0.5245754  0.37304622]\n",
      " [0.46142754 0.37299955 0.5245754  0.37304622]\n",
      " [0.46142754 0.37299955 0.5245754  0.37304622]\n",
      " [0.46142754 0.37299955 0.5245754  0.37304622]\n",
      " [0.46142754 0.37299955 0.5245754  0.37304622]]\n",
      "[[0.2983871  0.5282258  0.41532257 0.63709676]\n",
      " [0.3669355  0.41129032 0.6854839  0.62096775]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.5241935  0.48790324 1.         0.8346774 ]\n",
      " [0.         0.16129032 0.55241936 0.5       ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.6330645  0.3669355  0.8266129  0.45564517]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.43138292 0.46291292 0.5784482  0.52170515]\n",
      " [0.3868844  0.4310192  0.61488044 0.5317733 ]\n",
      " [0.46462673 0.37352845 0.5781151  0.3735449 ]\n",
      " [0.4352144  0.47638598 0.5901463  0.52918684]\n",
      " [0.4352144  0.47638598 0.5901463  0.52918684]\n",
      " [0.46462673 0.37352845 0.5781151  0.3735449 ]\n",
      " [0.46462673 0.37352845 0.5781151  0.3735449 ]\n",
      " [0.46462673 0.37352845 0.5781151  0.3735449 ]\n",
      " [0.46462673 0.37352845 0.5781151  0.3735449 ]\n",
      " [0.43138292 0.46291292 0.5784482  0.52170515]\n",
      " [0.46462673 0.37352845 0.5781151  0.3735449 ]\n",
      " [0.46462673 0.37352845 0.5781151  0.3735449 ]\n",
      " [0.46462673 0.37352845 0.5781151  0.3735449 ]\n",
      " [0.46462673 0.37352845 0.5781151  0.3735449 ]\n",
      " [0.46462673 0.37352845 0.5781151  0.3735449 ]\n",
      " [0.46462673 0.37352845 0.5781151  0.3735449 ]]\n",
      "[[0.25082508 0.07920792 0.8019802  0.7260726 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.1980198  0.         0.7458746  0.69636965]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.42574257 0.63366336 0.5214521  0.8151815 ]\n",
      " [0.45874587 0.8118812  0.55775577 1.        ]\n",
      " [0.23762377 0.5049505  0.34653464 0.8151815 ]\n",
      " [0.24092409 0.8118812  0.32013202 0.990099  ]\n",
      " [0.68646866 0.55775577 0.75247526 0.76567656]\n",
      " [0.70627064 0.7590759  0.7491749  0.90759075]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.5907591  0.71947193 0.660066   0.9207921 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.45950258 0.3727985  0.54708886 0.5594228 ]\n",
      " [0.43963146 0.28201738 0.526132   0.28213972]\n",
      " [0.43963146 0.28201738 0.526132   0.28213972]\n",
      " [0.4564412  0.43746328 0.5574348  0.64920723]\n",
      " [0.43963146 0.28201738 0.526132   0.28213972]\n",
      " [0.47201997 0.49314624 0.5413902  0.6533065 ]\n",
      " [0.48045284 0.577623   0.52961636 0.6782294 ]\n",
      " [0.47201997 0.49314624 0.5413901  0.6533065 ]\n",
      " [0.48045284 0.577623   0.52961636 0.6782294 ]\n",
      " [0.4726158  0.4324603  0.53281087 0.60130787]\n",
      " [0.4799338  0.54803425 0.522773   0.6629013 ]\n",
      " [0.43963146 0.28201738 0.526132   0.28213972]\n",
      " [0.49026376 0.62898576 0.50538707 0.6862695 ]\n",
      " [0.43963146 0.28201738 0.526132   0.28213972]\n",
      " [0.43963146 0.28201738 0.526132   0.28213972]\n",
      " [0.43963146 0.28201738 0.526132   0.28213972]]\n",
      "[[0.497006   0.15568863 0.6287425  0.2754491 ]\n",
      " [0.41317365 0.251497   0.6347305  0.5269461 ]\n",
      " [0.51497006 0.23952095 0.5508982  0.28143713]\n",
      " [0.6886228  0.11976048 0.7724551  0.24550898]\n",
      " [0.6047904  0.20958084 0.7305389  0.34131736]\n",
      " [0.6766467  0.         0.7784431  0.13772455]\n",
      " [0.28143713 0.19161677 0.4011976  0.31137726]\n",
      " [0.34131736 0.22155689 0.4730539  0.3473054 ]\n",
      " [0.22155689 0.15568863 0.3233533  0.22155689]\n",
      " [0.41916168 0.7245509  0.5269461  0.9221557 ]\n",
      " [0.44311377 0.46706587 0.61077845 0.76047903]\n",
      " [0.37125748 0.8982036  0.4550898  1.        ]\n",
      " [0.33532935 0.7185629  0.4311377  0.8982036 ]\n",
      " [0.37125748 0.46107784 0.52095807 0.75449103]\n",
      " [0.31736526 0.8742515  0.4011976  0.97604793]\n",
      " [0.         0.         0.         0.        ]] [[0.48235863 0.4017005  0.5106048  0.45916885]\n",
      " [0.4783047  0.38962716 0.51964414 0.4663336 ]\n",
      " [0.48235863 0.4017005  0.5106048  0.45916885]\n",
      " [0.48310977 0.46618706 0.50865453 0.5479963 ]\n",
      " [0.4817944  0.4324053  0.5105823  0.50810033]\n",
      " [0.48742917 0.51983595 0.5023901  0.5710324 ]\n",
      " [0.48310977 0.46618706 0.50865453 0.5479963 ]\n",
      " [0.4817944  0.4324053  0.5105823  0.50810033]\n",
      " [0.48742917 0.51983595 0.5023901  0.5710324 ]\n",
      " [0.48310977 0.46618706 0.50865453 0.5479963 ]\n",
      " [0.4817944  0.4324053  0.5105823  0.50810033]\n",
      " [0.48742917 0.51983595 0.5023901  0.5710324 ]\n",
      " [0.48310977 0.46618706 0.50865453 0.5479963 ]\n",
      " [0.4817944  0.4324053  0.5105823  0.50810033]\n",
      " [0.48742917 0.51983595 0.5023901  0.5710324 ]\n",
      " [0.47413588 0.37036687 0.51662886 0.3704033 ]]\n",
      "[[0.5467626  0.352518   0.96642685 0.6211031 ]\n",
      " [0.07913669 0.13429257 0.8729017  0.76978415]\n",
      " [0.57793766 0.5851319  0.88249403 0.7146283 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.47242206 0.66906476 0.64748204 1.        ]\n",
      " [0.5083933  0.87529975 0.58992803 0.99520385]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.10791367 0.4292566  0.19664268 0.5539568 ]\n",
      " [0.13429257 0.470024   0.20143884 0.5467626 ]\n",
      " [0.03597122 0.         0.19424461 0.2326139 ]\n",
      " [0.64748204 0.5083933  0.8489209  0.6067146 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.32245153 0.2852466  0.66635096 0.51590455]\n",
      " [0.2916215  0.27703342 0.7043811  0.5772616 ]\n",
      " [0.37270457 0.41646773 0.6207671  0.5992661 ]\n",
      " [0.37883034 0.18421116 0.56021047 0.18433361]\n",
      " [0.37883034 0.18421116 0.56021047 0.18433361]\n",
      " [0.3643064  0.4804551  0.6299752  0.65403   ]\n",
      " [0.3937127  0.6111989  0.5949715  0.6901631 ]\n",
      " [0.37883034 0.18421116 0.56021047 0.18433361]\n",
      " [0.37883034 0.18421116 0.56021047 0.18433361]\n",
      " [0.3643064  0.4804551  0.6299752  0.65403   ]\n",
      " [0.3937127  0.6111989  0.5949715  0.6901631 ]\n",
      " [0.35080838 0.31278318 0.63574684 0.5197765 ]\n",
      " [0.37727898 0.4477131  0.60584044 0.5966265 ]\n",
      " [0.37883034 0.18421116 0.56021047 0.18433361]\n",
      " [0.37883034 0.18421116 0.56021047 0.18433361]\n",
      " [0.37883034 0.18421116 0.56021047 0.18433361]]\n",
      "[[0.39488095 0.         0.6723809  0.385     ]\n",
      " [0.34738097 0.23       0.6098809  0.505     ]\n",
      " [0.51238096 0.21       0.59488094 0.295     ]\n",
      " [0.559881   0.445      0.6098809  0.5225    ]\n",
      " [0.617381   0.285      0.65238094 0.37      ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.36738095 0.4025     0.49238095 0.6       ]\n",
      " [0.37988096 0.225      0.48488095 0.4475    ]\n",
      " [0.45738095 0.5075     0.52988094 0.5875    ]\n",
      " [0.44988096 0.815      0.52988094 0.925     ]\n",
      " [0.48738095 0.585      0.5473809  0.7625    ]\n",
      " [0.44238096 0.95       0.49238095 1.        ]\n",
      " [0.34738097 0.7175     0.52488095 0.9625    ]\n",
      " [0.32988095 0.4825     0.52238095 0.765     ]\n",
      " [0.34738097 0.94       0.46488094 0.995     ]\n",
      " [0.         0.         0.         0.        ]] [[0.477605   0.2721919  0.5168861  0.48832723]\n",
      " [0.46336275 0.23997654 0.5389865  0.5499383 ]\n",
      " [0.477605   0.2721919  0.5168861  0.48832723]\n",
      " [0.49196547 0.53494287 0.50473285 0.71209544]\n",
      " [0.47918373 0.3057983  0.5174806  0.55672044]\n",
      " [0.4562415  0.18193485 0.5159364  0.18206626]\n",
      " [0.4845907  0.5162698  0.51317227 0.730723  ]\n",
      " [0.48441964 0.39522016 0.51035535 0.62911564]\n",
      " [0.49659398 0.6625503  0.49880224 0.7629595 ]\n",
      " [0.4845907  0.5162698  0.51317227 0.730723  ]\n",
      " [0.48441964 0.39522016 0.51035535 0.62911564]\n",
      " [0.49659398 0.6625503  0.49880224 0.7629595 ]\n",
      " [0.4845907  0.5162698  0.51317227 0.730723  ]\n",
      " [0.48441964 0.39522016 0.51035535 0.62911564]\n",
      " [0.49659398 0.6625503  0.49880224 0.7629595 ]\n",
      " [0.4562415  0.18193485 0.5159364  0.18206626]]\n",
      "[[0.77372265 0.01077511 1.         0.47062913]\n",
      " [0.609489   0.93413275 0.69343066 0.9888773 ]\n",
      " [0.42335767 0.91953427 0.4890511  0.99252695]\n",
      " [0.08394161 0.03632256 0.93430656 0.57281893]\n",
      " [0.649635   0.06187    0.8540146  0.36843935]\n",
      " [0.59489053 0.5691693  0.6970803  0.8173444 ]\n",
      " [0.580292   0.8173444  0.689781   0.9888773 ]\n",
      " [0.5        0.51442474 0.58759123 0.77354884]\n",
      " [0.41970804 0.7370525  0.5729927  0.99252695]\n",
      " [0.20437956 0.4779284  0.30656934 0.6750087 ]\n",
      " [0.20072992 0.71150506 0.24817519 0.9049357 ]\n",
      " [0.11678832 0.41953424 0.24087591 0.7188043 ]\n",
      " [0.15328467 0.6859576  0.24087591 0.94143206]\n",
      " [0.         0.24070212 0.2810219  0.6677094 ]\n",
      " [0.22627737 0.8611401  0.26277372 0.9085853 ]\n",
      " [0.18248175 0.8939868  0.24087591 0.9377824 ]] [[0.38111696 0.32586464 0.5827781  0.45580715]\n",
      " [0.4059221  0.62972414 0.51144725 0.6827323 ]\n",
      " [0.4059221  0.62972414 0.51144725 0.6827323 ]\n",
      " [0.32083884 0.30440962 0.66264164 0.52105147]\n",
      " [0.38111696 0.32586464 0.5827781  0.45580715]\n",
      " [0.39875868 0.4446689  0.5313139  0.5781399 ]\n",
      " [0.38613582 0.5964812  0.53204423 0.6786014 ]\n",
      " [0.39875868 0.44466892 0.5313139  0.5781399 ]\n",
      " [0.38613582 0.5964812  0.53204423 0.6786014 ]\n",
      " [0.39875868 0.4446689  0.5313139  0.5781399 ]\n",
      " [0.38613582 0.5964812  0.53204423 0.6786014 ]\n",
      " [0.39875868 0.44466892 0.5313139  0.5781399 ]\n",
      " [0.38613582 0.5964812  0.53204423 0.6786014 ]\n",
      " [0.36846873 0.30963445 0.6100465  0.4374656 ]\n",
      " [0.4059221  0.62972414 0.51144725 0.6827323 ]\n",
      " [0.4059221  0.62972414 0.51144725 0.6827323 ]]\n",
      "[[0.         0.00835655 0.20612814 0.44011143]\n",
      " [0.3286908  0.90250695 0.4317549  0.97493035]\n",
      " [0.17270195 0.9164345  0.27019498 0.9888579 ]\n",
      " [0.0362117  0.02228412 0.82451254 0.60445684]\n",
      " [0.1615599  0.03064067 0.3119777  0.40947077]\n",
      " [0.29526463 0.5598886  0.40668523 0.7938719 ]\n",
      " [0.31476322 0.7437326  0.41504177 0.97771585]\n",
      " [0.10584958 0.545961   0.24791086 0.80779946]\n",
      " [0.1615599  0.7632312  0.25905293 0.9916434 ]\n",
      " [0.7130919  0.4930362  0.91922003 0.66852367]\n",
      " [0.8551532  0.6406685  1.         0.89693594]\n",
      " [0.55431753 0.5041783  0.6963788  0.75487465]\n",
      " [0.5849582  0.7437326  0.68245125 0.908078  ]\n",
      " [0.70194983 0.5264624  0.83286905 0.88857937]\n",
      " [0.90529245 0.81894153 1.         0.89693594]\n",
      " [0.5961003  0.8440111  0.6908078  0.91922003]] [[0.36454824 0.2804947  0.62379456 0.4385991 ]\n",
      " [0.42640424 0.6625753  0.570179   0.7276939 ]\n",
      " [0.42640424 0.6625753  0.570179   0.7276939 ]\n",
      " [0.27515224 0.25111663 0.7228096  0.53595877]\n",
      " [0.36454824 0.2804947  0.62379456 0.4385991 ]\n",
      " [0.41604477 0.45111167 0.6085205  0.6092221 ]\n",
      " [0.3987143  0.62118447 0.6007514  0.72320384]\n",
      " [0.41604477 0.45111167 0.6085205  0.60922205]\n",
      " [0.39871427 0.62118447 0.6007514  0.72320384]\n",
      " [0.41604477 0.45111167 0.6085205  0.6092221 ]\n",
      " [0.3987143  0.62118447 0.6007514  0.72320384]\n",
      " [0.41604477 0.45111167 0.6085205  0.60922205]\n",
      " [0.39871427 0.62118447 0.6007514  0.72320384]\n",
      " [0.33637154 0.25950232 0.6562777  0.43926966]\n",
      " [0.42640424 0.6625753  0.570179   0.7276939 ]\n",
      " [0.42640424 0.6625753  0.570179   0.7276939 ]]\n",
      "[[0.4848247  0.         0.9797227  1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.02564103 0.1377551  0.76543695 0.8877551 ]\n",
      " [0.4338043  0.1734694  0.7297227  0.8520408 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.43372923 0.38358235 0.5967809  0.6142081 ]\n",
      " [0.4422384  0.3552768  0.5186821  0.35535732]\n",
      " [0.4422384  0.3552768  0.5186821  0.35535732]\n",
      " [0.43372923 0.38358235 0.5967809  0.6142081 ]\n",
      " [0.43372923 0.38358235 0.5967809  0.6142081 ]\n",
      " [0.4422384  0.3552768  0.5186821  0.35535732]\n",
      " [0.4422384  0.3552768  0.5186821  0.35535732]\n",
      " [0.4422384  0.3552768  0.5186821  0.35535732]\n",
      " [0.4422384  0.3552768  0.5186821  0.35535732]\n",
      " [0.4422384  0.3552768  0.5186821  0.35535732]\n",
      " [0.4422384  0.3552768  0.5186821  0.35535732]\n",
      " [0.4422384  0.3552768  0.5186821  0.35535732]\n",
      " [0.4422384  0.3552768  0.5186821  0.35535732]\n",
      " [0.4422384  0.3552768  0.5186821  0.35535732]\n",
      " [0.4422384  0.3552768  0.5186821  0.35535732]\n",
      " [0.4422384  0.3552768  0.5186821  0.35535732]]\n",
      "[[0.46835443 0.013261   1.         0.5322483 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.15250151 0.75949365 0.9752863 ]\n",
      " [0.5443038  0.12718505 0.7468355  0.34237492]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.21518987 0.46895722 0.39240506 0.6968053 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.46666408 0.45767593 0.54433775 0.50941986]\n",
      " [0.47349277 0.43956804 0.5099093  0.43959478]\n",
      " [0.47349277 0.43956804 0.5099093  0.43959478]\n",
      " [0.45225108 0.44611028 0.55219823 0.5074836 ]\n",
      " [0.46666408 0.45767593 0.54433775 0.50941986]\n",
      " [0.47349277 0.43956804 0.5099093  0.43959478]\n",
      " [0.47349277 0.43956804 0.5099093  0.43959478]\n",
      " [0.47349277 0.43956804 0.5099093  0.43959478]\n",
      " [0.47349277 0.43956804 0.5099093  0.43959478]\n",
      " [0.47349277 0.43956804 0.5099093  0.43959478]\n",
      " [0.47349277 0.43956804 0.5099093  0.43959478]\n",
      " [0.47349277 0.43956804 0.5099093  0.43959478]\n",
      " [0.47349277 0.43956804 0.5099093  0.43959478]\n",
      " [0.46452066 0.45883295 0.5431483  0.51591194]\n",
      " [0.47349277 0.43956804 0.5099093  0.43959478]\n",
      " [0.47349277 0.43956804 0.5099093  0.43959478]]\n",
      "[[0.11827957 0.         0.59408605 0.55376345]\n",
      " [0.11827957 0.48924732 0.77150536 1.        ]\n",
      " [0.11827957 0.4247312  0.46505377 0.73655915]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.6935484  0.6989247  0.87903225 1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.37263203 0.34447658 0.57947385 0.60327935]\n",
      " [0.33804232 0.4414178  0.61790144 0.74012554]\n",
      " [0.37263203 0.34447658 0.57947385 0.60327935]\n",
      " [0.42953655 0.2182798  0.55469304 0.21836857]\n",
      " [0.37277314 0.59656286 0.5768287  0.7492215 ]\n",
      " [0.42953655 0.2182798  0.55469304 0.21836857]\n",
      " [0.42953655 0.2182798  0.55469304 0.21836857]\n",
      " [0.42953655 0.2182798  0.55469304 0.21836857]\n",
      " [0.42953655 0.2182798  0.55469304 0.21836857]\n",
      " [0.42953655 0.2182798  0.55469304 0.21836857]\n",
      " [0.42953655 0.2182798  0.55469304 0.21836857]\n",
      " [0.42953655 0.2182798  0.55469304 0.21836857]\n",
      " [0.42953655 0.2182798  0.55469304 0.21836857]\n",
      " [0.42953655 0.2182798  0.55469304 0.21836857]\n",
      " [0.42953655 0.2182798  0.55469304 0.21836857]\n",
      " [0.42953655 0.2182798  0.55469304 0.21836857]]\n",
      "[[0.         0.14418605 0.3627907  0.37209302]\n",
      " [0.13488372 0.3488372  1.         0.855814  ]\n",
      " [0.16744186 0.33488372 0.3534884  0.53023255]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.3573253  0.43819335 0.4924725  0.5283548 ]\n",
      " [0.3573253  0.43819335 0.4924725  0.5283548 ]\n",
      " [0.3573253  0.43819335 0.4924725  0.5283548 ]\n",
      " [0.4321275  0.37833688 0.5232699  0.37838706]\n",
      " [0.4321275  0.37833688 0.5232699  0.37838706]\n",
      " [0.4321275  0.37833688 0.5232699  0.37838706]\n",
      " [0.4321275  0.37833688 0.5232699  0.37838706]\n",
      " [0.4321275  0.37833688 0.5232699  0.37838706]\n",
      " [0.4321275  0.37833688 0.5232699  0.37838706]\n",
      " [0.4321275  0.37833688 0.5232699  0.37838706]\n",
      " [0.4321275  0.37833688 0.5232699  0.37838706]\n",
      " [0.4321275  0.37833688 0.5232699  0.37838706]\n",
      " [0.4321275  0.37833688 0.5232699  0.37838706]\n",
      " [0.4321275  0.37833688 0.5232699  0.37838706]\n",
      " [0.4321275  0.37833688 0.5232699  0.37838706]\n",
      " [0.4321275  0.37833688 0.5232699  0.37838706]]\n",
      "[[0.4516129  0.         0.8566308  0.5125448 ]\n",
      " [0.14336918 0.35842294 0.78494626 1.        ]\n",
      " [0.34767026 0.28315413 0.50179213 0.49462366]\n",
      " [0.6344086  0.9283154  0.78853047 1.        ]\n",
      " [0.64157706 0.5197133  0.8172043  0.9713262 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.46639132 0.41684613 0.57910675 0.6233746 ]\n",
      " [0.45571178 0.36630207 0.59199345 0.6040399 ]\n",
      " [0.46639132 0.41684613 0.57910675 0.6233746 ]\n",
      " [0.47214824 0.5567319  0.56885356 0.6756191 ]\n",
      " [0.45540085 0.3813917  0.58676076 0.6130203 ]\n",
      " [0.45568356 0.29927444 0.5334949  0.29934144]\n",
      " [0.45568356 0.29927444 0.5334949  0.29934144]\n",
      " [0.45568356 0.29927444 0.5334949  0.29934144]\n",
      " [0.45568356 0.29927444 0.5334949  0.29934144]\n",
      " [0.45568356 0.29927444 0.5334949  0.29934144]\n",
      " [0.45568356 0.29927444 0.5334949  0.29934144]\n",
      " [0.45568356 0.29927444 0.5334949  0.29934144]\n",
      " [0.45568356 0.29927444 0.5334949  0.29934144]\n",
      " [0.45568356 0.29927444 0.5334949  0.29934144]\n",
      " [0.45568356 0.29927444 0.5334949  0.29934144]\n",
      " [0.45568356 0.29927444 0.5334949  0.29934144]]\n",
      "[[0.7758621  0.5        0.8965517  0.62068963]\n",
      " [0.44827586 0.43103448 0.7586207  0.6034483 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.3448276  0.         0.79310346 0.5       ]\n",
      " [0.0862069  0.5689655  0.7586207  1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.15517241 0.4827586  0.44827586 0.55172414]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.48861784 0.48854002 0.5161298  0.5144696 ]\n",
      " [0.48012277 0.4769773  0.5231582  0.5198757 ]\n",
      " [0.49442932 0.45827183 0.5143803  0.45827731]\n",
      " [0.49049193 0.49062398 0.51853085 0.5145003 ]\n",
      " [0.49049193 0.49062398 0.51853085 0.5145003 ]\n",
      " [0.49442932 0.45827183 0.5143803  0.45827731]\n",
      " [0.49442932 0.45827183 0.5143803  0.45827731]\n",
      " [0.49442932 0.45827183 0.5143803  0.45827731]\n",
      " [0.49442932 0.45827183 0.5143803  0.45827731]\n",
      " [0.48861784 0.48854002 0.5161298  0.5144696 ]\n",
      " [0.49442932 0.45827183 0.5143803  0.45827731]\n",
      " [0.49442932 0.45827183 0.5143803  0.45827731]\n",
      " [0.49442932 0.45827183 0.5143803  0.45827731]\n",
      " [0.49442932 0.45827183 0.5143803  0.45827731]\n",
      " [0.49442932 0.45827183 0.5143803  0.45827731]\n",
      " [0.49442932 0.45827183 0.5143803  0.45827731]]\n",
      "[[0.37125748 0.15568863 0.502994   0.2754491 ]\n",
      " [0.36526945 0.251497   0.5868263  0.5269461 ]\n",
      " [0.4491018  0.23952095 0.48502994 0.28143713]\n",
      " [0.5988024  0.19161677 0.7185629  0.31137726]\n",
      " [0.5269461  0.22155689 0.65868264 0.3473054 ]\n",
      " [0.6766467  0.15568863 0.7784431  0.22155689]\n",
      " [0.2275449  0.11976048 0.31137726 0.24550898]\n",
      " [0.26946107 0.20958084 0.39520958 0.34131736]\n",
      " [0.22155689 0.         0.3233533  0.13772455]\n",
      " [0.56886226 0.7185629  0.66467065 0.8982036 ]\n",
      " [0.4790419  0.46107784 0.6287425  0.75449103]\n",
      " [0.5988024  0.8742515  0.6826347  0.97604793]\n",
      " [0.4730539  0.7245509  0.5808383  0.9221557 ]\n",
      " [0.38922155 0.46706587 0.55688626 0.76047903]\n",
      " [0.5449102  0.8982036  0.6287425  1.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.48709536 0.411739   0.51329845 0.46768126]\n",
      " [0.48133197 0.3991237  0.5204514  0.47443765]\n",
      " [0.48709536 0.411739   0.51329845 0.46768126]\n",
      " [0.48975125 0.47393358 0.5134288  0.5486025 ]\n",
      " [0.48842883 0.44520706 0.51428837 0.5161413 ]\n",
      " [0.49538028 0.5250954  0.5086719  0.56936204]\n",
      " [0.48975125 0.47393358 0.5134288  0.5486025 ]\n",
      " [0.48842883 0.44520706 0.51428837 0.5161413 ]\n",
      " [0.49538028 0.5250954  0.5086719  0.56936204]\n",
      " [0.48975125 0.47393358 0.5134288  0.5486025 ]\n",
      " [0.48842883 0.44520706 0.51428837 0.5161413 ]\n",
      " [0.49538028 0.5250954  0.5086719  0.56936204]\n",
      " [0.48975125 0.47393358 0.5134288  0.5486025 ]\n",
      " [0.48842883 0.44520706 0.51428837 0.5161413 ]\n",
      " [0.49538028 0.5250954  0.5086719  0.56936204]\n",
      " [0.4781691  0.37985304 0.51797765 0.3798827 ]]\n",
      "[[0.5522388  0.25373134 1.         0.53731346]\n",
      " [0.         0.4402985  0.73880595 0.74626863]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.17910448 0.30597016 0.6492537  0.5671642 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.44935334 0.47665077 0.5570555  0.5173516 ]\n",
      " [0.4321634  0.46422797 0.57067245 0.5165196 ]\n",
      " [0.46471134 0.4525171  0.51838    0.45253465]\n",
      " [0.46471134 0.4525171  0.51838    0.45253465]\n",
      " [0.44935334 0.47665077 0.5570555  0.5173516 ]\n",
      " [0.46471134 0.4525171  0.51838    0.45253465]\n",
      " [0.46471134 0.4525171  0.51838    0.45253465]\n",
      " [0.46471134 0.4525171  0.51838    0.45253465]\n",
      " [0.46471134 0.4525171  0.51838    0.45253465]\n",
      " [0.46471134 0.4525171  0.51838    0.45253465]\n",
      " [0.46471134 0.4525171  0.51838    0.45253465]\n",
      " [0.46471134 0.4525171  0.51838    0.45253465]\n",
      " [0.46471134 0.4525171  0.51838    0.45253465]\n",
      " [0.46471134 0.4525171  0.51838    0.45253465]\n",
      " [0.46471134 0.4525171  0.51838    0.45253465]\n",
      " [0.46471134 0.4525171  0.51838    0.45253465]]\n",
      "[[0.4318095  0.         0.54780954 0.14      ]\n",
      " [0.3558095  0.104      0.6238095  0.484     ]\n",
      " [0.43980953 0.108      0.5078095  0.156     ]\n",
      " [0.5958095  0.316      0.66380954 0.52      ]\n",
      " [0.57980955 0.18       0.6518095  0.348     ]\n",
      " [0.6238095  0.504      0.6758095  0.548     ]\n",
      " [0.33580953 0.344      0.41180953 0.54      ]\n",
      " [0.3278095  0.168      0.39180952 0.38      ]\n",
      " [0.3558095  0.536      0.41980952 0.58      ]\n",
      " [0.54780954 0.7        0.6478095  0.932     ]\n",
      " [0.51580954 0.436      0.6278095  0.744     ]\n",
      " [0.5838095  0.92       0.6598095  0.98      ]\n",
      " [0.44780952 0.712      0.57180953 0.96      ]\n",
      " [0.4158095  0.468      0.54380953 0.74      ]\n",
      " [0.5078095  0.94       0.5878095  1.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.48996848 0.3333937  0.51107454 0.43605885]\n",
      " [0.47988442 0.31866714 0.5232239  0.4631939 ]\n",
      " [0.48996848 0.3333937  0.51107454 0.43605885]\n",
      " [0.49719158 0.47740272 0.5097002  0.629199  ]\n",
      " [0.4956961  0.3767627  0.5093131  0.51484835]\n",
      " [0.5059949  0.57599735 0.5026268  0.6537162 ]\n",
      " [0.49719158 0.47740272 0.5097002  0.629199  ]\n",
      " [0.4956961  0.3767627  0.5093131  0.51484835]\n",
      " [0.5059949  0.57599735 0.5026268  0.6537162 ]\n",
      " [0.49719158 0.47740272 0.5097002  0.629199  ]\n",
      " [0.4956961  0.3767627  0.5093131  0.51484835]\n",
      " [0.5059949  0.57599735 0.5026268  0.6537162 ]\n",
      " [0.49719158 0.47740272 0.5097002  0.629199  ]\n",
      " [0.4956961  0.3767627  0.5093131  0.51484835]\n",
      " [0.5059949  0.57599735 0.5026268  0.6537162 ]\n",
      " [0.47380254 0.30120745 0.51140654 0.3012861 ]]\n",
      "[[0.14691943 0.04028436 0.34360188 0.39810428]\n",
      " [0.30331755 0.5947867  0.3649289  0.6658768 ]\n",
      " [0.         0.63033175 0.07345971 0.6943128 ]\n",
      " [0.27251184 0.02606635 0.92890996 0.6255924 ]\n",
      " [0.25829384 0.01184834 0.5450237  0.3364929 ]\n",
      " [0.35308057 0.4170616  0.507109   0.5663507 ]\n",
      " [0.29620853 0.42890996 0.38151658 0.65876776]\n",
      " [0.1800948  0.44312796 0.32464454 0.53554505]\n",
      " [0.00236967 0.48104265 0.1943128  0.6848341 ]\n",
      " [0.73696685 0.5971564  0.8720379  0.8175355 ]\n",
      " [0.5947867  0.79620856 0.7985782  0.9834123 ]\n",
      " [0.69905216 0.6042654  0.8649289  0.8293839 ]\n",
      " [0.8080569  0.7819905  0.86966825 0.98815167]\n",
      " [0.89810425 0.4170616  1.         0.5947867 ]\n",
      " [0.5900474  0.92890996 0.65876776 0.9834123 ]\n",
      " [0.         0.         0.         0.        ]] [[0.34500977 0.24728179 0.64295214 0.42021284]\n",
      " [0.4139849  0.6227673  0.58330286 0.7232728 ]\n",
      " [0.4139849  0.6227673  0.58330286 0.7232728 ]\n",
      " [0.2494441  0.22003257 0.7490642  0.5357268 ]\n",
      " [0.34500977 0.24728179 0.64295214 0.42021284]\n",
      " [0.40545917 0.38945884 0.6305897  0.5667726 ]\n",
      " [0.3835538  0.5617123  0.6155376  0.7124173 ]\n",
      " [0.40545917 0.3894588  0.6305897  0.5667726 ]\n",
      " [0.3835538  0.5617123  0.61553764 0.7124173 ]\n",
      " [0.4233956  0.4418289  0.63114244 0.6012763 ]\n",
      " [0.3835538  0.5617123  0.6155376  0.7124173 ]\n",
      " [0.34382772 0.28759176 0.66312706 0.5165838 ]\n",
      " [0.39149868 0.5828184  0.6020441  0.7238628 ]\n",
      " [0.31074077 0.23160458 0.68248504 0.4518532 ]\n",
      " [0.4139849  0.6227673  0.58330286 0.7232728 ]\n",
      " [0.3520522  0.18020245 0.5602325  0.18036154]]\n",
      "[[0.7673469  0.07336473 1.         0.24071167]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.24489796 0.17948718 0.8979592  0.61214024]\n",
      " [0.66938776 0.17540555 0.90204084 0.42438513]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.6857143  0.82438517 0.74285716 0.9223443 ]\n",
      " [0.75918365 0.57132393 0.8122449  0.7182627 ]\n",
      " [0.67346936 0.7100994  0.8        0.82438517]\n",
      " [0.34285715 0.6611198  0.4122449  0.7468341 ]\n",
      " [0.37959182 0.7386708  0.53061223 0.8978545 ]\n",
      " [0.24081632 0.5835688  0.46530613 0.7060178 ]\n",
      " [0.24081632 0.681528   0.41632652 0.8203035 ]\n",
      " [0.         0.3549974  0.25306123 0.63663006]\n",
      " [0.48163265 0.8692831  0.5346939  0.8978545 ]\n",
      " [0.33877552 0.7754055  0.41632652 0.8203035 ]] [[0.4302716  0.3825994  0.57406116 0.42616326]\n",
      " [0.42016894 0.3464262  0.53337336 0.34649625]\n",
      " [0.42016894 0.3464262  0.53337336 0.34649625]\n",
      " [0.36927268 0.3650639  0.6305337  0.4469425 ]\n",
      " [0.4302716  0.3825994  0.57406116 0.42616326]\n",
      " [0.42016894 0.3464262  0.53337336 0.34649625]\n",
      " [0.4898842  0.60042363 0.5711947  0.6047944 ]\n",
      " [0.4385991  0.39435053 0.5795214  0.43930018]\n",
      " [0.45477003 0.51817954 0.558579   0.5563873 ]\n",
      " [0.473293   0.4706322  0.58273274 0.50475645]\n",
      " [0.45803216 0.5462741  0.5741589  0.57799745]\n",
      " [0.473293   0.47063217 0.58273274 0.50475645]\n",
      " [0.45803216 0.5462741  0.5741589  0.57799745]\n",
      " [0.40542313 0.3726488  0.5957785  0.44135946]\n",
      " [0.4734955  0.5675405  0.5574194  0.58462954]\n",
      " [0.4734955  0.5675405  0.5574194  0.58462954]]\n",
      "[[0.60583943 0.         0.8321168  0.1970803 ]\n",
      " [0.17518248 0.05109489 0.74452555 0.79562044]\n",
      " [0.47445256 0.09489051 0.7591241  0.30656934]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.5328467  0.5985401  0.71532845 0.7007299 ]\n",
      " [0.54744524 0.6277372  0.7518248  0.69343066]\n",
      " [0.4379562  0.5839416  0.69343066 0.7810219 ]\n",
      " [0.45255473 0.7007299  0.7080292  0.7810219 ]\n",
      " [0.18248175 0.72262776 0.3941606  1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.4929486  0.45682025 0.5299268  0.5310993 ]\n",
      " [0.46860284 0.42120627 0.54601914 0.54373384]\n",
      " [0.4929486  0.45682025 0.5299268  0.5310993 ]\n",
      " [0.4789128  0.39365634 0.51531523 0.39368907]\n",
      " [0.4789128  0.39365634 0.51531523 0.39368907]\n",
      " [0.49534327 0.4651895  0.5335792  0.5419154 ]\n",
      " [0.5057962  0.5303064  0.5257698  0.57105005]\n",
      " [0.49534327 0.4651895  0.5335792  0.5419154 ]\n",
      " [0.5057962  0.5303064  0.5257698  0.57105005]\n",
      " [0.47859782 0.44319928 0.53283453 0.5363921 ]\n",
      " [0.4789128  0.39365634 0.51531523 0.39368907]\n",
      " [0.4789128  0.39365634 0.51531523 0.39368907]\n",
      " [0.4789128  0.39365634 0.51531523 0.39368907]\n",
      " [0.4789128  0.39365634 0.51531523 0.39368907]\n",
      " [0.4789128  0.39365634 0.51531523 0.39368907]\n",
      " [0.4789128  0.39365634 0.51531523 0.39368907]]\n",
      "[[0.2038835  0.         0.79611653 0.8737864 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.21359223 0.80582523 0.3592233  1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.47299635 0.49012274 0.530257   0.5677376 ]\n",
      " [0.48114496 0.42589855 0.5068395  0.42593575]\n",
      " [0.48114496 0.42589855 0.5068395  0.42593575]\n",
      " [0.48114496 0.42589855 0.5068395  0.42593575]\n",
      " [0.48114496 0.42589855 0.5068395  0.42593575]\n",
      " [0.47299635 0.49012274 0.530257   0.5677376 ]\n",
      " [0.48114496 0.42589855 0.5068395  0.42593575]\n",
      " [0.48114496 0.42589855 0.5068395  0.42593575]\n",
      " [0.48114496 0.42589855 0.5068395  0.42593575]\n",
      " [0.48114496 0.42589855 0.5068395  0.42593575]\n",
      " [0.48114496 0.42589855 0.5068395  0.42593575]\n",
      " [0.48114496 0.42589855 0.5068395  0.42593575]\n",
      " [0.48114496 0.42589855 0.5068395  0.42593575]\n",
      " [0.48114496 0.42589855 0.5068395  0.42593575]\n",
      " [0.48114496 0.42589855 0.5068395  0.42593575]\n",
      " [0.48114496 0.42589855 0.5068395  0.42593575]]\n",
      "[[0.10108303 0.         0.6570397  0.42238268]\n",
      " [0.26714802 0.15884477 0.8519856  0.7761733 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.7436823  0.23465703 0.89891696 0.62815887]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.6895307  0.6209386  0.8267148  1.        ]\n",
      " [0.6714801  0.80866426 0.8158845  1.        ]\n",
      " [0.32129964 0.6895307  0.5162455  0.9494585 ]\n",
      " [0.32490975 0.80144405 0.5090253  0.94584835]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.44081372 0.3397715  0.5812803  0.5141357 ]\n",
      " [0.4051639  0.33676928 0.62402475 0.6225542 ]\n",
      " [0.4328587  0.2954755  0.52975976 0.2955717 ]\n",
      " [0.44081372 0.3397715  0.5812803  0.5141357 ]\n",
      " [0.4328587  0.2954755  0.52975976 0.2955717 ]\n",
      " [0.467117   0.4619527  0.58972    0.6484561 ]\n",
      " [0.50654113 0.64512825 0.5679477  0.6893896 ]\n",
      " [0.467117   0.4619527  0.58972    0.6484561 ]\n",
      " [0.50654113 0.64512825 0.5679477  0.6893896 ]\n",
      " [0.4328587  0.2954755  0.52975976 0.2955717 ]\n",
      " [0.4328587  0.2954755  0.52975976 0.2955717 ]\n",
      " [0.4328587  0.2954755  0.52975976 0.2955717 ]\n",
      " [0.4328587  0.2954755  0.52975976 0.2955717 ]\n",
      " [0.4328587  0.2954755  0.52975976 0.2955717 ]\n",
      " [0.4328587  0.2954755  0.52975976 0.2955717 ]\n",
      " [0.4328587  0.2954755  0.52975976 0.2955717 ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ww/3nytfk1917b8nz7dchtckxnm0000gn/T/ipykernel_2905/1910956142.py:86: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  result['obj_class'].replace(class_dict, inplace=True)\n",
      "/var/folders/ww/3nytfk1917b8nz7dchtckxnm0000gn/T/ipykernel_2905/1910956142.py:112: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['head' 'torso' 'lwing' ... 'lwing' 'head' 'torso']' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  result.loc[result['obj_class']==k, ['part_labels']] = result.loc[result['obj_class']==k,['part_labels']].replace(v).copy()\n"
     ]
    }
   ],
   "source": [
    "#testing loop\n",
    "model_path = ('/Users/amrutamuthal/Documents/training_runs/model/'+run_prefix+'/Obj-Box-'\n",
    "                        +str(learning_rate)\n",
    "                        +'-batch-'+str(batch_size)\n",
    "                        +'-h1-'+str(hidden1)\n",
    "                        +'-h2-'+str(hidden2)\n",
    "                        +'-h3-'+str(hidden3)+'-test')\n",
    "summary_path = ('/Users/amrutamuthal/Documents/training_runs/run/'+run_prefix+'/Obj-Box-'\n",
    "                        +str(learning_rate)\n",
    "                        +'-batch-'+str(batch_size)\n",
    "                        +'-h1-'+str(hidden1)\n",
    "                        +'-h2-'+str(hidden2)\n",
    "                        +'-h3-'+str(hidden3)+'-test')\n",
    "\n",
    "\n",
    "write_tensorboard = False\n",
    "if write_tensorboard:\n",
    "    writer = SummaryWriter(summary_path)\n",
    "\n",
    "vae = TwoStageAutoEncoder(latent_dims,\n",
    "                          num_nodes,\n",
    "                          bbx_size,\n",
    "                          num_classes,\n",
    "                          label_shape,\n",
    "                          hidden1,\n",
    "                          hidden2,\n",
    "                          hidden3,\n",
    "                          dense_hidden1,\n",
    "                          dense_hidden2,\n",
    "                          adaptive_margin,\n",
    "                          output_log,\n",
    "                          area_encoding,\n",
    "                          coupling,\n",
    "                          obj_bbx_conditioning,\n",
    "                          use_fft_on_bbx,\n",
    "                        )\n",
    "\n",
    "vae.load_state_dict(torch.load(model_path+ '/model_weights.pth'))\n",
    "\n",
    "# decoder = vae.decoder\n",
    "image_shape = [num_nodes, bbx_size]\n",
    "global_step = 250000\n",
    "class_dict = {0:'cow', 1:'sheep', 2:'bird', 3:'person', 4:'cat', 5:'dog', 6:'horse'}\n",
    "res_dfs = []\n",
    "for i, val_data in enumerate(batch_val_loader, 0):\n",
    "    \n",
    "    node_data_true = val_data.x\n",
    "\n",
    "    label_true = node_data_true[:,:1]\n",
    "    y_true = val_data.y\n",
    "    class_true = y_true[:, :num_classes]\n",
    "    X_obj_true = y_true[:, num_classes:]\n",
    "    X_obj_true_transformed = X_obj_true[:,2:]-X_obj_true[:,:2]\n",
    "    adj_true = val_data.edge_index\n",
    "    class_true  = torch.flatten(class_true)\n",
    "    \n",
    "    output = vae(adj_true, node_data_true, X_obj_true_transformed, label_true , class_true, variational, coupling)\n",
    "    node_data_pred_test = output[0]\n",
    "    X_obj_pred_test = output[1]\n",
    "    \n",
    "#     node_data_pred_test, X_obj_pred_test, label_pred, z_mean, z_logvar, margin = output\n",
    "    X_obj_pred_test = torch.cat(((torch.tensor([1.0])-X_obj_pred_test)*torch.tensor([0.5]),\n",
    "                                (torch.tensor([1.0])+X_obj_pred_test)*torch.tensor([0.5])),\n",
    "                              -1)\n",
    "    res_dfs.append(metrics.get_metrics(node_data_true, X_obj_true, node_data_pred_test,\n",
    "                               X_obj_pred_test,\n",
    "                               label_true,\n",
    "                               class_true,\n",
    "                               num_nodes,\n",
    "                               num_classes))\n",
    "    \n",
    "    if write_tensorboard:\n",
    "        \n",
    "        for j in range(int(len(node_data_true)/num_nodes)):\n",
    "            \n",
    "            image = plot_bbx(node_data_true[j].detach().to(\"cpu\").numpy().astype(float))/255\n",
    "            pred_image = plot_bbx(node_data_pred_test[j].detach().to(\"cpu\").numpy()).astype(float)/255\n",
    "\n",
    "            writer.add_image('test_result/images/'+str(j)+'-input/', image, global_step, dataformats='HWC')  \n",
    "            writer.add_image('test_result/images/'+str(j)+'-generated/', pred_image, global_step, dataformats='HWC')\n",
    "            \n",
    "\n",
    "result = pd.concat(res_dfs)\n",
    "result['obj_class'] = np.where(result['obj_class'].isna(), 0, result['obj_class'])\n",
    "result['obj_class'] = result['obj_class'].astype('int')\n",
    "result['obj_class'].replace(class_dict, inplace=True)\n",
    "result.where(result['part_labels']!=0, np.NaN, inplace=True)\n",
    "result['part_labels'] = np.where(result['part_labels'].isna(), 0, result['part_labels'])\n",
    "result['part_labels'] = result['part_labels'].astype('int')\n",
    "result['id'] = np.repeat(np.array(list(range(int(len(result)/num_nodes)))), 16)\n",
    "\n",
    "if write_tensorboard:\n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "\n",
    "bird_labels = {'head':1 , 'torso':2, 'neck':3, 'lwing':4, 'rwing':5, 'lleg':6, 'lfoot':7, 'rleg':8, 'rfoot':9, 'tail':10}\n",
    "cat_labels = {'head':1, 'torso':2, 'neck':3, 'lfleg':4, 'lfpa':5, 'rfleg':6, 'rfpa':7, 'lbleg':8, 'lbpa':9, 'rbleg':10, 'rbpa':11, 'tail':12}\n",
    "cow_labels = {'head':1,'lhorn':2, 'rhorn':3, 'torso':4, 'neck':5, 'lfuleg':6, 'lflleg':7, 'rfuleg':8, 'rflleg':9, 'lbuleg':10, 'lblleg':11, 'rbuleg':12, 'rblleg':13, 'tail':14}\n",
    "dog_labels = {'head':1,'torso':2, 'neck':3, 'lfleg':4, 'lfpa':5, 'rfleg':6, 'rfpa':7, 'lbleg':8, 'lbpa':9, 'rbleg':10, 'rbpa':11, 'tail':12, 'muzzle':13}\n",
    "horse_labels = {'head':1,'lfho':2, 'rfho':3, 'torso':4, 'neck':5, 'lfuleg':6, 'lflleg':7, 'rfuleg':8, 'rflleg':9, 'lbuleg':10, 'lblleg':11, 'rbuleg':12, 'rblleg':13, 'tail':14, 'lbho':15, 'rbho':16}\n",
    "person_labels = {'head':1, 'torso':2, 'neck': 3, 'llarm': 4, 'luarm': 5, 'lhand': 6, 'rlarm':7, 'ruarm':8, 'rhand': 9, 'llleg': 10, 'luleg':11, 'lfoot':12, 'rlleg':13, 'ruleg':14, 'rfoot':15}\n",
    "sheep_labels = cow_labels\n",
    "part_labels_combined_parts = {'bird': bird_labels, 'cat': cat_labels, 'cow': cow_labels, 'dog': dog_labels, 'sheep': sheep_labels, 'horse':horse_labels,'person':person_labels}\n",
    "\n",
    "for k, v in part_labels_combined_parts.items():\n",
    "    new_map = {}\n",
    "    for pk, pv in v.items():\n",
    "        new_map[pv]=pk\n",
    "    part_labels_combined_parts[k] = new_map\n",
    "    \n",
    "for k, v in part_labels_combined_parts.items():\n",
    "    result.loc[result['obj_class']==k, ['part_labels']] = result.loc[result['obj_class']==k,['part_labels']].replace(v).copy()\n",
    "\n",
    "# result.to_csv(model_path+'/raw_metrics.csv')\n",
    "# res_obj_level = result.groupby(['obj_class', 'id']).mean().reset_index()\n",
    "# res_obj_level.groupby(['obj_class']).mean().reset_index()[['obj_class', 'IOU', 'MSE']].to_csv(model_path+'/obj_level_metrics.csv')\n",
    "# result.groupby(['obj_class', 'part_labels']).mean().reset_index()[['obj_class', 'part_labels',  'IOU', 'MSE']].to_csv(model_path+'/part_level_metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4fc5e99b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0057, 0.2411, 0.2529, 0.4940],\n",
       "        [1.0000, 0.0920, 0.2641, 0.7586, 0.5917],\n",
       "        [1.0000, 0.1034, 0.2641, 0.2586, 0.4997],\n",
       "        [1.0000, 0.2816, 0.4595, 0.4598, 0.7239],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [1.0000, 0.0345, 0.4997, 0.3448, 0.6606],\n",
       "        [1.0000, 0.0230, 0.5917, 0.1609, 0.6606],\n",
       "        [1.0000, 0.4540, 0.3963, 0.6667, 0.7296],\n",
       "        [1.0000, 0.4425, 0.6779, 0.5345, 0.7239],\n",
       "        [1.0000, 0.7011, 0.4825, 0.9253, 0.7641],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [1.0000, 0.6954, 0.3331, 1.0000, 0.6836],\n",
       "        [1.0000, 0.0000, 0.3503, 0.0977, 0.5055],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.x[:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6821159c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000,  0.3624,  0.4625,  0.4308,  0.4991],\n",
       "        [ 2.0000,  0.3863,  0.4658,  0.5708,  0.5132],\n",
       "        [ 3.0000,  0.3895,  0.4658,  0.4324,  0.4999],\n",
       "        [ 4.0000,  0.4388,  0.4941,  0.4881,  0.5324],\n",
       "        [ 0.0000,  0.3608,  0.4276,  0.3608,  0.4276],\n",
       "        [ 6.0000,  0.3704,  0.4999,  0.4563,  0.5232],\n",
       "        [ 7.0000,  0.3672,  0.5132,  0.4054,  0.5232],\n",
       "        [ 8.0000,  0.4865,  0.4849,  0.5454,  0.5332],\n",
       "        [ 9.0000,  0.4833,  0.5257,  0.5088,  0.5324],\n",
       "        [10.0000,  0.5549,  0.4974,  0.6170,  0.5382],\n",
       "        [ 0.0000,  0.3608,  0.4276,  0.3608,  0.4276],\n",
       "        [12.0000,  0.5533,  0.4758,  0.6377,  0.5265],\n",
       "        [13.0000,  0.3608,  0.4783,  0.3879,  0.5008],\n",
       "        [ 0.0000,  0.3608,  0.4276,  0.3608,  0.4276],\n",
       "        [ 0.0000,  0.3608,  0.4276,  0.3608,  0.4276],\n",
       "        [ 0.0000,  0.3608,  0.4276,  0.3608,  0.4276]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_data_true[:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ad14a28f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3877, 0.4610, 0.5213, 0.5035],\n",
       "        [0.3765, 0.4433, 0.5899, 0.5129],\n",
       "        [0.3986, 0.4754, 0.5038, 0.5103],\n",
       "        [0.4023, 0.4803, 0.5337, 0.5286],\n",
       "        [0.4412, 0.4276, 0.5242, 0.4277],\n",
       "        [0.3915, 0.4872, 0.4811, 0.5143],\n",
       "        [0.3928, 0.5083, 0.4519, 0.5178],\n",
       "        [0.3915, 0.4872, 0.4811, 0.5143],\n",
       "        [0.3928, 0.5083, 0.4519, 0.5178],\n",
       "        [0.4023, 0.4803, 0.5337, 0.5286],\n",
       "        [0.4412, 0.4276, 0.5242, 0.4277],\n",
       "        [0.4023, 0.4803, 0.5337, 0.5286],\n",
       "        [0.3963, 0.4860, 0.4770, 0.5073],\n",
       "        [0.4412, 0.4276, 0.5242, 0.4277],\n",
       "        [0.4412, 0.4276, 0.5242, 0.4277],\n",
       "        [0.4412, 0.4276, 0.5242, 0.4277]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_data_pred_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "78220c71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>obj_class</th>\n",
       "      <th>part_labels</th>\n",
       "      <th>IOU</th>\n",
       "      <th>MSE</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bird</td>\n",
       "      <td>head</td>\n",
       "      <td>0.064389</td>\n",
       "      <td>0.009350</td>\n",
       "      <td>25502.542267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bird</td>\n",
       "      <td>lfoot</td>\n",
       "      <td>0.110243</td>\n",
       "      <td>0.001683</td>\n",
       "      <td>25680.663877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bird</td>\n",
       "      <td>lleg</td>\n",
       "      <td>0.107820</td>\n",
       "      <td>0.003614</td>\n",
       "      <td>25468.209844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bird</td>\n",
       "      <td>lwing</td>\n",
       "      <td>0.171063</td>\n",
       "      <td>0.005438</td>\n",
       "      <td>25187.588382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bird</td>\n",
       "      <td>neck</td>\n",
       "      <td>0.159798</td>\n",
       "      <td>0.003774</td>\n",
       "      <td>25436.863582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>sheep</td>\n",
       "      <td>rflleg</td>\n",
       "      <td>0.055902</td>\n",
       "      <td>0.004451</td>\n",
       "      <td>25265.913462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>sheep</td>\n",
       "      <td>rfuleg</td>\n",
       "      <td>0.047802</td>\n",
       "      <td>0.005275</td>\n",
       "      <td>25516.823614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>sheep</td>\n",
       "      <td>rhorn</td>\n",
       "      <td>0.086659</td>\n",
       "      <td>0.009735</td>\n",
       "      <td>25164.576628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>sheep</td>\n",
       "      <td>tail</td>\n",
       "      <td>0.014137</td>\n",
       "      <td>0.016326</td>\n",
       "      <td>25610.042060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>sheep</td>\n",
       "      <td>torso</td>\n",
       "      <td>0.461996</td>\n",
       "      <td>0.003866</td>\n",
       "      <td>25399.519360</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>94 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   obj_class part_labels       IOU       MSE            id\n",
       "0       bird        head  0.064389  0.009350  25502.542267\n",
       "1       bird       lfoot  0.110243  0.001683  25680.663877\n",
       "2       bird        lleg  0.107820  0.003614  25468.209844\n",
       "3       bird       lwing  0.171063  0.005438  25187.588382\n",
       "4       bird        neck  0.159798  0.003774  25436.863582\n",
       "..       ...         ...       ...       ...           ...\n",
       "89     sheep      rflleg  0.055902  0.004451  25265.913462\n",
       "90     sheep      rfuleg  0.047802  0.005275  25516.823614\n",
       "91     sheep       rhorn  0.086659  0.009735  25164.576628\n",
       "92     sheep        tail  0.014137  0.016326  25610.042060\n",
       "93     sheep       torso  0.461996  0.003866  25399.519360\n",
       "\n",
       "[94 rows x 5 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[['obj_class', 'part_labels', 'IOU', 'MSE', 'id']].groupby(['obj_class', 'part_labels']).mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "02e6f6eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>obj_class</th>\n",
       "      <th>IOU</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bird</td>\n",
       "      <td>0.193992</td>\n",
       "      <td>0.005513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cat</td>\n",
       "      <td>0.255797</td>\n",
       "      <td>0.009347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cow</td>\n",
       "      <td>0.182808</td>\n",
       "      <td>0.007806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dog</td>\n",
       "      <td>0.204083</td>\n",
       "      <td>0.007434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>horse</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.007352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>person</td>\n",
       "      <td>0.165491</td>\n",
       "      <td>0.006016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sheep</td>\n",
       "      <td>0.260519</td>\n",
       "      <td>0.005719</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  obj_class       IOU       MSE\n",
       "0      bird  0.193992  0.005513\n",
       "1       cat  0.255797  0.009347\n",
       "2       cow  0.182808  0.007806\n",
       "3       dog  0.204083  0.007434\n",
       "4     horse  0.170000  0.007352\n",
       "5    person  0.165491  0.006016\n",
       "6     sheep  0.260519  0.005719"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_obj_level = result[['obj_class', 'IOU', 'MSE', 'id']].groupby(['obj_class', 'id']).mean().reset_index()\n",
    "res_obj_level.groupby(['obj_class']).mean().reset_index()[['obj_class', 'IOU', 'MSE']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d9367e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(X_obj, part_decoder, obj_class, nodes, batch_size, latent_dims):\n",
    "    \n",
    "    nodes = torch.reshape(nodes,(batch_size,part_decoder.num_nodes))\n",
    "    obj_class = torch.reshape(obj_class, \n",
    "                              (batch_size, num_classes))\n",
    "    \n",
    "    # obj sampling\n",
    "    z_latent_obj = X_obj\n",
    "    \n",
    "    print(z_latent_obj.shape, obj_class.shape)\n",
    "    conditioned_obj_latent = torch.cat([obj_class, z_latent_obj],dim=-1)\n",
    "    conditioned_obj_latent = torch.cat([nodes, conditioned_obj_latent],dim=-1)\n",
    "\n",
    "    # part sampling\n",
    "    z_latent_part = torch.normal(torch.zeros([batch_size,latent_dims]))\n",
    "    conditioned_part_latent = torch.cat([conditioned_obj_latent, z_latent_part],dim=-1)\n",
    "    \n",
    "    x_bbx, _, _, _ = part_decoder(conditioned_part_latent)\n",
    "    \n",
    "    return x_bbx, X_obj\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c39d671",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#testing loop\n",
    "model_path = ('D:/meronym_data/model/'+run_prefix+'/Obj-Box-'\n",
    "                        +str(learning_rate)\n",
    "                        +'-batch-'+str(batch_size)\n",
    "                        +'-h1-'+str(hidden1)\n",
    "                        +'-h2-'+str(hidden2)\n",
    "                        +'-h3-'+str(hidden3)+'-test')\n",
    "summary_path = ('D:/meronym_data/runs/'+run_prefix+'/Obj-Box-'\n",
    "                        +str(learning_rate)\n",
    "                        +'-batch-'+str(batch_size)\n",
    "                        +'-h1-'+str(hidden1)\n",
    "                        +'-h2-'+str(hidden2)\n",
    "                        +'-h3-'+str(hidden3)+'-test')\n",
    "\n",
    "\n",
    "write_tensorboard = True\n",
    "if write_tensorboard:\n",
    "    writer = SummaryWriter(summary_path)\n",
    "\n",
    "vae = TwoStageAutoEncoder(latent_dims,\n",
    "                          num_nodes,\n",
    "                          bbx_size,\n",
    "                          num_classes,\n",
    "                          label_shape,\n",
    "                          hidden1,\n",
    "                          hidden2,\n",
    "                          hidden3,\n",
    "                          dense_hidden1,\n",
    "                          dense_hidden2,\n",
    "                          adaptive_margin,\n",
    "                          output_log,\n",
    "                          area_encoding,\n",
    "                          coupling,\n",
    "                          obj_bbx_conditioning,\n",
    "                        )\n",
    "\n",
    "\n",
    "vae.load_state_dict(torch.load(model_path+ '/model_weights.pth'))\n",
    "\n",
    "part_decoder = vae.gcn_decoder\n",
    "image_shape = [num_nodes, bbx_size]\n",
    "global_step = 250000\n",
    "class_dict = {0:'cow', 1:'sheep', 2:'bird', 3:'person', 4:'cat', 5:'dog', 6:'horse'}\n",
    "res_dfs = []\n",
    "for i, val_data in enumerate(batch_val_loader, 0):\n",
    "    \n",
    "    val_data\n",
    "    node_data_true = val_data.x\n",
    "    label_true = node_data_true[:,:1]\n",
    "    y_true = val_data.y\n",
    "    class_true = y_true[:, :num_classes]\n",
    "    X_obj_true = y_true[:, num_classes:]\n",
    "    adj_true = val_data.edge_index\n",
    "    class_true  = torch.flatten(class_true)\n",
    "    \n",
    "    output = inference(X_obj_true, part_decoder, class_true, label_true, batch_size, latent_dims)\n",
    "    node_data_pred_test = output[0]\n",
    "    X_obj_pred_test = output[1]\n",
    "#     node_data_pred_test, X_obj_pred_test, label_pred, z_mean, z_logvar, margin = output\n",
    "    \n",
    "#     res_dfs.append(metrics.get_metrics(node_data_true, X_obj_true, node_data_pred_test,\n",
    "#                                X_obj_pred_test,\n",
    "#                                label_true,\n",
    "#                                class_true,\n",
    "#                                num_nodes,\n",
    "#                                num_classes))\n",
    "    \n",
    "    if write_tensorboard:\n",
    "        \n",
    "        for j in range(int(len(node_data_true)/num_nodes)):\n",
    "            image = plot_utils.plot_bbx((node_data_true[num_nodes*j:num_nodes*(j+1)\n",
    "                                                       ,1:5]*label_true[num_nodes*j:num_nodes*(j+1)]).detach().to(\"cpu\").numpy().astype(float))/255\n",
    "            pred_image = plot_utils.plot_bbx((node_data_pred_test[j]*label_true[num_nodes*j:num_nodes*(j+1)]).detach().to(\"cpu\").numpy()).astype(float)/255\n",
    "\n",
    "            writer.add_image('test_result/images/'+str(j)+'-input/', image, global_step, dataformats='HWC')  \n",
    "            writer.add_image('test_result/images/'+str(j)+'-generated/', pred_image, global_step, dataformats='HWC')\n",
    "            \n",
    "\n",
    "result = pd.concat(res_dfs)\n",
    "result['obj_class'] = np.where(result['obj_class'].isna(), 0, result['obj_class'])\n",
    "result['obj_class'] = result['obj_class'].astype('int')\n",
    "result['obj_class'].replace(class_dict, inplace=True)\n",
    "result.where(result['part_labels']!=0, np.NaN, inplace=True)\n",
    "result['part_labels'] = np.where(result['part_labels'].isna(), 0, result['part_labels'])\n",
    "result['part_labels'] = result['part_labels'].astype('int')\n",
    "result['id'] = np.repeat(np.array(list(range(int(len(result)/num_nodes)))), 16)\n",
    "\n",
    "if write_tensorboard:\n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "\n",
    "bird_labels = {'head':1 , 'torso':2, 'neck':3, 'lwing':4, 'rwing':5, 'lleg':6, 'lfoot':7, 'rleg':8, 'rfoot':9, 'tail':10}\n",
    "cat_labels = {'head':1, 'torso':2, 'neck':3, 'lfleg':4, 'lfpa':5, 'rfleg':6, 'rfpa':7, 'lbleg':8, 'lbpa':9, 'rbleg':10, 'rbpa':11, 'tail':12}\n",
    "cow_labels = {'head':1,'lhorn':2, 'rhorn':3, 'torso':4, 'neck':5, 'lfuleg':6, 'lflleg':7, 'rfuleg':8, 'rflleg':9, 'lbuleg':10, 'lblleg':11, 'rbuleg':12, 'rblleg':13, 'tail':14}\n",
    "dog_labels = {'head':1,'torso':2, 'neck':3, 'lfleg':4, 'lfpa':5, 'rfleg':6, 'rfpa':7, 'lbleg':8, 'lbpa':9, 'rbleg':10, 'rbpa':11, 'tail':12, 'muzzle':13}\n",
    "horse_labels = {'head':1,'lfho':2, 'rfho':3, 'torso':4, 'neck':5, 'lfuleg':6, 'lflleg':7, 'rfuleg':8, 'rflleg':9, 'lbuleg':10, 'lblleg':11, 'rbuleg':12, 'rblleg':13, 'tail':14, 'lbho':15, 'rbho':16}\n",
    "person_labels = {'head':1, 'torso':2, 'neck': 3, 'llarm': 4, 'luarm': 5, 'lhand': 6, 'rlarm':7, 'ruarm':8, 'rhand': 9, 'llleg': 10, 'luleg':11, 'lfoot':12, 'rlleg':13, 'ruleg':14, 'rfoot':15}\n",
    "sheep_labels = cow_labels\n",
    "part_labels_combined_parts = {'bird': bird_labels, 'cat': cat_labels, 'cow': cow_labels, 'dog': dog_labels, 'sheep': sheep_labels, 'horse':horse_labels,'person':person_labels}\n",
    "\n",
    "for k, v in part_labels_combined_parts.items():\n",
    "    new_map = {}\n",
    "    for pk, pv in v.items():\n",
    "        new_map[pv]=pk\n",
    "    part_labels_combined_parts[k] = new_map\n",
    "    \n",
    "for k, v in part_labels_combined_parts.items():\n",
    "    result.loc[result['obj_class']==k, ['part_labels']] = result.loc[result['obj_class']==k,['part_labels']].replace(v).copy()\n",
    "\n",
    "result.to_csv(model_path+'/raw_metrics.csv')\n",
    "res_obj_level = result.groupby(['obj_class', 'id']).mean().reset_index()\n",
    "res_obj_level.groupby(['obj_class']).mean().reset_index()[['obj_class', 'IOU', 'MSE']].to_csv(model_path+'/obj_level_metrics.csv')\n",
    "result.groupby(['obj_class', 'part_labels']).mean().reset_index()[['obj_class', 'part_labels',  'IOU', 'MSE']].to_csv(model_path+'/part_level_metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a379c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = ('D:/meronym_data/model/'+run_prefix+'/Obj-Box-'\n",
    "                        +str(learning_rate)\n",
    "                        +'-batch-'+str(batch_size)\n",
    "                        +'-h1-'+str(hidden1)\n",
    "                        +'-h2-'+str(hidden2)\n",
    "                        +'-h3-'+str(hidden3)+'-test')\n",
    "summary_path = ('D:/meronym_data/runs/'+run_prefix+'/Obj-Box-'\n",
    "                        +str(learning_rate)\n",
    "                        +'-batch-'+str(batch_size)\n",
    "                        +'-h1-'+str(hidden1)\n",
    "                        +'-h2-'+str(hidden2)\n",
    "                        +'-h3-'+str(hidden3)+'-test')\n",
    "\n",
    "result = pd.read_csv(model_path+'/raw_metrics.csv')\n",
    "result.groupby(['obj_class', 'part_labels']).mean().reset_index()[['obj_class', 'part_labels',\n",
    "                                                                   'IOU', 'MSE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2d2cbb0",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "agg function failed [how->mean,dtype->object]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/groupby/groupby.py:1942\u001b[0m, in \u001b[0;36mGroupBy._agg_py_fallback\u001b[0;34m(self, how, values, ndim, alt)\u001b[0m\n\u001b[1;32m   1941\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1942\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_grouper\u001b[38;5;241m.\u001b[39magg_series(ser, alt, preserve_dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1943\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/groupby/ops.py:864\u001b[0m, in \u001b[0;36mBaseGrouper.agg_series\u001b[0;34m(self, obj, func, preserve_dtype)\u001b[0m\n\u001b[1;32m    862\u001b[0m     preserve_dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 864\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aggregate_series_pure_python(obj, func)\n\u001b[1;32m    866\u001b[0m npvalues \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mmaybe_convert_objects(result, try_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/groupby/ops.py:885\u001b[0m, in \u001b[0;36mBaseGrouper._aggregate_series_pure_python\u001b[0;34m(self, obj, func)\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(splitter):\n\u001b[0;32m--> 885\u001b[0m     res \u001b[38;5;241m=\u001b[39m func(group)\n\u001b[1;32m    886\u001b[0m     res \u001b[38;5;241m=\u001b[39m extract_result(res)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/groupby/groupby.py:2454\u001b[0m, in \u001b[0;36mGroupBy.mean.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   2451\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2452\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cython_agg_general(\n\u001b[1;32m   2453\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m-> 2454\u001b[0m         alt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: Series(x, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mmean(numeric_only\u001b[38;5;241m=\u001b[39mnumeric_only),\n\u001b[1;32m   2455\u001b[0m         numeric_only\u001b[38;5;241m=\u001b[39mnumeric_only,\n\u001b[1;32m   2456\u001b[0m     )\n\u001b[1;32m   2457\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroupby\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/series.py:6549\u001b[0m, in \u001b[0;36mSeries.mean\u001b[0;34m(self, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m   6541\u001b[0m \u001b[38;5;129m@doc\u001b[39m(make_doc(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m, ndim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m   6542\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmean\u001b[39m(\n\u001b[1;32m   6543\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   6547\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   6548\u001b[0m ):\n\u001b[0;32m-> 6549\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m NDFrame\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;28mself\u001b[39m, axis, skipna, numeric_only, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/generic.py:12420\u001b[0m, in \u001b[0;36mNDFrame.mean\u001b[0;34m(self, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m  12413\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmean\u001b[39m(\n\u001b[1;32m  12414\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m  12415\u001b[0m     axis: Axis \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  12418\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m  12419\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[0;32m> 12420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stat_function(\n\u001b[1;32m  12421\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m, nanops\u001b[38;5;241m.\u001b[39mnanmean, axis, skipna, numeric_only, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m  12422\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/generic.py:12377\u001b[0m, in \u001b[0;36mNDFrame._stat_function\u001b[0;34m(self, name, func, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m  12375\u001b[0m validate_bool_kwarg(skipna, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskipna\u001b[39m\u001b[38;5;124m\"\u001b[39m, none_allowed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m> 12377\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reduce(\n\u001b[1;32m  12378\u001b[0m     func, name\u001b[38;5;241m=\u001b[39mname, axis\u001b[38;5;241m=\u001b[39maxis, skipna\u001b[38;5;241m=\u001b[39mskipna, numeric_only\u001b[38;5;241m=\u001b[39mnumeric_only\n\u001b[1;32m  12379\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/series.py:6457\u001b[0m, in \u001b[0;36mSeries._reduce\u001b[0;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[0m\n\u001b[1;32m   6453\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   6454\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeries.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not allow \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwd_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnumeric_only\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   6455\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith non-numeric dtypes.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   6456\u001b[0m     )\n\u001b[0;32m-> 6457\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op(delegate, skipna\u001b[38;5;241m=\u001b[39mskipna, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/nanops.py:147\u001b[0m, in \u001b[0;36mbottleneck_switch.__call__.<locals>.f\u001b[0;34m(values, axis, skipna, **kwds)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 147\u001b[0m     result \u001b[38;5;241m=\u001b[39m alt(values, axis\u001b[38;5;241m=\u001b[39maxis, skipna\u001b[38;5;241m=\u001b[39mskipna, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/nanops.py:404\u001b[0m, in \u001b[0;36m_datetimelike_compat.<locals>.new_func\u001b[0;34m(values, axis, skipna, mask, **kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m     mask \u001b[38;5;241m=\u001b[39m isna(values)\n\u001b[0;32m--> 404\u001b[0m result \u001b[38;5;241m=\u001b[39m func(values, axis\u001b[38;5;241m=\u001b[39maxis, skipna\u001b[38;5;241m=\u001b[39mskipna, mask\u001b[38;5;241m=\u001b[39mmask, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m datetimelike:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/nanops.py:720\u001b[0m, in \u001b[0;36mnanmean\u001b[0;34m(values, axis, skipna, mask)\u001b[0m\n\u001b[1;32m    719\u001b[0m the_sum \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39msum(axis, dtype\u001b[38;5;241m=\u001b[39mdtype_sum)\n\u001b[0;32m--> 720\u001b[0m the_sum \u001b[38;5;241m=\u001b[39m _ensure_numeric(the_sum)\n\u001b[1;32m    722\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(the_sum, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/nanops.py:1701\u001b[0m, in \u001b[0;36m_ensure_numeric\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1699\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   1700\u001b[0m     \u001b[38;5;66;03m# GH#44008, GH#36703 avoid casting e.g. strings to numeric\u001b[39;00m\n\u001b[0;32m-> 1701\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not convert string \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to numeric\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1702\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: Could not convert string 'headtorsorlegrfoottail' to numeric",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m res_obj_level \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mgroupby([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobj_class\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[1;32m      2\u001b[0m res_obj_level\u001b[38;5;241m.\u001b[39mgroupby([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobj_class\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mreset_index()[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobj_class\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIOU\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMSE\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/groupby/groupby.py:2452\u001b[0m, in \u001b[0;36mGroupBy.mean\u001b[0;34m(self, numeric_only, engine, engine_kwargs)\u001b[0m\n\u001b[1;32m   2445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_numba_agg_general(\n\u001b[1;32m   2446\u001b[0m         grouped_mean,\n\u001b[1;32m   2447\u001b[0m         executor\u001b[38;5;241m.\u001b[39mfloat_dtype_mapping,\n\u001b[1;32m   2448\u001b[0m         engine_kwargs,\n\u001b[1;32m   2449\u001b[0m         min_periods\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   2450\u001b[0m     )\n\u001b[1;32m   2451\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2452\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cython_agg_general(\n\u001b[1;32m   2453\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2454\u001b[0m         alt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: Series(x, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mmean(numeric_only\u001b[38;5;241m=\u001b[39mnumeric_only),\n\u001b[1;32m   2455\u001b[0m         numeric_only\u001b[38;5;241m=\u001b[39mnumeric_only,\n\u001b[1;32m   2456\u001b[0m     )\n\u001b[1;32m   2457\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroupby\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/groupby/groupby.py:1998\u001b[0m, in \u001b[0;36mGroupBy._cython_agg_general\u001b[0;34m(self, how, alt, numeric_only, min_count, **kwargs)\u001b[0m\n\u001b[1;32m   1995\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agg_py_fallback(how, values, ndim\u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39mndim, alt\u001b[38;5;241m=\u001b[39malt)\n\u001b[1;32m   1996\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m-> 1998\u001b[0m new_mgr \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mgrouped_reduce(array_func)\n\u001b[1;32m   1999\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_agged_manager(new_mgr)\n\u001b[1;32m   2000\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m how \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midxmin\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midxmax\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/internals/managers.py:1469\u001b[0m, in \u001b[0;36mBlockManager.grouped_reduce\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m   1465\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m blk\u001b[38;5;241m.\u001b[39mis_object:\n\u001b[1;32m   1466\u001b[0m     \u001b[38;5;66;03m# split on object-dtype blocks bc some columns may raise\u001b[39;00m\n\u001b[1;32m   1467\u001b[0m     \u001b[38;5;66;03m#  while others do not.\u001b[39;00m\n\u001b[1;32m   1468\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sb \u001b[38;5;129;01min\u001b[39;00m blk\u001b[38;5;241m.\u001b[39m_split():\n\u001b[0;32m-> 1469\u001b[0m         applied \u001b[38;5;241m=\u001b[39m sb\u001b[38;5;241m.\u001b[39mapply(func)\n\u001b[1;32m   1470\u001b[0m         result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[1;32m   1471\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/internals/blocks.py:393\u001b[0m, in \u001b[0;36mBlock.apply\u001b[0;34m(self, func, **kwargs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Block]:\n\u001b[1;32m    389\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;124;03m    apply the function to my values; return a block if we are not\u001b[39;00m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;124;03m    one\u001b[39;00m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 393\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    395\u001b[0m     result \u001b[38;5;241m=\u001b[39m maybe_coerce_values(result)\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split_op_result(result)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/groupby/groupby.py:1995\u001b[0m, in \u001b[0;36mGroupBy._cython_agg_general.<locals>.array_func\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m   1992\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m   1994\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m alt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1995\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agg_py_fallback(how, values, ndim\u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39mndim, alt\u001b[38;5;241m=\u001b[39malt)\n\u001b[1;32m   1996\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/groupby/groupby.py:1946\u001b[0m, in \u001b[0;36mGroupBy._agg_py_fallback\u001b[0;34m(self, how, values, ndim, alt)\u001b[0m\n\u001b[1;32m   1944\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magg function failed [how->\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhow\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,dtype->\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mser\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1945\u001b[0m     \u001b[38;5;66;03m# preserve the kind of exception that raised\u001b[39;00m\n\u001b[0;32m-> 1946\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(err)(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   1948\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ser\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[1;32m   1949\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m res_values\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: agg function failed [how->mean,dtype->object]"
     ]
    }
   ],
   "source": [
    "res_obj_level = result.groupby(['obj_class', 'id']).mean().reset_index()\n",
    "res_obj_level.groupby(['obj_class']).mean().reset_index()[['obj_class', 'IOU', 'MSE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0a03e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_obj_level[['IOU', 'MSE']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e231a317",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_obj_pred[:10,2:]-X_obj_pred[:10,:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90832eb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_obj_true[:10, 2:]-X_obj_true[:10, :2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b789243a",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_obj_level.to_csv(model_path+'/obj_metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838534e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7e5b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = 'D:/meronym_data/generate_boxes.npy'\n",
    "with open(outfile, 'wb') as pickle_file:\n",
    "    pred_boxes = np.concatenate(pred_boxes)\n",
    "    pickle.dump(pred_boxes, pickle_file)\n",
    "outfile = 'D:/meronym_data/generate_boxesobj_class.npy'\n",
    "with open(outfile, 'wb') as pickle_file:\n",
    "    pickle.dump(classes,pickle_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
