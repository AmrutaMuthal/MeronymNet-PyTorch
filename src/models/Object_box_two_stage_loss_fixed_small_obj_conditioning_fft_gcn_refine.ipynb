{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cfd1439",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5671166e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_LAUNCH_BLOCKING=1\n"
     ]
    }
   ],
   "source": [
    "%set_env CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd5cb46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21434d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import collections\n",
    "import cv2\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e88c7fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "from torch_geometric.data import DataLoader\n",
    "import torch.utils.data as data_utils\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e46b5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from losses import BoxVAE_losses as loss\n",
    "from evaluation import metrics\n",
    "from utils import plot_utils\n",
    "from utils import data_utils as data_loading\n",
    "from components.DenseAutoencoder import DenseAutoencoder\n",
    "from components.DenseAutoencoder import Decoder\n",
    "from components.TwoStageAutoEncoder import TwoStageAutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d19349cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b59f5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mask_generation import masked_sketch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9cd8d4a6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "latent_dims = 64\n",
    "num_nodes = 16\n",
    "bbx_size = 4\n",
    "num_classes = 7\n",
    "label_shape = 1\n",
    "nb_epochs = 1000\n",
    "klw = loss.frange_cycle_linear(nb_epochs)\n",
    "learning_rate = 0.0001\n",
    "hidden1 = 32\n",
    "hidden2 = 16\n",
    "hidden3 = 128\n",
    "dense_hidden1=8\n",
    "dense_hidden2=4\n",
    "adaptive_margin = True\n",
    "fine_tune_box = False\n",
    "output_log = False\n",
    "area_encoding = False\n",
    "run_prefix = \"two_stage_small_obj_conditioning_fft_sqr_gcn_reccurent_refine_coarse_bbx_loss\"\n",
    "variational=False\n",
    "coupling = True\n",
    "obj_bbx_conditioning = True\n",
    "use_fft_on_bbx = True\n",
    "use_gcn_in_decoder = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7f3f906",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_train_loader, batch_val_loader = data_loading.load_data(obj_data_postfix = '_obj_boundary_sqr',\n",
    "                                                              part_data_post_fix = '_scaled_sqr',\n",
    "                                                              file_postfix = '_combined',\n",
    "                                                              seed=345,\n",
    "                                                              batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "361b90d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73f4529b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e573aa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d41ca05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "[1,   399] loss: 210.317\n",
      "200\n",
      "[2,   399] loss: 130.633\n",
      "200\n",
      "[3,   399] loss: 124.478\n",
      "200\n",
      "[4,   399] loss: 120.538\n",
      "200\n",
      "[5,   399] loss: 117.053\n",
      "200\n",
      "[6,   399] loss: 113.927\n",
      "200\n",
      "[7,   399] loss: 111.058\n",
      "200\n",
      "[8,   399] loss: 107.711\n",
      "200\n",
      "[9,   399] loss: 104.662\n",
      "200\n",
      "[10,   399] loss: 101.881\n",
      "200\n",
      "[11,   399] loss: 99.025\n",
      "200\n",
      "[12,   399] loss: 95.754\n",
      "200\n",
      "[13,   399] loss: 92.347\n",
      "200\n",
      "[14,   399] loss: 89.447\n",
      "200\n",
      "[15,   399] loss: 87.102\n",
      "200\n",
      "[16,   399] loss: 84.976\n",
      "200\n",
      "[17,   399] loss: 82.882\n",
      "200\n",
      "[18,   399] loss: 80.771\n",
      "200\n",
      "[19,   399] loss: 78.585\n",
      "200\n",
      "[20,   399] loss: 76.198\n",
      "200\n",
      "[21,   399] loss: 73.522\n",
      "200\n",
      "[22,   399] loss: 70.871\n",
      "200\n",
      "[23,   399] loss: 68.135\n",
      "200\n",
      "[24,   399] loss: 64.668\n",
      "200\n",
      "[25,   399] loss: 61.982\n",
      "200\n",
      "[26,   399] loss: 59.318\n",
      "200\n",
      "[27,   399] loss: 56.602\n",
      "200\n",
      "[28,   399] loss: 54.172\n",
      "200\n",
      "[29,   399] loss: 51.860\n",
      "200\n",
      "[30,   399] loss: 49.541\n",
      "200\n",
      "[31,   399] loss: 47.161\n",
      "200\n",
      "[32,   399] loss: 44.876\n",
      "200\n",
      "[33,   399] loss: 42.899\n",
      "200\n",
      "[34,   399] loss: 41.343\n",
      "200\n",
      "[35,   399] loss: 39.973\n",
      "200\n",
      "[36,   399] loss: 38.714\n",
      "200\n",
      "[37,   399] loss: 37.421\n",
      "200\n",
      "[38,   399] loss: 36.089\n",
      "200\n",
      "[39,   399] loss: 34.993\n",
      "200\n",
      "[40,   399] loss: 34.025\n",
      "200\n",
      "[41,   399] loss: 33.169\n",
      "200\n",
      "[42,   399] loss: 32.418\n",
      "200\n",
      "[43,   399] loss: 31.745\n",
      "200\n",
      "[44,   399] loss: 31.131\n",
      "200\n",
      "[45,   399] loss: 30.562\n",
      "200\n",
      "[46,   399] loss: 30.026\n",
      "200\n",
      "[47,   399] loss: 29.490\n",
      "200\n",
      "[48,   399] loss: 28.922\n",
      "200\n",
      "[49,   399] loss: 28.368\n",
      "200\n",
      "[50,   399] loss: 27.795\n",
      "200\n",
      "[51,   399] loss: 27.203\n",
      "200\n",
      "[52,   399] loss: 29.265\n",
      "200\n",
      "[53,   399] loss: 28.412\n",
      "200\n",
      "[54,   399] loss: 27.788\n",
      "200\n",
      "[55,   399] loss: 27.272\n",
      "200\n",
      "[56,   399] loss: 26.821\n",
      "200\n",
      "[57,   399] loss: 26.412\n",
      "200\n",
      "[58,   399] loss: 26.030\n",
      "200\n",
      "[59,   399] loss: 25.668\n",
      "200\n",
      "[60,   399] loss: 25.320\n",
      "200\n",
      "[61,   399] loss: 24.979\n",
      "200\n",
      "[62,   399] loss: 24.641\n",
      "200\n",
      "[63,   399] loss: 24.305\n",
      "200\n",
      "[64,   399] loss: 23.984\n",
      "200\n",
      "[65,   399] loss: 23.683\n",
      "200\n",
      "[66,   399] loss: 23.401\n",
      "200\n",
      "[67,   399] loss: 23.133\n",
      "200\n",
      "[68,   399] loss: 22.877\n",
      "200\n",
      "[69,   399] loss: 22.628\n",
      "200\n",
      "[70,   399] loss: 22.386\n",
      "200\n",
      "[71,   399] loss: 22.134\n",
      "200\n",
      "[72,   399] loss: 21.881\n",
      "200\n",
      "[73,   399] loss: 21.633\n",
      "200\n",
      "[74,   399] loss: 21.384\n",
      "200\n",
      "[75,   399] loss: 21.141\n",
      "200\n",
      "[76,   399] loss: 20.892\n",
      "200\n",
      "[77,   399] loss: 20.637\n",
      "200\n",
      "[78,   399] loss: 20.390\n",
      "200\n",
      "[79,   399] loss: 20.151\n",
      "200\n",
      "[80,   399] loss: 19.916\n",
      "200\n",
      "[81,   399] loss: 19.686\n",
      "200\n",
      "[82,   399] loss: 19.460\n",
      "200\n",
      "[83,   399] loss: 19.240\n",
      "200\n",
      "[84,   399] loss: 19.025\n",
      "200\n",
      "[85,   399] loss: 18.815\n",
      "200\n",
      "[86,   399] loss: 18.609\n",
      "200\n",
      "[87,   399] loss: 18.411\n",
      "200\n",
      "[88,   399] loss: 18.218\n",
      "200\n",
      "[89,   399] loss: 18.032\n",
      "200\n",
      "[90,   399] loss: 17.852\n",
      "200\n",
      "[91,   399] loss: 17.678\n",
      "200\n",
      "[92,   399] loss: 17.508\n",
      "200\n",
      "[93,   399] loss: 17.342\n",
      "200\n",
      "[94,   399] loss: 17.182\n",
      "200\n",
      "[95,   399] loss: 17.023\n",
      "200\n",
      "[96,   399] loss: 16.866\n",
      "200\n",
      "[97,   399] loss: 16.710\n",
      "200\n",
      "[98,   399] loss: 16.553\n",
      "200\n",
      "[99,   399] loss: 16.397\n",
      "200\n",
      "[100,   399] loss: 16.247\n",
      "200\n",
      "[101,   399] loss: 16.102\n",
      "200\n",
      "[102,   399] loss: 15.962\n",
      "200\n",
      "[103,   399] loss: 15.825\n",
      "200\n",
      "[104,   399] loss: 15.691\n",
      "200\n",
      "[105,   399] loss: 15.560\n",
      "200\n",
      "[106,   399] loss: 15.432\n",
      "200\n",
      "[107,   399] loss: 15.306\n",
      "200\n",
      "[108,   399] loss: 15.183\n",
      "200\n",
      "[109,   399] loss: 15.062\n",
      "200\n",
      "[110,   399] loss: 14.945\n",
      "200\n",
      "[111,   399] loss: 14.830\n",
      "200\n",
      "[112,   399] loss: 14.718\n",
      "200\n",
      "[113,   399] loss: 14.609\n",
      "200\n",
      "[114,   399] loss: 14.503\n",
      "200\n",
      "[115,   399] loss: 14.399\n",
      "200\n",
      "[116,   399] loss: 14.298\n",
      "200\n",
      "[117,   399] loss: 14.200\n",
      "200\n",
      "[118,   399] loss: 14.104\n",
      "200\n",
      "[119,   399] loss: 14.010\n",
      "200\n",
      "[120,   399] loss: 13.918\n",
      "200\n",
      "[121,   399] loss: 13.829\n",
      "200\n",
      "[122,   399] loss: 13.743\n",
      "200\n",
      "[123,   399] loss: 13.658\n",
      "200\n",
      "[124,   399] loss: 13.575\n",
      "200\n",
      "[125,   399] loss: 13.495\n",
      "200\n",
      "[126,   399] loss: 13.415\n",
      "200\n",
      "[127,   399] loss: 13.338\n",
      "200\n",
      "[128,   399] loss: 13.261\n",
      "200\n",
      "[129,   399] loss: 13.187\n",
      "200\n",
      "[130,   399] loss: 13.114\n",
      "200\n",
      "[131,   399] loss: 13.042\n",
      "200\n",
      "[132,   399] loss: 12.972\n",
      "200\n",
      "[133,   399] loss: 12.903\n",
      "200\n",
      "[134,   399] loss: 12.834\n",
      "200\n",
      "[135,   399] loss: 12.768\n",
      "200\n",
      "[136,   399] loss: 12.702\n",
      "200\n",
      "[137,   399] loss: 12.638\n",
      "200\n",
      "[138,   399] loss: 12.574\n",
      "200\n",
      "[139,   399] loss: 12.511\n",
      "200\n",
      "[140,   399] loss: 12.449\n",
      "200\n",
      "[141,   399] loss: 12.388\n",
      "200\n",
      "[142,   399] loss: 12.328\n",
      "200\n",
      "[143,   399] loss: 12.269\n",
      "200\n",
      "[144,   399] loss: 12.210\n",
      "200\n",
      "[145,   399] loss: 12.153\n",
      "200\n",
      "[146,   399] loss: 12.096\n",
      "200\n",
      "[147,   399] loss: 12.040\n",
      "200\n",
      "[148,   399] loss: 11.985\n",
      "200\n",
      "[149,   399] loss: 11.930\n",
      "200\n",
      "[150,   399] loss: 11.876\n",
      "200\n",
      "[151,   399] loss: 11.822\n",
      "200\n",
      "[152,   399] loss: 11.768\n",
      "200\n",
      "[153,   399] loss: 11.715\n",
      "200\n",
      "[154,   399] loss: 11.663\n",
      "200\n",
      "[155,   399] loss: 11.611\n",
      "200\n",
      "[156,   399] loss: 11.558\n",
      "200\n",
      "[157,   399] loss: 11.507\n",
      "200\n",
      "[158,   399] loss: 11.456\n",
      "200\n",
      "[159,   399] loss: 11.407\n",
      "200\n",
      "[160,   399] loss: 11.357\n",
      "200\n",
      "[161,   399] loss: 11.308\n",
      "200\n",
      "[162,   399] loss: 11.259\n",
      "200\n",
      "[163,   399] loss: 11.211\n",
      "200\n",
      "[164,   399] loss: 11.163\n",
      "200\n",
      "[165,   399] loss: 11.115\n",
      "200\n",
      "[166,   399] loss: 11.068\n",
      "200\n",
      "[167,   399] loss: 11.021\n",
      "200\n",
      "[168,   399] loss: 10.974\n",
      "200\n",
      "[169,   399] loss: 10.927\n",
      "200\n",
      "[170,   399] loss: 10.881\n",
      "200\n",
      "[171,   399] loss: 10.835\n",
      "200\n",
      "[172,   399] loss: 10.790\n",
      "200\n",
      "[173,   399] loss: 10.745\n",
      "200\n",
      "[174,   399] loss: 10.700\n",
      "200\n",
      "[175,   399] loss: 10.656\n",
      "200\n",
      "[176,   399] loss: 10.611\n",
      "200\n",
      "[177,   399] loss: 10.567\n",
      "200\n",
      "[178,   399] loss: 10.524\n",
      "200\n",
      "[179,   399] loss: 10.481\n",
      "200\n",
      "[180,   399] loss: 10.438\n",
      "200\n",
      "[181,   399] loss: 10.395\n",
      "200\n",
      "[182,   399] loss: 10.352\n",
      "200\n",
      "[183,   399] loss: 10.310\n",
      "200\n",
      "[184,   399] loss: 10.268\n",
      "200\n",
      "[185,   399] loss: 10.226\n",
      "200\n",
      "[186,   399] loss: 10.185\n",
      "200\n",
      "[187,   399] loss: 10.143\n",
      "200\n",
      "[188,   399] loss: 10.102\n",
      "200\n",
      "[189,   399] loss: 10.062\n",
      "200\n",
      "[190,   399] loss: 10.022\n",
      "200\n",
      "[191,   399] loss: 9.982\n",
      "200\n",
      "[192,   399] loss: 9.941\n",
      "200\n",
      "[193,   399] loss: 9.902\n",
      "200\n",
      "[194,   399] loss: 9.863\n",
      "200\n",
      "[195,   399] loss: 9.823\n",
      "200\n",
      "[196,   399] loss: 9.785\n",
      "200\n",
      "[197,   399] loss: 9.747\n",
      "200\n",
      "[198,   399] loss: 9.708\n",
      "200\n",
      "[199,   399] loss: 9.670\n",
      "200\n",
      "[200,   399] loss: 9.632\n",
      "200\n",
      "[201,   399] loss: 9.594\n",
      "200\n",
      "[202,   399] loss: 9.557\n",
      "200\n",
      "[203,   399] loss: 9.520\n",
      "200\n",
      "[204,   399] loss: 9.482\n",
      "200\n",
      "[205,   399] loss: 9.446\n",
      "200\n",
      "[206,   399] loss: 9.409\n",
      "200\n",
      "[207,   399] loss: 9.372\n",
      "200\n",
      "[208,   399] loss: 9.335\n",
      "200\n",
      "[209,   399] loss: 9.299\n",
      "200\n",
      "[210,   399] loss: 9.263\n",
      "200\n",
      "[211,   399] loss: 9.227\n",
      "200\n",
      "[212,   399] loss: 9.191\n",
      "200\n",
      "[213,   399] loss: 9.155\n",
      "200\n",
      "[214,   399] loss: 9.119\n",
      "200\n",
      "[215,   399] loss: 9.084\n",
      "200\n",
      "[216,   399] loss: 9.048\n",
      "200\n",
      "[217,   399] loss: 9.013\n",
      "200\n",
      "[218,   399] loss: 8.977\n",
      "200\n",
      "[219,   399] loss: 8.942\n",
      "200\n",
      "[220,   399] loss: 8.907\n",
      "200\n",
      "[221,   399] loss: 8.872\n",
      "200\n",
      "[222,   399] loss: 8.836\n",
      "200\n",
      "[223,   399] loss: 8.801\n",
      "200\n",
      "[224,   399] loss: 8.766\n",
      "200\n",
      "[225,   399] loss: 8.731\n",
      "200\n",
      "[226,   399] loss: 8.697\n",
      "200\n",
      "[227,   399] loss: 8.662\n",
      "200\n",
      "[228,   399] loss: 8.627\n",
      "200\n",
      "[229,   399] loss: 8.593\n",
      "200\n",
      "[230,   399] loss: 8.558\n",
      "200\n",
      "[231,   399] loss: 8.524\n",
      "200\n",
      "[232,   399] loss: 8.490\n",
      "200\n",
      "[233,   399] loss: 8.453\n",
      "200\n",
      "[234,   399] loss: 8.415\n",
      "200\n",
      "[235,   399] loss: 8.379\n",
      "200\n",
      "[236,   399] loss: 8.344\n",
      "200\n",
      "[237,   399] loss: 8.310\n",
      "200\n",
      "[238,   399] loss: 8.275\n",
      "200\n",
      "[239,   399] loss: 8.241\n",
      "200\n",
      "[240,   399] loss: 8.208\n",
      "200\n",
      "[241,   399] loss: 8.174\n",
      "200\n",
      "[242,   399] loss: 8.140\n",
      "200\n",
      "[243,   399] loss: 8.106\n",
      "200\n",
      "[244,   399] loss: 8.073\n",
      "200\n",
      "[245,   399] loss: 8.039\n",
      "200\n",
      "[246,   399] loss: 8.005\n",
      "200\n",
      "[247,   399] loss: 7.971\n",
      "200\n",
      "[248,   399] loss: 7.937\n",
      "200\n",
      "[249,   399] loss: 7.903\n",
      "200\n",
      "[250,   399] loss: 7.869\n",
      "200\n",
      "[251,   399] loss: 7.836\n",
      "200\n",
      "[252,   399] loss: 7.802\n",
      "200\n",
      "[253,   399] loss: 7.769\n",
      "200\n",
      "[254,   399] loss: 7.735\n",
      "200\n",
      "[255,   399] loss: 7.702\n",
      "200\n",
      "[256,   399] loss: 7.668\n",
      "200\n",
      "[257,   399] loss: 7.634\n",
      "200\n",
      "[258,   399] loss: 7.600\n",
      "200\n",
      "[259,   399] loss: 7.567\n",
      "200\n",
      "[260,   399] loss: 7.533\n",
      "200\n",
      "[261,   399] loss: 7.499\n",
      "200\n",
      "[262,   399] loss: 7.465\n",
      "200\n",
      "[263,   399] loss: 7.431\n",
      "200\n",
      "[264,   399] loss: 7.397\n",
      "200\n",
      "[265,   399] loss: 7.364\n",
      "200\n",
      "[266,   399] loss: 7.329\n",
      "200\n",
      "[267,   399] loss: 7.295\n",
      "200\n",
      "[268,   399] loss: 7.261\n",
      "200\n",
      "[269,   399] loss: 7.227\n",
      "200\n",
      "[270,   399] loss: 7.193\n",
      "200\n",
      "[271,   399] loss: 7.160\n",
      "200\n",
      "[272,   399] loss: 7.125\n",
      "200\n",
      "[273,   399] loss: 7.091\n",
      "200\n",
      "[274,   399] loss: 7.057\n",
      "200\n",
      "[275,   399] loss: 7.023\n",
      "200\n",
      "[276,   399] loss: 6.989\n",
      "200\n",
      "[277,   399] loss: 6.955\n",
      "200\n",
      "[278,   399] loss: 6.920\n",
      "200\n",
      "[279,   399] loss: 6.886\n",
      "200\n",
      "[280,   399] loss: 6.853\n",
      "200\n",
      "[281,   399] loss: 6.818\n",
      "200\n",
      "[282,   399] loss: 6.785\n",
      "200\n",
      "[283,   399] loss: 6.751\n",
      "200\n",
      "[284,   399] loss: 6.717\n",
      "200\n",
      "[285,   399] loss: 6.683\n",
      "200\n",
      "[286,   399] loss: 6.650\n",
      "200\n",
      "[287,   399] loss: 6.617\n",
      "200\n",
      "[288,   399] loss: 6.584\n",
      "200\n",
      "[289,   399] loss: 6.551\n",
      "200\n",
      "[290,   399] loss: 6.519\n",
      "200\n",
      "[291,   399] loss: 6.486\n",
      "200\n",
      "[292,   399] loss: 6.454\n",
      "200\n",
      "[293,   399] loss: 6.422\n",
      "200\n",
      "[294,   399] loss: 6.390\n",
      "200\n",
      "[295,   399] loss: 6.359\n",
      "200\n",
      "[296,   399] loss: 6.328\n",
      "200\n",
      "[297,   399] loss: 6.297\n",
      "200\n",
      "[298,   399] loss: 6.266\n",
      "200\n",
      "[299,   399] loss: 6.235\n",
      "200\n",
      "[300,   399] loss: 6.205\n",
      "200\n",
      "[301,   399] loss: 6.175\n",
      "200\n",
      "[302,   399] loss: 6.146\n",
      "200\n",
      "[303,   399] loss: 6.116\n",
      "200\n",
      "[304,   399] loss: 6.087\n",
      "200\n",
      "[305,   399] loss: 6.058\n",
      "200\n",
      "[306,   399] loss: 6.028\n",
      "200\n",
      "[307,   399] loss: 6.000\n",
      "200\n",
      "[308,   399] loss: 5.972\n",
      "200\n",
      "[309,   399] loss: 5.944\n",
      "200\n",
      "[310,   399] loss: 5.916\n",
      "200\n",
      "[311,   399] loss: 5.889\n",
      "200\n",
      "[312,   399] loss: 5.861\n",
      "200\n",
      "[313,   399] loss: 5.834\n",
      "200\n",
      "[314,   399] loss: 5.807\n",
      "200\n",
      "[315,   399] loss: 5.780\n",
      "200\n",
      "[316,   399] loss: 5.754\n",
      "200\n",
      "[317,   399] loss: 5.728\n",
      "200\n",
      "[318,   399] loss: 5.702\n",
      "200\n",
      "[319,   399] loss: 5.676\n",
      "200\n",
      "[320,   399] loss: 5.651\n",
      "200\n",
      "[321,   399] loss: 5.626\n",
      "200\n",
      "[322,   399] loss: 5.601\n",
      "200\n",
      "[323,   399] loss: 5.576\n",
      "200\n",
      "[324,   399] loss: 5.552\n",
      "200\n",
      "[325,   399] loss: 5.527\n",
      "200\n",
      "[326,   399] loss: 5.503\n",
      "200\n",
      "[327,   399] loss: 5.479\n",
      "200\n",
      "[328,   399] loss: 5.455\n",
      "200\n",
      "[329,   399] loss: 5.431\n",
      "200\n",
      "[330,   399] loss: 5.408\n",
      "200\n",
      "[331,   399] loss: 5.384\n",
      "200\n",
      "[332,   399] loss: 5.361\n",
      "200\n",
      "[333,   399] loss: 5.338\n",
      "200\n",
      "[334,   399] loss: 5.315\n",
      "200\n",
      "[335,   399] loss: 5.292\n",
      "200\n",
      "[336,   399] loss: 5.269\n",
      "200\n",
      "[337,   399] loss: 5.247\n",
      "200\n",
      "[338,   399] loss: 5.224\n",
      "200\n",
      "[339,   399] loss: 5.203\n",
      "200\n",
      "[340,   399] loss: 5.180\n",
      "200\n",
      "[341,   399] loss: 5.158\n",
      "200\n",
      "[342,   399] loss: 5.137\n",
      "200\n",
      "[343,   399] loss: 5.116\n",
      "200\n",
      "[344,   399] loss: 5.094\n",
      "200\n",
      "[345,   399] loss: 5.074\n",
      "200\n",
      "[346,   399] loss: 5.053\n",
      "200\n",
      "[347,   399] loss: 5.032\n",
      "200\n",
      "[348,   399] loss: 5.011\n",
      "200\n",
      "[349,   399] loss: 4.991\n",
      "200\n",
      "[350,   399] loss: 4.971\n",
      "200\n",
      "[351,   399] loss: 4.950\n",
      "200\n",
      "[352,   399] loss: 4.930\n",
      "200\n",
      "[353,   399] loss: 4.909\n",
      "200\n",
      "[354,   399] loss: 4.889\n",
      "200\n",
      "[355,   399] loss: 4.870\n",
      "200\n",
      "[356,   399] loss: 4.851\n",
      "200\n",
      "[357,   399] loss: 4.831\n",
      "200\n",
      "[358,   399] loss: 4.811\n",
      "200\n",
      "[359,   399] loss: 4.793\n",
      "200\n",
      "[360,   399] loss: 4.774\n",
      "200\n",
      "[361,   399] loss: 4.755\n",
      "200\n",
      "[362,   399] loss: 4.736\n",
      "200\n",
      "[363,   399] loss: 4.716\n",
      "200\n",
      "[364,   399] loss: 4.698\n",
      "200\n",
      "[365,   399] loss: 4.679\n",
      "200\n",
      "[366,   399] loss: 4.661\n",
      "200\n",
      "[367,   399] loss: 4.643\n",
      "200\n",
      "[368,   399] loss: 4.625\n",
      "200\n",
      "[369,   399] loss: 4.607\n",
      "200\n",
      "[370,   399] loss: 4.588\n",
      "200\n",
      "[371,   399] loss: 4.571\n",
      "200\n",
      "[372,   399] loss: 4.554\n",
      "200\n",
      "[373,   399] loss: 4.537\n",
      "200\n",
      "[374,   399] loss: 4.521\n",
      "200\n",
      "[375,   399] loss: 4.503\n",
      "200\n",
      "[376,   399] loss: 4.485\n",
      "200\n",
      "[377,   399] loss: 4.468\n",
      "200\n",
      "[378,   399] loss: 4.451\n",
      "200\n",
      "[379,   399] loss: 4.435\n",
      "200\n",
      "[380,   399] loss: 4.417\n",
      "200\n",
      "[381,   399] loss: 4.400\n",
      "200\n",
      "[382,   399] loss: 4.383\n",
      "200\n",
      "[383,   399] loss: 4.367\n",
      "200\n",
      "[384,   399] loss: 4.349\n",
      "200\n",
      "[385,   399] loss: 4.333\n",
      "200\n",
      "[386,   399] loss: 4.317\n",
      "200\n",
      "[387,   399] loss: 4.300\n",
      "200\n",
      "[388,   399] loss: 4.285\n",
      "200\n",
      "[389,   399] loss: 4.269\n",
      "200\n",
      "[390,   399] loss: 4.253\n",
      "200\n",
      "[391,   399] loss: 4.237\n",
      "200\n",
      "[392,   399] loss: 4.221\n",
      "200\n",
      "[393,   399] loss: 4.205\n",
      "200\n",
      "[394,   399] loss: 4.189\n",
      "200\n",
      "[395,   399] loss: 4.174\n",
      "200\n",
      "[396,   399] loss: 4.159\n",
      "200\n",
      "[397,   399] loss: 4.144\n",
      "200\n",
      "[398,   399] loss: 4.129\n",
      "200\n",
      "[399,   399] loss: 4.114\n",
      "200\n",
      "[400,   399] loss: 4.099\n",
      "200\n",
      "[401,   399] loss: 4.085\n",
      "200\n",
      "[402,   399] loss: 4.070\n",
      "200\n",
      "[403,   399] loss: 4.055\n",
      "200\n",
      "[404,   399] loss: 4.041\n",
      "200\n",
      "[405,   399] loss: 4.029\n",
      "200\n",
      "[406,   399] loss: 4.017\n",
      "200\n",
      "[407,   399] loss: 4.003\n",
      "200\n",
      "[408,   399] loss: 3.988\n",
      "200\n",
      "[409,   399] loss: 3.974\n",
      "200\n",
      "[410,   399] loss: 3.959\n",
      "200\n",
      "[411,   399] loss: 3.946\n",
      "200\n",
      "[412,   399] loss: 3.931\n",
      "200\n",
      "[413,   399] loss: 3.918\n",
      "200\n",
      "[414,   399] loss: 3.904\n",
      "200\n",
      "[415,   399] loss: 3.890\n",
      "200\n",
      "[416,   399] loss: 3.876\n",
      "200\n",
      "[417,   399] loss: 3.861\n",
      "200\n",
      "[418,   399] loss: 3.849\n",
      "200\n",
      "[419,   399] loss: 3.835\n",
      "200\n",
      "[420,   399] loss: 3.822\n",
      "200\n",
      "[421,   399] loss: 3.810\n",
      "200\n",
      "[422,   399] loss: 3.797\n",
      "200\n",
      "[423,   399] loss: 3.785\n",
      "200\n",
      "[424,   399] loss: 3.772\n",
      "200\n",
      "[425,   399] loss: 3.759\n",
      "200\n",
      "[426,   399] loss: 3.747\n",
      "200\n",
      "[427,   399] loss: 3.734\n",
      "200\n",
      "[428,   399] loss: 3.722\n",
      "200\n",
      "[429,   399] loss: 3.710\n",
      "200\n",
      "[430,   399] loss: 3.698\n",
      "200\n",
      "[431,   399] loss: 3.686\n",
      "200\n",
      "[432,   399] loss: 3.675\n",
      "200\n",
      "[433,   399] loss: 3.663\n",
      "200\n",
      "[434,   399] loss: 3.652\n",
      "200\n",
      "[435,   399] loss: 3.640\n",
      "200\n",
      "[436,   399] loss: 3.627\n",
      "200\n",
      "[437,   399] loss: 3.615\n",
      "200\n",
      "[438,   399] loss: 3.603\n",
      "200\n",
      "[439,   399] loss: 3.591\n",
      "200\n",
      "[440,   399] loss: 3.581\n",
      "200\n",
      "[441,   399] loss: 3.569\n",
      "200\n",
      "[442,   399] loss: 3.557\n",
      "200\n",
      "[443,   399] loss: 3.546\n",
      "200\n",
      "[444,   399] loss: 3.535\n",
      "200\n",
      "[445,   399] loss: 3.524\n",
      "200\n",
      "[446,   399] loss: 3.513\n",
      "200\n",
      "[447,   399] loss: 3.502\n",
      "200\n",
      "[448,   399] loss: 3.492\n",
      "200\n",
      "[449,   399] loss: 3.480\n",
      "200\n",
      "[450,   399] loss: 3.468\n",
      "200\n",
      "[451,   399] loss: 3.458\n",
      "200\n",
      "[452,   399] loss: 3.448\n",
      "200\n",
      "[453,   399] loss: 3.437\n",
      "200\n",
      "[454,   399] loss: 3.426\n",
      "200\n",
      "[455,   399] loss: 3.416\n",
      "200\n",
      "[456,   399] loss: 3.406\n",
      "200\n",
      "[457,   399] loss: 3.396\n",
      "200\n",
      "[458,   399] loss: 3.386\n",
      "200\n",
      "[459,   399] loss: 3.376\n",
      "200\n",
      "[460,   399] loss: 3.366\n",
      "200\n",
      "[461,   399] loss: 3.356\n",
      "200\n",
      "[462,   399] loss: 3.346\n",
      "200\n",
      "[463,   399] loss: 3.339\n",
      "200\n",
      "[464,   399] loss: 3.329\n",
      "200\n",
      "[465,   399] loss: 3.318\n",
      "200\n",
      "[466,   399] loss: 3.308\n",
      "200\n",
      "[467,   399] loss: 3.297\n",
      "200\n",
      "[468,   399] loss: 3.288\n",
      "200\n",
      "[469,   399] loss: 3.278\n",
      "200\n",
      "[470,   399] loss: 3.268\n",
      "200\n",
      "[471,   399] loss: 3.259\n",
      "200\n",
      "[472,   399] loss: 3.249\n",
      "200\n",
      "[473,   399] loss: 3.239\n",
      "200\n",
      "[474,   399] loss: 3.231\n",
      "200\n",
      "[475,   399] loss: 3.220\n",
      "200\n",
      "[476,   399] loss: 3.212\n",
      "200\n",
      "[477,   399] loss: 3.204\n",
      "200\n",
      "[478,   399] loss: 3.195\n",
      "200\n",
      "[479,   399] loss: 3.186\n",
      "200\n",
      "[480,   399] loss: 3.176\n",
      "200\n",
      "[481,   399] loss: 3.168\n",
      "200\n",
      "[482,   399] loss: 3.158\n",
      "200\n",
      "[483,   399] loss: 3.150\n",
      "200\n",
      "[484,   399] loss: 3.140\n",
      "200\n",
      "[485,   399] loss: 3.132\n",
      "200\n",
      "[486,   399] loss: 3.123\n",
      "200\n",
      "[487,   399] loss: 3.114\n",
      "200\n",
      "[488,   399] loss: 3.105\n",
      "200\n",
      "[489,   399] loss: 3.096\n",
      "200\n",
      "[490,   399] loss: 3.089\n",
      "200\n",
      "[491,   399] loss: 3.080\n",
      "200\n",
      "[492,   399] loss: 3.072\n",
      "200\n",
      "[493,   399] loss: 3.064\n",
      "200\n",
      "[494,   399] loss: 3.056\n",
      "200\n",
      "[495,   399] loss: 3.049\n",
      "200\n",
      "[496,   399] loss: 3.041\n",
      "200\n",
      "[497,   399] loss: 3.036\n",
      "200\n",
      "[498,   399] loss: 3.028\n",
      "200\n",
      "[499,   399] loss: 3.019\n",
      "200\n",
      "[500,   399] loss: 3.010\n",
      "200\n",
      "[501,   399] loss: 3.001\n",
      "200\n",
      "[502,   399] loss: 2.994\n",
      "200\n",
      "[503,   399] loss: 2.986\n",
      "200\n",
      "[504,   399] loss: 2.978\n",
      "200\n",
      "[505,   399] loss: 2.969\n",
      "200\n",
      "[506,   399] loss: 2.962\n",
      "200\n",
      "[507,   399] loss: 2.955\n",
      "200\n",
      "[508,   399] loss: 2.947\n",
      "200\n",
      "[509,   399] loss: 2.938\n",
      "200\n",
      "[510,   399] loss: 2.930\n",
      "200\n",
      "[511,   399] loss: 2.924\n",
      "200\n",
      "[512,   399] loss: 2.916\n",
      "200\n",
      "[513,   399] loss: 2.909\n",
      "200\n",
      "[514,   399] loss: 2.902\n",
      "200\n",
      "[515,   399] loss: 2.895\n",
      "200\n",
      "[516,   399] loss: 2.888\n",
      "200\n",
      "[517,   399] loss: 2.881\n",
      "200\n",
      "[518,   399] loss: 2.874\n",
      "200\n",
      "[519,   399] loss: 2.865\n",
      "200\n",
      "[520,   399] loss: 2.859\n",
      "200\n",
      "[521,   399] loss: 2.853\n",
      "200\n",
      "[522,   399] loss: 2.846\n",
      "200\n",
      "[523,   399] loss: 2.839\n",
      "200\n",
      "[524,   399] loss: 2.832\n",
      "200\n",
      "[525,   399] loss: 2.826\n",
      "200\n",
      "[526,   399] loss: 2.818\n",
      "200\n",
      "[527,   399] loss: 2.813\n",
      "200\n",
      "[528,   399] loss: 2.805\n",
      "200\n",
      "[529,   399] loss: 2.798\n",
      "200\n",
      "[530,   399] loss: 2.792\n",
      "200\n",
      "[531,   399] loss: 2.785\n",
      "200\n",
      "[532,   399] loss: 2.779\n",
      "200\n",
      "[533,   399] loss: 2.774\n",
      "200\n",
      "[534,   399] loss: 2.770\n",
      "200\n",
      "[535,   399] loss: 2.762\n",
      "200\n",
      "[536,   399] loss: 2.757\n",
      "200\n",
      "[537,   399] loss: 2.748\n",
      "200\n",
      "[538,   399] loss: 2.741\n",
      "200\n",
      "[539,   399] loss: 2.734\n",
      "200\n",
      "[540,   399] loss: 2.728\n",
      "200\n",
      "[541,   399] loss: 2.721\n",
      "200\n",
      "[542,   399] loss: 2.715\n",
      "200\n",
      "[543,   399] loss: 2.708\n",
      "200\n",
      "[544,   399] loss: 2.702\n",
      "200\n",
      "[545,   399] loss: 2.697\n",
      "200\n",
      "[546,   399] loss: 2.690\n",
      "200\n",
      "[547,   399] loss: 2.684\n",
      "200\n",
      "[548,   399] loss: 2.678\n",
      "200\n",
      "[549,   399] loss: 2.672\n",
      "200\n",
      "[550,   399] loss: 2.666\n",
      "200\n",
      "[551,   399] loss: 2.660\n",
      "200\n",
      "[552,   399] loss: 2.655\n",
      "200\n",
      "[553,   399] loss: 2.649\n",
      "200\n",
      "[554,   399] loss: 2.643\n",
      "200\n",
      "[555,   399] loss: 2.637\n",
      "200\n",
      "[556,   399] loss: 2.632\n",
      "200\n",
      "[557,   399] loss: 2.625\n",
      "200\n",
      "[558,   399] loss: 2.620\n",
      "200\n",
      "[559,   399] loss: 2.615\n",
      "200\n",
      "[560,   399] loss: 2.609\n",
      "200\n",
      "[561,   399] loss: 2.603\n",
      "200\n",
      "[562,   399] loss: 2.596\n",
      "200\n",
      "[563,   399] loss: 2.591\n",
      "200\n",
      "[564,   399] loss: 2.586\n",
      "200\n",
      "[565,   399] loss: 2.580\n",
      "200\n",
      "[566,   399] loss: 2.576\n",
      "200\n",
      "[567,   399] loss: 2.571\n",
      "200\n",
      "[568,   399] loss: 2.565\n",
      "200\n",
      "[569,   399] loss: 2.560\n",
      "200\n",
      "[570,   399] loss: 2.554\n",
      "200\n",
      "[571,   399] loss: 2.548\n",
      "200\n",
      "[572,   399] loss: 2.542\n",
      "200\n",
      "[573,   399] loss: 2.537\n",
      "200\n",
      "[574,   399] loss: 2.532\n",
      "200\n",
      "[575,   399] loss: 2.526\n",
      "200\n",
      "[576,   399] loss: 2.521\n",
      "200\n",
      "[577,   399] loss: 2.516\n",
      "200\n",
      "[578,   399] loss: 2.510\n",
      "200\n",
      "[579,   399] loss: 2.505\n",
      "200\n",
      "[580,   399] loss: 2.501\n",
      "200\n",
      "[581,   399] loss: 2.496\n",
      "200\n",
      "[582,   399] loss: 2.490\n",
      "200\n",
      "[583,   399] loss: 2.485\n",
      "200\n",
      "[584,   399] loss: 2.480\n",
      "200\n",
      "[585,   399] loss: 2.476\n",
      "200\n",
      "[586,   399] loss: 2.472\n",
      "200\n",
      "[587,   399] loss: 2.466\n",
      "200\n",
      "[588,   399] loss: 2.461\n",
      "200\n",
      "[589,   399] loss: 2.456\n",
      "200\n",
      "[590,   399] loss: 2.452\n",
      "200\n",
      "[591,   399] loss: 2.446\n",
      "200\n",
      "[592,   399] loss: 2.441\n",
      "200\n",
      "[593,   399] loss: 2.436\n",
      "200\n",
      "[594,   399] loss: 2.432\n",
      "200\n",
      "[595,   399] loss: 2.428\n",
      "200\n",
      "[596,   399] loss: 2.423\n",
      "200\n",
      "[597,   399] loss: 2.418\n",
      "200\n",
      "[598,   399] loss: 2.414\n",
      "200\n",
      "[599,   399] loss: 2.408\n",
      "200\n",
      "[600,   399] loss: 2.403\n",
      "200\n",
      "[601,   399] loss: 2.398\n",
      "200\n",
      "[602,   399] loss: 2.394\n",
      "200\n",
      "[603,   399] loss: 2.389\n",
      "200\n",
      "[604,   399] loss: 2.384\n",
      "200\n",
      "[605,   399] loss: 2.379\n",
      "200\n",
      "[606,   399] loss: 2.375\n",
      "200\n",
      "[607,   399] loss: 2.370\n",
      "200\n",
      "[608,   399] loss: 2.365\n",
      "200\n",
      "[609,   399] loss: 2.362\n",
      "200\n",
      "[610,   399] loss: 2.358\n",
      "200\n",
      "[611,   399] loss: 2.354\n",
      "200\n",
      "[612,   399] loss: 2.349\n",
      "200\n",
      "[613,   399] loss: 2.343\n",
      "200\n",
      "[614,   399] loss: 2.339\n",
      "200\n",
      "[615,   399] loss: 2.335\n",
      "200\n",
      "[616,   399] loss: 2.332\n",
      "200\n",
      "[617,   399] loss: 2.328\n",
      "200\n",
      "[618,   399] loss: 2.323\n",
      "200\n",
      "[619,   399] loss: 2.319\n",
      "200\n",
      "[620,   399] loss: 2.316\n",
      "200\n",
      "[621,   399] loss: 2.313\n",
      "200\n",
      "[622,   399] loss: 2.308\n",
      "200\n",
      "[623,   399] loss: 2.305\n",
      "200\n",
      "[624,   399] loss: 2.300\n",
      "200\n",
      "[625,   399] loss: 2.297\n",
      "200\n",
      "[626,   399] loss: 2.293\n",
      "200\n",
      "[627,   399] loss: 2.289\n",
      "200\n",
      "[628,   399] loss: 2.285\n",
      "200\n",
      "[629,   399] loss: 2.279\n",
      "200\n",
      "[630,   399] loss: 2.276\n",
      "200\n",
      "[631,   399] loss: 2.270\n",
      "200\n",
      "[632,   399] loss: 2.267\n",
      "200\n",
      "[633,   399] loss: 2.264\n",
      "200\n",
      "[634,   399] loss: 2.259\n",
      "200\n",
      "[635,   399] loss: 2.256\n",
      "200\n",
      "[636,   399] loss: 2.251\n",
      "200\n",
      "[637,   399] loss: 2.246\n",
      "200\n",
      "[638,   399] loss: 2.243\n",
      "200\n",
      "[639,   399] loss: 2.237\n",
      "200\n",
      "[640,   399] loss: 2.234\n",
      "200\n",
      "[641,   399] loss: 2.229\n",
      "200\n",
      "[642,   399] loss: 2.225\n",
      "200\n",
      "[643,   399] loss: 2.223\n",
      "200\n",
      "[644,   399] loss: 2.219\n",
      "200\n",
      "[645,   399] loss: 2.216\n",
      "200\n",
      "[646,   399] loss: 2.210\n",
      "200\n",
      "[647,   399] loss: 2.206\n",
      "200\n",
      "[648,   399] loss: 2.202\n",
      "200\n",
      "[649,   399] loss: 2.199\n",
      "200\n",
      "[650,   399] loss: 2.195\n",
      "200\n",
      "[651,   399] loss: 2.192\n",
      "200\n",
      "[652,   399] loss: 2.189\n",
      "200\n",
      "[653,   399] loss: 2.185\n",
      "200\n",
      "[654,   399] loss: 2.181\n",
      "200\n",
      "[655,   399] loss: 2.177\n",
      "200\n",
      "[656,   399] loss: 2.174\n",
      "200\n",
      "[657,   399] loss: 2.171\n",
      "200\n",
      "[658,   399] loss: 2.167\n",
      "200\n",
      "[659,   399] loss: 2.163\n",
      "200\n",
      "[660,   399] loss: 2.160\n",
      "200\n",
      "[661,   399] loss: 2.155\n",
      "200\n",
      "[662,   399] loss: 2.152\n",
      "200\n",
      "[663,   399] loss: 2.148\n",
      "200\n",
      "[664,   399] loss: 2.145\n",
      "200\n",
      "[665,   399] loss: 2.141\n",
      "200\n",
      "[666,   399] loss: 2.138\n",
      "200\n",
      "[667,   399] loss: 2.134\n",
      "200\n",
      "[668,   399] loss: 2.130\n",
      "200\n",
      "[669,   399] loss: 2.127\n",
      "200\n",
      "[670,   399] loss: 2.122\n",
      "200\n",
      "[671,   399] loss: 2.120\n",
      "200\n",
      "[672,   399] loss: 2.117\n",
      "200\n",
      "[673,   399] loss: 2.112\n",
      "200\n",
      "[674,   399] loss: 2.110\n",
      "200\n",
      "[675,   399] loss: 2.105\n",
      "200\n",
      "[676,   399] loss: 2.102\n",
      "200\n",
      "[677,   399] loss: 2.099\n",
      "200\n",
      "[678,   399] loss: 2.095\n",
      "200\n",
      "[679,   399] loss: 2.092\n",
      "200\n",
      "[680,   399] loss: 2.088\n",
      "200\n",
      "[681,   399] loss: 2.085\n",
      "200\n",
      "[682,   399] loss: 2.083\n",
      "200\n",
      "[683,   399] loss: 2.080\n",
      "200\n",
      "[684,   399] loss: 2.075\n",
      "200\n",
      "[685,   399] loss: 2.072\n",
      "200\n",
      "[686,   399] loss: 2.070\n",
      "200\n",
      "[687,   399] loss: 2.069\n",
      "200\n",
      "[688,   399] loss: 2.066\n",
      "200\n",
      "[689,   399] loss: 2.061\n",
      "200\n",
      "[690,   399] loss: 2.058\n",
      "200\n",
      "[691,   399] loss: 2.056\n",
      "200\n",
      "[692,   399] loss: 2.052\n",
      "200\n",
      "[693,   399] loss: 2.048\n",
      "200\n",
      "[694,   399] loss: 2.046\n",
      "200\n",
      "[695,   399] loss: 2.042\n",
      "200\n",
      "[696,   399] loss: 2.038\n",
      "200\n",
      "[697,   399] loss: 2.035\n",
      "200\n",
      "[698,   399] loss: 2.032\n",
      "200\n",
      "[699,   399] loss: 2.028\n",
      "200\n",
      "[700,   399] loss: 2.024\n",
      "200\n",
      "[701,   399] loss: 2.021\n",
      "200\n",
      "[702,   399] loss: 2.019\n",
      "200\n",
      "[703,   399] loss: 2.016\n",
      "200\n",
      "[704,   399] loss: 2.014\n",
      "200\n",
      "[705,   399] loss: 2.010\n",
      "200\n",
      "[706,   399] loss: 2.008\n",
      "200\n",
      "[707,   399] loss: 2.005\n",
      "200\n",
      "[708,   399] loss: 2.002\n",
      "200\n",
      "[709,   399] loss: 1.999\n",
      "200\n",
      "[710,   399] loss: 1.995\n",
      "200\n",
      "[711,   399] loss: 1.993\n",
      "200\n",
      "[712,   399] loss: 1.992\n",
      "200\n",
      "[713,   399] loss: 1.989\n",
      "200\n",
      "[714,   399] loss: 1.987\n",
      "200\n",
      "[715,   399] loss: 1.984\n",
      "200\n",
      "[716,   399] loss: 1.981\n",
      "200\n",
      "[717,   399] loss: 1.977\n",
      "200\n",
      "[718,   399] loss: 1.973\n",
      "200\n",
      "[719,   399] loss: 1.969\n",
      "200\n",
      "[720,   399] loss: 1.964\n",
      "200\n",
      "[721,   399] loss: 1.962\n",
      "200\n",
      "[722,   399] loss: 1.959\n",
      "200\n",
      "[723,   399] loss: 1.957\n",
      "200\n",
      "[724,   399] loss: 1.955\n",
      "200\n",
      "[725,   399] loss: 1.953\n",
      "200\n",
      "[726,   399] loss: 1.949\n",
      "200\n",
      "[727,   399] loss: 1.945\n",
      "200\n",
      "[728,   399] loss: 1.943\n",
      "200\n",
      "[729,   399] loss: 1.940\n",
      "200\n",
      "[730,   399] loss: 1.938\n",
      "200\n",
      "[731,   399] loss: 1.935\n",
      "200\n",
      "[732,   399] loss: 1.935\n",
      "200\n",
      "[733,   399] loss: 1.934\n",
      "200\n",
      "[734,   399] loss: 1.929\n",
      "200\n",
      "[735,   399] loss: 1.927\n",
      "200\n",
      "[736,   399] loss: 1.924\n",
      "200\n",
      "[737,   399] loss: 1.922\n",
      "200\n",
      "[738,   399] loss: 1.918\n",
      "200\n",
      "[739,   399] loss: 1.916\n",
      "200\n",
      "[740,   399] loss: 1.913\n",
      "200\n",
      "[741,   399] loss: 1.910\n",
      "200\n",
      "[742,   399] loss: 1.908\n",
      "200\n",
      "[743,   399] loss: 1.905\n",
      "200\n",
      "[744,   399] loss: 1.901\n",
      "200\n",
      "[745,   399] loss: 1.898\n",
      "200\n",
      "[746,   399] loss: 1.894\n",
      "200\n",
      "[747,   399] loss: 1.891\n",
      "200\n",
      "[748,   399] loss: 1.889\n",
      "200\n",
      "[749,   399] loss: 1.887\n",
      "200\n",
      "[750,   399] loss: 1.885\n",
      "200\n",
      "[751,   399] loss: 1.881\n",
      "200\n",
      "[752,   399] loss: 1.878\n",
      "200\n",
      "[753,   399] loss: 1.877\n",
      "200\n",
      "[754,   399] loss: 1.875\n",
      "200\n",
      "[755,   399] loss: 1.873\n",
      "200\n",
      "[756,   399] loss: 1.868\n",
      "200\n",
      "[757,   399] loss: 1.868\n",
      "200\n",
      "[758,   399] loss: 1.865\n",
      "200\n",
      "[759,   399] loss: 1.862\n",
      "200\n",
      "[760,   399] loss: 1.861\n",
      "200\n",
      "[761,   399] loss: 1.861\n",
      "200\n",
      "[762,   399] loss: 1.857\n",
      "200\n",
      "[763,   399] loss: 1.856\n",
      "200\n",
      "[764,   399] loss: 1.853\n",
      "200\n",
      "[765,   399] loss: 1.851\n",
      "200\n",
      "[766,   399] loss: 1.848\n",
      "200\n",
      "[767,   399] loss: 1.846\n",
      "200\n",
      "[768,   399] loss: 1.843\n",
      "200\n",
      "[769,   399] loss: 1.840\n",
      "200\n",
      "[770,   399] loss: 1.836\n",
      "200\n",
      "[771,   399] loss: 1.833\n",
      "200\n",
      "[772,   399] loss: 1.829\n",
      "200\n",
      "[773,   399] loss: 1.829\n",
      "200\n",
      "[774,   399] loss: 1.826\n",
      "200\n",
      "[775,   399] loss: 1.823\n",
      "200\n",
      "[776,   399] loss: 1.821\n",
      "200\n",
      "[777,   399] loss: 1.818\n",
      "200\n",
      "[778,   399] loss: 1.817\n",
      "200\n",
      "[779,   399] loss: 1.813\n",
      "200\n",
      "[780,   399] loss: 1.812\n",
      "200\n",
      "[781,   399] loss: 1.809\n",
      "200\n",
      "[782,   399] loss: 1.806\n",
      "200\n",
      "[783,   399] loss: 1.805\n",
      "200\n",
      "[784,   399] loss: 1.803\n",
      "200\n",
      "[785,   399] loss: 1.800\n",
      "200\n",
      "[786,   399] loss: 1.797\n",
      "200\n",
      "[787,   399] loss: 1.797\n",
      "200\n",
      "[788,   399] loss: 1.797\n",
      "200\n",
      "[789,   399] loss: 1.795\n",
      "200\n",
      "[790,   399] loss: 1.793\n",
      "200\n",
      "[791,   399] loss: 1.790\n",
      "200\n",
      "[792,   399] loss: 1.788\n",
      "200\n",
      "[793,   399] loss: 1.784\n",
      "200\n",
      "[794,   399] loss: 1.784\n",
      "200\n",
      "[795,   399] loss: 1.780\n",
      "200\n",
      "[796,   399] loss: 1.778\n",
      "200\n",
      "[797,   399] loss: 1.775\n",
      "200\n",
      "[798,   399] loss: 1.771\n",
      "200\n",
      "[799,   399] loss: 1.769\n",
      "200\n",
      "[800,   399] loss: 1.767\n",
      "200\n",
      "[801,   399] loss: 1.764\n",
      "200\n",
      "[802,   399] loss: 1.762\n",
      "200\n",
      "[803,   399] loss: 1.762\n",
      "200\n",
      "[804,   399] loss: 1.758\n",
      "200\n",
      "[805,   399] loss: 1.756\n",
      "200\n",
      "[806,   399] loss: 1.754\n",
      "200\n",
      "[807,   399] loss: 1.751\n",
      "200\n",
      "[808,   399] loss: 1.750\n",
      "200\n",
      "[809,   399] loss: 1.748\n",
      "200\n",
      "[810,   399] loss: 1.747\n",
      "200\n",
      "[811,   399] loss: 1.744\n",
      "200\n",
      "[812,   399] loss: 1.742\n",
      "200\n",
      "[813,   399] loss: 1.740\n",
      "200\n",
      "[814,   399] loss: 1.741\n",
      "200\n",
      "[815,   399] loss: 1.740\n",
      "200\n",
      "[816,   399] loss: 1.737\n",
      "200\n",
      "[817,   399] loss: 1.734\n",
      "200\n",
      "[818,   399] loss: 1.733\n",
      "200\n",
      "[819,   399] loss: 1.730\n",
      "200\n",
      "[820,   399] loss: 1.727\n",
      "200\n",
      "[821,   399] loss: 1.723\n",
      "200\n",
      "[822,   399] loss: 1.721\n",
      "200\n",
      "[823,   399] loss: 1.717\n",
      "200\n",
      "[824,   399] loss: 1.716\n",
      "200\n",
      "[825,   399] loss: 1.714\n",
      "200\n",
      "[826,   399] loss: 1.711\n",
      "200\n",
      "[827,   399] loss: 1.711\n",
      "200\n",
      "[828,   399] loss: 1.708\n",
      "200\n",
      "[829,   399] loss: 1.707\n",
      "200\n",
      "[830,   399] loss: 1.706\n",
      "200\n",
      "[831,   399] loss: 1.705\n",
      "200\n",
      "[832,   399] loss: 1.703\n",
      "200\n",
      "[833,   399] loss: 1.703\n",
      "200\n",
      "[834,   399] loss: 1.700\n",
      "200\n",
      "[835,   399] loss: 1.700\n",
      "200\n",
      "[836,   399] loss: 1.696\n",
      "200\n",
      "[837,   399] loss: 1.693\n",
      "200\n",
      "[838,   399] loss: 1.690\n",
      "200\n",
      "[839,   399] loss: 1.687\n",
      "200\n",
      "[840,   399] loss: 1.684\n",
      "200\n",
      "[841,   399] loss: 1.683\n",
      "200\n",
      "[842,   399] loss: 1.681\n",
      "200\n",
      "[843,   399] loss: 1.679\n",
      "200\n",
      "[844,   399] loss: 1.677\n",
      "200\n",
      "[845,   399] loss: 1.675\n",
      "200\n",
      "[846,   399] loss: 1.674\n",
      "200\n",
      "[847,   399] loss: 1.674\n",
      "200\n",
      "[848,   399] loss: 1.671\n",
      "200\n",
      "[849,   399] loss: 1.671\n",
      "200\n",
      "[850,   399] loss: 1.668\n",
      "200\n",
      "[851,   399] loss: 1.666\n",
      "200\n",
      "[852,   399] loss: 1.666\n",
      "200\n",
      "[853,   399] loss: 1.663\n",
      "200\n",
      "[854,   399] loss: 1.663\n",
      "200\n",
      "[855,   399] loss: 1.661\n",
      "200\n",
      "[856,   399] loss: 1.659\n",
      "200\n",
      "[857,   399] loss: 1.657\n",
      "200\n",
      "[858,   399] loss: 1.652\n",
      "200\n",
      "[859,   399] loss: 1.649\n",
      "200\n",
      "[860,   399] loss: 1.649\n",
      "200\n",
      "[861,   399] loss: 1.644\n",
      "200\n",
      "[862,   399] loss: 1.645\n",
      "200\n",
      "[863,   399] loss: 1.643\n",
      "200\n",
      "[864,   399] loss: 1.641\n",
      "200\n",
      "[865,   399] loss: 1.639\n",
      "200\n",
      "[866,   399] loss: 1.639\n",
      "200\n",
      "[867,   399] loss: 1.637\n",
      "200\n",
      "[868,   399] loss: 1.634\n",
      "200\n",
      "[869,   399] loss: 1.632\n",
      "200\n",
      "[870,   399] loss: 1.631\n",
      "200\n",
      "[871,   399] loss: 1.629\n",
      "200\n",
      "[872,   399] loss: 1.628\n",
      "200\n",
      "[873,   399] loss: 1.626\n",
      "200\n",
      "[874,   399] loss: 1.625\n",
      "200\n",
      "[875,   399] loss: 1.624\n",
      "200\n",
      "[876,   399] loss: 1.623\n",
      "200\n",
      "[877,   399] loss: 1.621\n",
      "200\n",
      "[878,   399] loss: 1.620\n",
      "200\n",
      "[879,   399] loss: 1.620\n",
      "200\n",
      "[880,   399] loss: 1.616\n",
      "200\n",
      "[881,   399] loss: 1.612\n",
      "200\n",
      "[882,   399] loss: 1.611\n",
      "200\n",
      "[883,   399] loss: 1.608\n",
      "200\n",
      "[884,   399] loss: 1.605\n",
      "200\n",
      "[885,   399] loss: 1.604\n",
      "200\n",
      "[886,   399] loss: 1.601\n",
      "200\n",
      "[887,   399] loss: 1.599\n",
      "200\n",
      "[888,   399] loss: 1.599\n",
      "200\n",
      "[889,   399] loss: 1.597\n",
      "200\n",
      "[890,   399] loss: 1.597\n",
      "200\n",
      "[891,   399] loss: 1.594\n",
      "200\n",
      "[892,   399] loss: 1.593\n",
      "200\n",
      "[893,   399] loss: 1.593\n",
      "200\n",
      "[894,   399] loss: 1.590\n",
      "200\n",
      "[895,   399] loss: 1.588\n",
      "200\n",
      "[896,   399] loss: 1.587\n",
      "200\n",
      "[897,   399] loss: 1.586\n",
      "200\n",
      "[898,   399] loss: 1.584\n",
      "200\n",
      "[899,   399] loss: 1.583\n",
      "200\n",
      "[900,   399] loss: 1.582\n",
      "200\n",
      "[901,   399] loss: 1.581\n",
      "200\n",
      "[902,   399] loss: 1.579\n",
      "200\n",
      "[903,   399] loss: 1.578\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/PIL/ImageFile.py:536\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 536\u001b[0m     fh \u001b[38;5;241m=\u001b[39m fp\u001b[38;5;241m.\u001b[39mfileno()\n\u001b[1;32m    537\u001b[0m     fp\u001b[38;5;241m.\u001b[39mflush()\n",
      "\u001b[0;31mAttributeError\u001b[0m: '_idat' object has no attribute 'fileno'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 247\u001b[0m\n\u001b[1;32m    245\u001b[0m writer\u001b[38;5;241m.\u001b[39madd_image(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval/images/generated\u001b[39m\u001b[38;5;124m'\u001b[39m, image, global_step, dataformats\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHWC\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    246\u001b[0m image \u001b[38;5;241m=\u001b[39m plot_utils\u001b[38;5;241m.\u001b[39mplot_bbx((node_data_pred_refined[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m*\u001b[39mlabel_true[:num_nodes])\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy())\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mfloat\u001b[39m)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255\u001b[39m\n\u001b[0;32m--> 247\u001b[0m writer\u001b[38;5;241m.\u001b[39madd_image(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval/images/refined\u001b[39m\u001b[38;5;124m'\u001b[39m, image, global_step, dataformats\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHWC\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    249\u001b[0m reconstruction_loss_val_arr\u001b[38;5;241m.\u001b[39mappend(batch_loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;241m/\u001b[39m(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    250\u001b[0m bbox_loss_val_arr\u001b[38;5;241m.\u001b[39mappend(batch_bbox_loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;241m/\u001b[39m(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/utils/tensorboard/writer.py:625\u001b[0m, in \u001b[0;36mSummaryWriter.add_image\u001b[0;34m(self, tag, img_tensor, global_step, walltime, dataformats)\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Add image data to summary.\u001b[39;00m\n\u001b[1;32m    581\u001b[0m \n\u001b[1;32m    582\u001b[0m \u001b[38;5;124;03mNote that this requires the ``pillow`` package.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    621\u001b[0m \n\u001b[1;32m    622\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    623\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_once(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorboard.logging.add_image\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    624\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_file_writer()\u001b[38;5;241m.\u001b[39madd_summary(\n\u001b[0;32m--> 625\u001b[0m     image(tag, img_tensor, dataformats\u001b[38;5;241m=\u001b[39mdataformats), global_step, walltime\n\u001b[1;32m    626\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/utils/tensorboard/summary.py:577\u001b[0m, in \u001b[0;36mimage\u001b[0;34m(tag, tensor, rescale, dataformats)\u001b[0m\n\u001b[1;32m    575\u001b[0m tensor \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m    576\u001b[0m tensor \u001b[38;5;241m=\u001b[39m (tensor \u001b[38;5;241m*\u001b[39m scale_factor)\u001b[38;5;241m.\u001b[39mclip(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8)\n\u001b[0;32m--> 577\u001b[0m image \u001b[38;5;241m=\u001b[39m make_image(tensor, rescale\u001b[38;5;241m=\u001b[39mrescale)\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Summary(value\u001b[38;5;241m=\u001b[39m[Summary\u001b[38;5;241m.\u001b[39mValue(tag\u001b[38;5;241m=\u001b[39mtag, image\u001b[38;5;241m=\u001b[39mimage)])\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/utils/tensorboard/summary.py:630\u001b[0m, in \u001b[0;36mmake_image\u001b[0;34m(tensor, rescale, rois, labels)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mio\u001b[39;00m\n\u001b[1;32m    629\u001b[0m output \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO()\n\u001b[0;32m--> 630\u001b[0m image\u001b[38;5;241m.\u001b[39msave(output, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPNG\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    631\u001b[0m image_string \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[1;32m    632\u001b[0m output\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/PIL/Image.py:2459\u001b[0m, in \u001b[0;36mImage.save\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2456\u001b[0m         fp \u001b[38;5;241m=\u001b[39m builtins\u001b[38;5;241m.\u001b[39mopen(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw+b\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2458\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2459\u001b[0m     save_handler(\u001b[38;5;28mself\u001b[39m, fp, filename)\n\u001b[1;32m   2460\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   2461\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m open_fp:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/PIL/PngImagePlugin.py:1412\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(im, fp, filename, chunk, save_all)\u001b[0m\n\u001b[1;32m   1408\u001b[0m     im \u001b[38;5;241m=\u001b[39m _write_multiple_frames(\n\u001b[1;32m   1409\u001b[0m         im, fp, chunk, rawmode, default_image, append_images\n\u001b[1;32m   1410\u001b[0m     )\n\u001b[1;32m   1411\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m im:\n\u001b[0;32m-> 1412\u001b[0m     ImageFile\u001b[38;5;241m.\u001b[39m_save(im, _idat(fp, chunk), [(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzip\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m im\u001b[38;5;241m.\u001b[39msize, \u001b[38;5;241m0\u001b[39m, rawmode)])\n\u001b[1;32m   1414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info:\n\u001b[1;32m   1415\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m info_chunk \u001b[38;5;129;01min\u001b[39;00m info\u001b[38;5;241m.\u001b[39mchunks:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/PIL/ImageFile.py:540\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[1;32m    538\u001b[0m     _encode_tile(im, fp, tile, bufsize, fh)\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, io\u001b[38;5;241m.\u001b[39mUnsupportedOperation) \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m--> 540\u001b[0m     _encode_tile(im, fp, tile, bufsize, \u001b[38;5;28;01mNone\u001b[39;00m, exc)\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflush\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    542\u001b[0m     fp\u001b[38;5;241m.\u001b[39mflush()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/PIL/ImageFile.py:559\u001b[0m, in \u001b[0;36m_encode_tile\u001b[0;34m(im, fp, tile, bufsize, fh, exc)\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exc:\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;66;03m# compress to Python file-compatible object\u001b[39;00m\n\u001b[1;32m    558\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 559\u001b[0m         errcode, data \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39mencode(bufsize)[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m    560\u001b[0m         fp\u001b[38;5;241m.\u001b[39mwrite(data)\n\u001b[1;32m    561\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m errcode:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "reconstruction_loss_arr = []\n",
    "kl_loss_obj_arr = []\n",
    "kl_loss_part_arr = []\n",
    "bbox_loss_arr = []\n",
    "refined_bbox_loss_arr = []\n",
    "adj_loss_arr = []\n",
    "node_loss_arr = []\n",
    "\n",
    "reconstruction_loss_val_arr = []\n",
    "kl_loss_val_arr = []\n",
    "bbox_loss_val_arr = []\n",
    "refined_bbox_loss_val_arr = []\n",
    "adj_loss_val_arr = []\n",
    "node_loss_val_arr = []\n",
    "\n",
    "bbox_loss_threshold = 1.0\n",
    "vae = TwoStageAutoEncoder(latent_dims,\n",
    "                          num_nodes,\n",
    "                          bbx_size,\n",
    "                          num_classes,\n",
    "                          label_shape,\n",
    "                          hidden1,\n",
    "                          hidden2,\n",
    "                          hidden3,\n",
    "                          dense_hidden1,\n",
    "                          dense_hidden2,\n",
    "                          adaptive_margin,\n",
    "                          output_log,\n",
    "                          area_encoding,\n",
    "                          coupling,\n",
    "                          obj_bbx_conditioning,\n",
    "                          use_fft_on_bbx,\n",
    "                          use_gcn_in_decoder\n",
    "                        )\n",
    "vae.to(device)\n",
    "optimizer = optim.Adam(vae.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100,400], gamma=0.85)\n",
    "model_path = ('/Users/amrutamuthal/Documents/training_runs/model/'+run_prefix+'/Obj-Box-'\n",
    "                        +str(learning_rate)\n",
    "                        +'-batch-'+str(batch_size)\n",
    "                        +'-h1-'+str(hidden1)\n",
    "                        +'-h2-'+str(hidden2)\n",
    "                        +'-h3-'+str(hidden3)+'-test')\n",
    "summary_path = ('/Users/amrutamuthal/Documents/training_runs/runs/'+run_prefix+'/Obj-Box-'\n",
    "                        +str(learning_rate)\n",
    "                        +'-batch-'+str(batch_size)\n",
    "                        +'-h1-'+str(hidden1)\n",
    "                        +'-h2-'+str(hidden2)\n",
    "                        +'-h3-'+str(hidden3)+'-test')\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "writer = SummaryWriter(summary_path)\n",
    "icoef = 0\n",
    "\n",
    "for epoch in range(nb_epochs):\n",
    "    \n",
    "    batch_loss = torch.tensor([0.0])\n",
    "    batch_kl_loss_part = torch.tensor([0.0])\n",
    "    batch_kl_loss_obj = torch.tensor([0.0])\n",
    "    batch_bbox_loss = torch.tensor([0.0])\n",
    "    batch_refined_bbox_loss = torch.tensor([0.0])\n",
    "    batch_obj_bbox_loss = torch.tensor([0.0])\n",
    "    batch_node_loss = torch.tensor([0.0])\n",
    "    IOU_weight_delta = torch.tensor([(1+epoch)/nb_epochs])\n",
    "    images = []\n",
    "    \n",
    "    vae.train()\n",
    "    i=0\n",
    "    for train_data in batch_train_loader:\n",
    "        \n",
    "        node_data_true = train_data.x\n",
    "        label_true = node_data_true[:,:1]\n",
    "        y_true = train_data.y\n",
    "        class_true = y_true[:, :num_classes]\n",
    "        X_obj_true = y_true[:, num_classes:]\n",
    "        X_obj_true_transformed = X_obj_true[:,2:]-X_obj_true[:,:2]\n",
    "        adj_true = train_data.edge_index\n",
    "        batch = train_data.batch\n",
    "        \n",
    "        class_true  = torch.flatten(class_true)\n",
    "        \n",
    "\n",
    "        for param in vae.parameters():\n",
    "            param.grad=None\n",
    "        \n",
    "        output = vae(adj_true, node_data_true, X_obj_true_transformed, label_true , class_true, variational, coupling, refine_iter=2)\n",
    "        \n",
    "        node_data_pred = output[0]\n",
    "        X_obj_pred = output[1]\n",
    "        X_obj_pred = torch.cat(((torch.tensor([1.0])-X_obj_pred)*torch.tensor([0.5]),\n",
    "                                (torch.tensor([1.0])+X_obj_pred)*torch.tensor([0.5])),\n",
    "                              -1)\n",
    "        label_pred = output[2]\n",
    "        z_mean_part = output[3]\n",
    "        z_logvar_part = output[4]\n",
    "        margin = output[5]\n",
    "        z_mean_obj = output[6]\n",
    "        z_logvar_obj = output[7]\n",
    "        node_data_pred_refined = output[8]\n",
    "        \n",
    "        obj_bbox_loss = loss.obj_bbox_loss(pred_box=X_obj_pred, true_box=X_obj_true, weight=IOU_weight_delta, has_mse=False)\n",
    "        kl_loss_obj = loss.kl_loss(z_mean_obj, z_logvar_obj)\n",
    "        kl_loss_part = loss.kl_loss(z_mean_part, z_logvar_part)\n",
    "        bbox_loss = loss.weighted_bbox_loss(pred_box=node_data_pred, true_box=node_data_true[:,1:], weight=IOU_weight_delta, margin=margin)\n",
    "        bbox_loss_coarse = loss.coarse_bbx_loss(pred_box=node_data_pred, true_box=node_data_true[:,1:])\n",
    "        node_loss = loss.node_loss(label_pred,label_true)\n",
    "        \n",
    "        # reconstruction_loss = (node_loss)*num_nodes\n",
    "\n",
    "        if use_gcn_in_decoder:\n",
    "            refined_bbox_loss = loss.weighted_bbox_loss(\n",
    "                pred_box=node_data_pred_refined,\n",
    "                true_box=node_data_true[:, 1:],\n",
    "                weight=IOU_weight_delta,\n",
    "                margin=margin,\n",
    "            )\n",
    "            reconstruction_loss = (refined_bbox_loss + node_loss) * num_nodes # * epoch / nb_epochs\n",
    "        \n",
    "        else:\n",
    "            refined_bbox_loss = torch.tensor([0.0])\n",
    "        \n",
    "        kl_weight = klw[icoef]\n",
    "        if variational and (kl_weight>0):\n",
    "            reconstruction_loss += kl_loss_part*kl_weight\n",
    "\n",
    "        if epoch > 50:\n",
    "            reconstruction_loss += bbox_loss_coarse*10  \n",
    "\n",
    "        reconstruction_loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        i+=1\n",
    "      \n",
    "        batch_loss += reconstruction_loss\n",
    "        batch_kl_loss_part += kl_loss_part\n",
    "        batch_kl_loss_obj += kl_loss_obj\n",
    "        batch_bbox_loss += bbox_loss\n",
    "        batch_refined_bbox_loss += refined_bbox_loss\n",
    "        batch_obj_bbox_loss += obj_bbox_loss\n",
    "        batch_node_loss += node_loss\n",
    "    \n",
    "        if i%200==0:\n",
    "            print(i)\n",
    "            global_step = epoch*len(batch_train_loader)+i\n",
    "            \n",
    "            writer.add_scalar(\"Loss/train/reconstruction_loss\", batch_loss.item()/(i+1), global_step)\n",
    "            writer.add_scalar(\"Loss/train/kl_loss_part\", batch_kl_loss_part.item()/(i+1), global_step)\n",
    "            writer.add_scalar(\"Loss/train/kl_loss_obj\", batch_kl_loss_obj.item()/(i+1), global_step)\n",
    "            writer.add_scalar(\"Loss/train/bbox_loss\", batch_bbox_loss.item()/(i+1), global_step)\n",
    "            writer.add_scalar(\"Loss/train/refined_bbox_loss\", batch_refined_bbox_loss.item()/(i+1), global_step)\n",
    "            writer.add_scalar(\"Loss/train/obj_bbox_loss\", batch_obj_bbox_loss.item()/(i+1), global_step)\n",
    "            writer.add_scalar(\"Loss/train/node_loss\", batch_node_loss.item()/(i+1), global_step)\n",
    "            \n",
    "            \n",
    "#     scheduler.step()\n",
    "    global_step = epoch*len(batch_train_loader)+i\n",
    "    image_shape = [num_nodes, bbx_size]\n",
    "\n",
    "    image = plot_utils.plot_bbx(np.reshape((node_data_true[:num_nodes,1:5]*label_true[:num_nodes]).detach().to(\"cpu\").numpy(),\n",
    "                                image_shape)).astype(float)/255\n",
    "    writer.add_image('train/images/input', image, global_step, dataformats='HWC')\n",
    "    image = plot_utils.plot_bbx((node_data_pred[0]*label_true[:num_nodes]).detach().to(\"cpu\").numpy()).astype(float)/255\n",
    "    writer.add_image('train/images/generated', image, global_step, dataformats='HWC')\n",
    "    image = plot_utils.plot_bbx((node_data_pred_refined[0]*label_true[:num_nodes]).detach().to(\"cpu\").numpy()).astype(float)/255\n",
    "    writer.add_image('train/images/refined', image, global_step, dataformats='HWC')\n",
    "    \n",
    "    reconstruction_loss_arr.append(batch_loss.detach().item()/(i+1))\n",
    "    kl_loss_obj_arr.append(batch_kl_loss_obj.detach().item()/(i+1))\n",
    "    kl_loss_part_arr.append(batch_kl_loss_part.detach().item()/(i+1))\n",
    "    bbox_loss_arr.append(batch_bbox_loss.detach().item()/(i+1))\n",
    "    refined_bbox_loss_arr.append(batch_refined_bbox_loss.detach().item()/(i+1))\n",
    "    node_loss_arr.append(batch_node_loss.detach().item()/(i+1))\n",
    "    \n",
    "    print('[%d, %5d] loss: %.3f' %\n",
    "              (epoch + 1, i + 1, batch_loss/(i+1) ))\n",
    "    \n",
    "    \n",
    "    batch_loss = torch.tensor([0.0])\n",
    "    batch_kl_loss_part = torch.tensor([0.0])\n",
    "    batch_kl_loss_obj = torch.tensor([0.0])\n",
    "    batch_bbox_loss = torch.tensor([0.0])\n",
    "    batch_refined_bbox_loss = torch.tensor([0.0])\n",
    "    batch_obj_bbox_loss = torch.tensor([0.0])\n",
    "    batch_node_loss = torch.tensor([0.0])\n",
    "    images = []\n",
    "    vae.eval()\n",
    "    for i, val_data in enumerate(batch_val_loader, 0):\n",
    "        \n",
    "        node_data_true = val_data.x\n",
    "        label_true = node_data_true[:,:1]\n",
    "        y_true = val_data.y\n",
    "        class_true = torch.flatten(y_true[:, :num_classes])\n",
    "        X_obj_true = y_true[:, num_classes:]\n",
    "        X_obj_true_transformed = X_obj_true[:,2:]-X_obj_true[:,:2]\n",
    "        adj_true = val_data.edge_index\n",
    "        batch = val_data.batch\n",
    "        \n",
    "        class_true  = torch.flatten(class_true)\n",
    "        \n",
    "        output = vae(adj_true, node_data_true, X_obj_true_transformed, label_true , class_true, variational, coupling, refine_iter=2)\n",
    "        \n",
    "        node_data_pred = output[0]\n",
    "        X_obj_pred = output[1]\n",
    "        X_obj_pred = torch.cat(((torch.tensor([1.0])-X_obj_pred)*torch.tensor([0.5]),\n",
    "                                (torch.tensor([1.0])+X_obj_pred)*torch.tensor([0.5])),\n",
    "                              -1)\n",
    "        label_pred = output[2]\n",
    "        z_mean_part = output[3]\n",
    "        z_logvar_part = output[4]\n",
    "        margin = output[5]\n",
    "        z_mean_obj = output[6]\n",
    "        z_logvar_obj = output[7]\n",
    "        node_data_pred_refined = output[8]\n",
    "        \n",
    "        \n",
    "        obj_bbox_loss = loss.obj_bbox_loss(pred_box=X_obj_pred, true_box=X_obj_true, weight=IOU_weight_delta, has_mse=False)\n",
    "        kl_loss_obj = loss.kl_loss(z_mean_obj, z_logvar_obj)\n",
    "        kl_loss_part = loss.kl_loss(z_mean_part, z_logvar_part)\n",
    "        bbox_loss = loss.weighted_bbox_loss(pred_box=node_data_pred, true_box=node_data_true[:,1:], weight=IOU_weight_delta, margin=margin)\n",
    "        bbox_loss_coarse = loss.coarse_bbx_loss(pred_box=node_data_pred, true_box=node_data_true[:,1:])\n",
    "        refined_bbox_loss = loss.weighted_bbox_loss(pred_box=node_data_pred_refined, true_box=node_data_true[:,1:], weight=IOU_weight_delta, margin=margin)\n",
    "        node_loss = loss.node_loss(label_pred,label_true)\n",
    "        \n",
    "        if kl_weight>0:\n",
    "            reconstruction_loss = kl_loss_part*kl_weight + (bbox_loss + node_loss)*num_nodes\n",
    "        else:\n",
    "            reconstruction_loss = (refined_bbox_loss + node_loss)*num_nodes\n",
    "\n",
    "        if epoch > 50:\n",
    "            reconstruction_loss += bbox_loss_coarse*10\n",
    "            \n",
    "        batch_loss += reconstruction_loss\n",
    "        batch_kl_loss_part += kl_loss_part\n",
    "        batch_kl_loss_obj += kl_loss_obj\n",
    "        batch_bbox_loss += bbox_loss\n",
    "        batch_refined_bbox_loss += refined_bbox_loss\n",
    "        batch_obj_bbox_loss += obj_bbox_loss\n",
    "        batch_node_loss += node_loss\n",
    "    \n",
    "    image = plot_utils.plot_bbx(np.reshape((node_data_true[:num_nodes,1:5]*label_true[:num_nodes]).detach().to(\"cpu\").numpy(),\n",
    "                                image_shape)).astype(float)/255\n",
    "    writer.add_image('val/images/input', image, global_step, dataformats='HWC')\n",
    "    image = plot_utils.plot_bbx((node_data_pred[0]*label_true[:num_nodes]).detach().to(\"cpu\").numpy()).astype(float)/255\n",
    "    writer.add_image('val/images/generated', image, global_step, dataformats='HWC')\n",
    "    image = plot_utils.plot_bbx((node_data_pred_refined[0]*label_true[:num_nodes]).detach().to(\"cpu\").numpy()).astype(float)/255\n",
    "    writer.add_image('val/images/refined', image, global_step, dataformats='HWC')\n",
    "\n",
    "    reconstruction_loss_val_arr.append(batch_loss.detach().item()/(i+1))\n",
    "    bbox_loss_val_arr.append(batch_bbox_loss.detach().item()/(i+1))\n",
    "    node_loss_val_arr.append(batch_node_loss.detach().item()/(i+1))\n",
    "    \n",
    "    writer.add_scalar(\"Loss/val/reconstruction_loss\", batch_loss.detach()/(i+1), global_step)\n",
    "    writer.add_scalar(\"Loss/train/kl_loss_part\", batch_kl_loss_part.item()/(i+1), global_step)\n",
    "    writer.add_scalar(\"Loss/train/kl_loss_obj\", batch_kl_loss_obj.item()/(i+1), global_step)\n",
    "    writer.add_scalar(\"Loss/val/bbox_loss\", batch_bbox_loss.item()/(i+1), global_step)\n",
    "    writer.add_scalar(\"Loss/val/refined_bbox_loss\", batch_refined_bbox_loss.item()/(i+1), global_step)\n",
    "    writer.add_scalar(\"Loss/val/obj_bbox_loss\", batch_obj_bbox_loss.item()/(i+1), global_step)\n",
    "    writer.add_scalar(\"Loss/val/node_loss\", batch_node_loss.item()/(i+1), global_step)\n",
    "       \n",
    "    if epoch%50 == 0:\n",
    "        torch.save(vae.state_dict(), model_path + '/model_weights.pth')\n",
    "        \n",
    "    if ((kl_loss_part_arr[-1]>0.5) and \n",
    "        (abs(bbox_loss_arr[-1] - bbox_loss_val_arr[-1]) < 0.07) and \n",
    "        (bbox_loss_arr[-1]<bbox_loss_threshold) and (epoch>300)):\n",
    "        \n",
    "        icoef = icoef + 1\n",
    "        bbox_loss_threshold*=0.9\n",
    "\n",
    "torch.save(vae.state_dict(),model_path + '/model_weights.pth')\n",
    "\n",
    "for i in range(min(100,int(len(node_data_true)/num_nodes))):    \n",
    "    image = plot_utils.plot_bbx(np.reshape((node_data_true[num_nodes*i:num_nodes*(i+1),1:5]).detach().to(\"cpu\").numpy(),\n",
    "                                    image_shape)).astype(float)/255\n",
    "    writer.add_image('result/images/'+str(i)+'-input', image, global_step, dataformats='HWC')\n",
    "    image = plot_utils.plot_bbx((node_data_pred[i]*label_true[num_nodes*i:num_nodes*(i+1)]).detach().to(\"cpu\").numpy()).astype(float)/255\n",
    "    writer.add_image('result/images/'+str(i)+'-generated', image, global_step, dataformats='HWC')\n",
    "    \n",
    "writer.flush()\n",
    "writer.close()\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a642a867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "173005"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in vae.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b72cd01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ww/3nytfk1917b8nz7dchtckxnm0000gn/T/ipykernel_76803/3945016058.py:39: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  vae.load_state_dict(torch.load(model_path+ '/model_weights.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.13461539 0.31923077 0.45      ]\n",
      " [0.20384616 0.13846155 0.32692307 0.22307692]\n",
      " [0.01538462 0.13846155 0.15384616 0.25      ]\n",
      " [0.01923077 0.26153848 1.         0.86538464]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.31206554 0.4116043  0.4263142  0.4792027 ]\n",
      " [0.37431136 0.40921012 0.4391694  0.4486777 ]\n",
      " [0.32719073 0.3953026  0.36708042 0.43652308]\n",
      " [0.33984467 0.43129802 0.68453526 0.6031955 ]\n",
      " [0.30927113 0.36338723 0.3215127  0.37703204]\n",
      " [0.30927002 0.36338338 0.32149655 0.3770109 ]\n",
      " [0.30927002 0.36338338 0.32149652 0.3770109 ]\n",
      " [0.30927002 0.36338338 0.32149652 0.37701088]\n",
      " [0.30927002 0.36338338 0.32149652 0.37701088]\n",
      " [0.30927002 0.36338338 0.32149652 0.37701088]\n",
      " [0.30927002 0.36338338 0.32149652 0.37701088]\n",
      " [0.30927002 0.36338338 0.32149652 0.37701088]\n",
      " [0.30927002 0.36338338 0.32149652 0.37701088]\n",
      " [0.30927002 0.36338338 0.32149652 0.37701088]\n",
      " [0.30927002 0.36338338 0.32149652 0.37701088]\n",
      " [0.30927002 0.36338338 0.32149652 0.37701088]]\n",
      "[[0.29642856 0.         0.62857145 0.47857141]\n",
      " [0.04285714 0.15       0.6892857  0.6571429 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.66785717 0.64285713 0.8535714  0.8642857 ]\n",
      " [0.6035714  0.2857143  0.78571427 0.73214287]\n",
      " [0.7607143  0.8        0.9607143  1.        ]\n",
      " [0.06785715 0.71428573 0.31071427 0.9035714 ]\n",
      " [0.06428572 0.2607143  0.23571429 0.75      ]\n",
      " [0.175      0.825      0.38214287 1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.34642857 0.58214283 0.6357143  1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.03928572 0.5107143  0.3857143  0.9071429 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.4434501  0.29443085 0.6010555  0.446874  ]\n",
      " [0.37369898 0.3680644  0.53794974 0.5779486 ]\n",
      " [0.31074366 0.29870248 0.3229592  0.31892666]\n",
      " [0.5014219  0.54679394 0.6229995  0.639184  ]\n",
      " [0.5243937  0.47345546 0.584726   0.5862812 ]\n",
      " [0.5978935  0.63837934 0.6615263  0.69836974]\n",
      " [0.35082853 0.6108815  0.42951968 0.70085   ]\n",
      " [0.33360454 0.41112116 0.40057296 0.5773158 ]\n",
      " [0.35651076 0.67863476 0.41059756 0.72277164]\n",
      " [0.3107372  0.2986705  0.32287028 0.31875423]\n",
      " [0.39279094 0.5199563  0.51046014 0.65153915]\n",
      " [0.31073716 0.29867026 0.3228696  0.31875294]\n",
      " [0.3107373  0.29867104 0.32287174 0.31875706]\n",
      " [0.32537937 0.57327354 0.38945657 0.69870996]\n",
      " [0.31073716 0.29867032 0.32286975 0.3187532 ]\n",
      " [0.31073716 0.29867026 0.3228696  0.31875294]]\n",
      "[[0.7673469  0.07755102 1.         0.24489796]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.24489796 0.18367347 0.8979592  0.6163265 ]\n",
      " [0.66938776 0.17959183 0.90204084 0.42857143]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.6857143  0.82857144 0.74285716 0.9265306 ]\n",
      " [0.75918365 0.5755102  0.8122449  0.722449  ]\n",
      " [0.67346936 0.71428573 0.8        0.82857144]\n",
      " [0.34285715 0.66530615 0.4122449  0.75102043]\n",
      " [0.37959182 0.74285716 0.53061223 0.90204084]\n",
      " [0.24081632 0.5877551  0.46530613 0.71020406]\n",
      " [0.24081632 0.6857143  0.41632652 0.8244898 ]\n",
      " [0.         0.35918367 0.25306123 0.64081633]\n",
      " [0.48163265 0.8734694  0.5346939  0.90204084]\n",
      " [0.33877552 0.77959186 0.41632652 0.8244898 ]] [[0.61384463 0.36478347 0.67153656 0.4536031 ]\n",
      " [0.32488903 0.35429573 0.3361766  0.36891463]\n",
      " [0.32488263 0.354278   0.33611625 0.36882457]\n",
      " [0.42241043 0.39649108 0.6444521  0.54334927]\n",
      " [0.5804938  0.3939339  0.63458437 0.47060686]\n",
      " [0.32488322 0.3542801  0.3361241  0.3688362 ]\n",
      " [0.5580203  0.5753503  0.59429216 0.6096849 ]\n",
      " [0.5657446  0.5102245  0.6044931  0.56853306]\n",
      " [0.56022197 0.56262064 0.59220076 0.6003922 ]\n",
      " [0.42523372 0.5286712  0.48769298 0.57481086]\n",
      " [0.4232659  0.57725257 0.480614   0.6319166 ]\n",
      " [0.43484494 0.4757763  0.47820222 0.53798676]\n",
      " [0.40322024 0.54283947 0.45212266 0.6037693 ]\n",
      " [0.3346635  0.44292855 0.43659258 0.5688143 ]\n",
      " [0.46482283 0.5881417  0.490604   0.61716837]\n",
      " [0.4417233  0.5853934  0.45787257 0.5935869 ]]\n",
      "[[0.06647399 0.         0.7196532  0.60982656]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.39595374 0.14450867 0.9306358  1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.28255647 0.24008825 0.6119988  0.543771  ]\n",
      " [0.26941237 0.2384558  0.28419417 0.26454538]\n",
      " [0.2694124  0.2384559  0.28419444 0.26454595]\n",
      " [0.45936668 0.3145905  0.71849704 0.77049905]\n",
      " [0.2694124  0.23845594 0.28419453 0.26454613]\n",
      " [0.26941237 0.2384557  0.28419387 0.26454476]\n",
      " [0.26941237 0.2384557  0.28419387 0.26454476]\n",
      " [0.26941237 0.2384557  0.28419387 0.26454476]\n",
      " [0.26941237 0.2384557  0.28419387 0.26454476]\n",
      " [0.26941237 0.2384557  0.28419387 0.26454476]\n",
      " [0.26941237 0.2384557  0.28419387 0.26454476]\n",
      " [0.26941237 0.2384557  0.28419387 0.26454476]\n",
      " [0.26941237 0.2384557  0.28419387 0.26454476]\n",
      " [0.26941237 0.2384557  0.28419387 0.26454476]\n",
      " [0.26941237 0.2384557  0.28419387 0.26454476]\n",
      " [0.26941237 0.2384557  0.28419387 0.26454476]]\n",
      "[[0.6935484  0.         0.9354839  0.21774194]\n",
      " [0.11290322 0.05645161 0.83870965 0.7983871 ]\n",
      " [0.5322581  0.06451613 0.83870965 0.32258064]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.5080645  0.6451613  0.7016129  0.7580645 ]\n",
      " [0.516129   0.67741936 0.75       0.75      ]\n",
      " [0.4032258  0.62096775 0.6854839  0.83064514]\n",
      " [0.4032258  0.7419355  0.6854839  0.83064514]\n",
      " [0.05645161 0.7096774  0.33064517 1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.5285595  0.4081937  0.5780088  0.44859987]\n",
      " [0.43130434 0.42391095 0.56117624 0.5561807 ]\n",
      " [0.51547945 0.42617628 0.5588112  0.4651822 ]\n",
      " [0.4179899  0.4085535  0.42324707 0.41767535]\n",
      " [0.4179899  0.4085535  0.42324707 0.41767535]\n",
      " [0.49933    0.5286969  0.53150105 0.54944193]\n",
      " [0.5090836  0.5265379  0.53127134 0.54198533]\n",
      " [0.48603612 0.528181   0.5257033  0.5698125 ]\n",
      " [0.49251035 0.55060524 0.51798034 0.56770045]\n",
      " [0.42595422 0.5500306  0.47078794 0.59405476]\n",
      " [0.4179899  0.40855348 0.42324704 0.4176753 ]\n",
      " [0.4179899  0.4085535  0.42324707 0.41767535]\n",
      " [0.4179899  0.40855348 0.42324704 0.4176753 ]\n",
      " [0.4179899  0.40855348 0.42324704 0.4176753 ]\n",
      " [0.4179899  0.40855348 0.42324704 0.4176753 ]\n",
      " [0.4179899  0.40855348 0.42324704 0.4176753 ]]\n",
      "[[0.25012094 0.         0.75483793 0.6698113 ]\n",
      " [0.29729077 0.6509434  0.6888002  1.        ]\n",
      " [0.49068698 0.5471698  0.7123851  0.8349057 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.4597997  0.34599018 0.53754836 0.55387986]\n",
      " [0.47504747 0.5490426  0.5342668  0.6579633 ]\n",
      " [0.49514169 0.51989985 0.53423256 0.59915864]\n",
      " [0.42150727 0.3475646  0.42653894 0.36277005]\n",
      " [0.4215073  0.34756464 0.42653903 0.36277032]\n",
      " [0.42150727 0.3475646  0.42653894 0.36277005]\n",
      " [0.42150727 0.3475646  0.42653894 0.36277005]\n",
      " [0.4215073  0.3475646  0.42653897 0.36277008]\n",
      " [0.42150727 0.3475646  0.42653894 0.36277005]\n",
      " [0.42150727 0.3475646  0.42653894 0.36277005]\n",
      " [0.42150727 0.3475646  0.42653894 0.36277005]\n",
      " [0.42150727 0.3475646  0.42653894 0.36277005]\n",
      " [0.42150727 0.3475646  0.42653894 0.36277005]\n",
      " [0.42150727 0.3475646  0.42653894 0.36277005]\n",
      " [0.42150727 0.3475646  0.42653894 0.36277005]\n",
      " [0.42150727 0.3475646  0.42653894 0.36277005]]\n",
      "[[0.7020408  0.16530612 1.         0.4122449 ]\n",
      " [0.22857143 0.2734694  0.8040816  0.59387755]\n",
      " [0.67755103 0.3        0.78775513 0.4489796 ]\n",
      " [0.41020408 0.45918366 0.71632653 0.7755102 ]\n",
      " [0.422449   0.5857143  0.5244898  0.7755102 ]\n",
      " [0.4877551  0.3632653  0.6959184  0.74897957]\n",
      " [0.48367348 0.6102041  0.59795916 0.75306123]\n",
      " [0.02244898 0.5469388  0.32040817 0.83469385]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.01632653 0.40816328 0.29591838 0.7632653 ]\n",
      " [0.02653061 0.62857145 0.12857144 0.75714284]\n",
      " [0.         0.34081632 0.18775511 0.5755102 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.61624277 0.3288256  0.8482244  0.47997895]\n",
      " [0.2626904  0.34997284 0.7291833  0.5239804 ]\n",
      " [0.64891183 0.3742349  0.7501674  0.4773354 ]\n",
      " [0.4235157  0.49054414 0.61107075 0.63076437]\n",
      " [0.42398039 0.57074416 0.51008487 0.60906124]\n",
      " [0.47519636 0.41921413 0.6358576  0.63126814]\n",
      " [0.51214975 0.55781364 0.57886577 0.61300313]\n",
      " [0.21250045 0.50037086 0.33923554 0.64357066]\n",
      " [0.12217322 0.25229952 0.1471229  0.27778828]\n",
      " [0.15927903 0.46908563 0.29432186 0.6423594 ]\n",
      " [0.17363575 0.603134   0.23034245 0.6480093 ]\n",
      " [0.12568754 0.3458407  0.24029437 0.48457518]\n",
      " [0.12211395 0.2520623  0.14633888 0.27679524]\n",
      " [0.12211388 0.25206205 0.14633778 0.2767939 ]\n",
      " [0.12211388 0.25206205 0.14633778 0.2767939 ]\n",
      " [0.12211388 0.25206205 0.14633778 0.2767939 ]]\n",
      "[[0.22532189 0.         0.8476395  0.64377683]\n",
      " [0.09871244 0.24678111 0.8690987  0.88412017]\n",
      " [0.21030043 0.38412017 0.84549356 0.8133047 ]\n",
      " [0.74463516 0.6523605  0.8991416  0.7811159 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.09871244 0.6223176  0.39914164 1.        ]\n",
      " [0.10300429 0.77253217 0.3969957  1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.31279564 0.16747837 0.7060733  0.61865175]\n",
      " [0.2555323  0.30796877 0.697822   0.8118866 ]\n",
      " [0.37724733 0.42024204 0.64045787 0.67233926]\n",
      " [0.59477186 0.6162498  0.73150283 0.7659422 ]\n",
      " [0.21935983 0.15633824 0.23734996 0.19061866]\n",
      " [0.26308572 0.60755664 0.43032938 0.8447241 ]\n",
      " [0.29088646 0.70484984 0.39016312 0.8124793 ]\n",
      " [0.21935982 0.15633823 0.23734991 0.19061854]\n",
      " [0.21935982 0.15633821 0.2373499  0.19061852]\n",
      " [0.21935982 0.15633821 0.2373499  0.19061852]\n",
      " [0.21935982 0.15633821 0.2373499  0.19061852]\n",
      " [0.21935982 0.15633823 0.2373499  0.19061853]\n",
      " [0.21935986 0.15633847 0.23735051 0.19061989]\n",
      " [0.21935982 0.15633821 0.2373499  0.19061852]\n",
      " [0.21935982 0.15633821 0.2373499  0.19061852]\n",
      " [0.21935982 0.15633821 0.2373499  0.19061852]]\n",
      "[[0.         0.19518448 0.13170731 0.446404  ]\n",
      " [0.06829268 0.75128204 0.11951219 0.78055036]\n",
      " [0.3219512  0.6903064  0.36097562 0.73176986]\n",
      " [0.07073171 0.20250157 0.81463414 0.5781113 ]\n",
      " [0.10243902 0.19030644 0.31707317 0.4073796 ]\n",
      " [0.19512194 0.54152596 0.2902439  0.6268918 ]\n",
      " [0.07560976 0.6049406  0.22195122 0.7927455 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.2512195  0.61225766 0.35609755 0.72933084]\n",
      " [0.67317075 0.5244528  0.7487805  0.6415259 ]\n",
      " [0.66341466 0.62445277 0.7585366  0.81225765]\n",
      " [0.6        0.48298937 0.6707317  0.63420886]\n",
      " [0.48292682 0.61225766 0.66097564 0.76347715]\n",
      " [0.77804875 0.30494058 1.         0.56347716]\n",
      " [0.66097564 0.7829894  0.7121951  0.81225765]\n",
      " [0.47560975 0.72933084 0.52682924 0.76591617]] [[0.20042631 0.43105707 0.28514355 0.49999076]\n",
      " [0.27184257 0.6276389  0.29466137 0.61329514]\n",
      " [0.3535722  0.59952104 0.36715758 0.6038685 ]\n",
      " [0.25342345 0.39006037 0.72451246 0.5243419 ]\n",
      " [0.2480249  0.36761644 0.3551309  0.4451815 ]\n",
      " [0.27919352 0.4737016  0.34204435 0.53923565]\n",
      " [0.26487687 0.5575609  0.30878586 0.5970576 ]\n",
      " [0.2639109  0.39895523 0.32048672 0.43840742]\n",
      " [0.35492086 0.5328498  0.40816772 0.57321537]\n",
      " [0.56008315 0.4596015  0.66719043 0.54672444]\n",
      " [0.5937449  0.5464027  0.66046476 0.60453844]\n",
      " [0.48142007 0.43347254 0.5293963  0.49221283]\n",
      " [0.5278698  0.53305054 0.58466035 0.59537464]\n",
      " [0.73509413 0.4071622  0.82635164 0.5445598 ]\n",
      " [0.5046228  0.55470145 0.535254   0.5680727 ]\n",
      " [0.5296788  0.60474    0.5445033  0.6147032 ]]\n",
      "[[0.2682927  0.0779237  0.6926829  0.3657286 ]\n",
      " [0.12195122 0.2779237  0.77560973 0.7315822 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.70731705 0.51207006 0.99512196 0.66328955]\n",
      " [0.8        0.5657286  1.         0.6681676 ]\n",
      " [0.3512195  0.62914324 0.5463415  0.9169481 ]\n",
      " [0.43414634 0.809631   0.55609757 0.91207004]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.07804878 0.5657286  0.26341462 0.809631  ]\n",
      " [0.07804878 0.6925579  0.16585366 0.79499686]\n",
      " [0.         0.5657286  0.13658537 0.707192  ]\n",
      " [0.36097562 0.15109444 0.6097561  0.33158225]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.425973   0.38453764 0.5821254  0.48740348]\n",
      " [0.40217167 0.46415335 0.5894022  0.5554102 ]\n",
      " [0.34961697 0.3763269  0.35926005 0.38866726]\n",
      " [0.5897747  0.48296964 0.652393   0.5482598 ]\n",
      " [0.5960969  0.5102612  0.6466024  0.5523836 ]\n",
      " [0.42364824 0.5339379  0.51846147 0.6108172 ]\n",
      " [0.4785006  0.58976257 0.53114414 0.6176058 ]\n",
      " [0.34961942 0.37633714 0.35929438 0.3887187 ]\n",
      " [0.34961677 0.37632602 0.3592569  0.38866255]\n",
      " [0.36682138 0.5340955  0.42796087 0.5804691 ]\n",
      " [0.3742317  0.5381628  0.39727437 0.56058455]\n",
      " [0.3551271  0.54851115 0.4021265  0.56602776]\n",
      " [0.4566313  0.43439782 0.5232153  0.48040012]\n",
      " [0.34961677 0.37632602 0.3592569  0.38866252]\n",
      " [0.34961677 0.37632602 0.3592569  0.38866252]\n",
      " [0.34961677 0.37632602 0.3592569  0.38866252]]\n",
      "[[0.7133333  0.29333332 1.         0.56      ]\n",
      " [0.12       0.22333333 0.87       0.6666667 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.70666665 0.62       0.83666664 0.75      ]\n",
      " [0.70666665 0.69666666 0.77666664 0.75333333]\n",
      " [0.48666668 0.63       0.5966667  0.7733333 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.58       0.67333335 0.72       0.77666664]\n",
      " [0.6166667  0.73333335 0.72333336 0.77666664]\n",
      " [0.35666665 0.56       0.52       0.75666666]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.22333333 0.17       0.51666665]\n",
      " [0.86       0.35       0.97       0.47666666]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.6265989  0.4291262  0.7294308  0.5208667 ]\n",
      " [0.32457966 0.43282372 0.6835488  0.5542078 ]\n",
      " [0.26299834 0.37145898 0.2781924  0.38428217]\n",
      " [0.6043583  0.53615713 0.66835654 0.5756806 ]\n",
      " [0.58958757 0.5615834  0.65009713 0.5762287 ]\n",
      " [0.5055934  0.5267165  0.57429826 0.5616126 ]\n",
      " [0.26300123 0.3714658  0.2782296  0.3843186 ]\n",
      " [0.5227078  0.5437813  0.63495123 0.5976385 ]\n",
      " [0.59961766 0.5750552  0.6448884  0.5915818 ]\n",
      " [0.38976106 0.51801527 0.5498955  0.58702075]\n",
      " [0.26337463 0.37308875 0.28037512 0.38707998]\n",
      " [0.27726892 0.41260135 0.37853882 0.47051412]\n",
      " [0.6506859  0.479797   0.71723974 0.51194954]\n",
      " [0.26299825 0.3714587  0.27819094 0.3842807 ]\n",
      " [0.26299825 0.3714587  0.27819094 0.3842807 ]\n",
      " [0.26299825 0.3714587  0.27819094 0.3842807 ]]\n",
      "[[0.03940887 0.07389162 0.27093595 0.43842363]\n",
      " [0.29802954 0.6182266  0.36453202 0.6970443 ]\n",
      " [0.         0.71428573 0.07142857 0.7832512 ]\n",
      " [0.16502462 0.03940887 0.9236453  0.6059113 ]\n",
      " [0.14778325 0.02955665 0.4729064  0.35714287]\n",
      " [0.31527093 0.4359606  0.49014777 0.5714286 ]\n",
      " [0.2783251  0.4507389  0.36453202 0.6921182 ]\n",
      " [0.14778325 0.47536945 0.30295566 0.57389164]\n",
      " [0.00246305 0.5344828  0.1724138  0.77093595]\n",
      " [0.75615764 0.5369458  0.8817734  0.7758621 ]\n",
      " [0.6625616  0.74876845 0.8399015  0.9655172 ]\n",
      " [0.70689654 0.56157637 0.90640396 0.77832514]\n",
      " [0.84729064 0.7241379  0.9359606  0.9359606 ]\n",
      " [0.8743842  0.33990148 1.         0.5147783 ]\n",
      " [0.6551724  0.9162561  0.726601   0.97044337]\n",
      " [0.         0.         0.         0.        ]] [[0.19427344 0.24304518 0.31151634 0.49135414]\n",
      " [0.3693077  0.67618424 0.4160255  0.69610745]\n",
      " [0.2289468  0.5570958  0.24733654 0.5751939 ]\n",
      " [0.27876636 0.22366871 0.770125   0.56234574]\n",
      " [0.28656548 0.22764176 0.4722776  0.40803713]\n",
      " [0.36949188 0.45628735 0.45622286 0.5707611 ]\n",
      " [0.3379368  0.5004715  0.3869971  0.6507079 ]\n",
      " [0.3014689  0.4898344  0.3538643  0.53668046]\n",
      " [0.23725247 0.46717614 0.27795023 0.61998165]\n",
      " [0.6365849  0.5049388  0.7386413  0.661567  ]\n",
      " [0.6327975  0.6441846  0.71742666 0.7715593 ]\n",
      " [0.68517095 0.53510356 0.7472515  0.6809075 ]\n",
      " [0.6742513  0.6749071  0.7351163  0.77484924]\n",
      " [0.72123975 0.39783943 0.81400675 0.5511619 ]\n",
      " [0.5880829  0.7391828  0.6297517  0.7736965 ]\n",
      " [0.18690664 0.2112861  0.20710026 0.2402702 ]]\n",
      "[[0.09190372 0.         0.76367617 0.66739607]\n",
      " [0.09846827 0.25382933 0.892779   0.9256017 ]\n",
      " [0.13785557 0.39824945 0.78993434 0.80306345]\n",
      " [0.595186   0.6083151  0.90153176 1.        ]\n",
      " [0.6083151  0.7658643  0.9146608  0.99124724]\n",
      " [0.08533917 0.6827133  0.25164112 0.8183808 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.25140688 0.17380308 0.65051603 0.6367252 ]\n",
      " [0.25296408 0.32363623 0.72025496 0.81310946]\n",
      " [0.31361765 0.3981024  0.5806853  0.6744682 ]\n",
      " [0.56662893 0.5802368  0.7435476  0.8232396 ]\n",
      " [0.5432973  0.72686464 0.6874945  0.834138  ]\n",
      " [0.26554096 0.5712033  0.39468354 0.8048747 ]\n",
      " [0.21484557 0.16297583 0.23312591 0.19659573]\n",
      " [0.21484551 0.16297548 0.233125   0.19659378]\n",
      " [0.21484551 0.16297546 0.23312496 0.19659369]\n",
      " [0.21484551 0.16297546 0.23312496 0.19659369]\n",
      " [0.21484551 0.16297546 0.23312496 0.19659369]\n",
      " [0.21484551 0.16297546 0.23312497 0.1965937 ]\n",
      " [0.21484552 0.1629755  0.23312509 0.19659397]\n",
      " [0.21484551 0.16297546 0.23312496 0.19659369]\n",
      " [0.21484551 0.16297546 0.23312496 0.19659369]\n",
      " [0.21484551 0.16297546 0.23312496 0.19659369]]\n",
      "[[0.42528737 0.         0.743295   0.29118773]\n",
      " [0.256705   0.19157088 0.66283524 0.9310345 ]\n",
      " [0.42145595 0.14942528 0.54789275 0.29118773]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.34099618 0.5900383  0.532567   0.9118774 ]\n",
      " [0.29118773 0.2605364  0.49042144 0.66283524]\n",
      " [0.4559387  0.87356323 0.54789275 1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.27586207 0.8888889  0.47509578 1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.4896528  0.29331908 0.56086063 0.43873495]\n",
      " [0.45825067 0.34806204 0.5459043  0.6288391 ]\n",
      " [0.4973883  0.34640366 0.52832025 0.39473718]\n",
      " [0.39966962 0.29789934 0.4061059  0.3180744 ]\n",
      " [0.3996693  0.29789612 0.40610117 0.318057  ]\n",
      " [0.39967006 0.29790375 0.40611246 0.31809834]\n",
      " [0.46248227 0.57597446 0.507224   0.6758557 ]\n",
      " [0.45495608 0.40997416 0.49412346 0.545845  ]\n",
      " [0.48663464 0.61892146 0.51130533 0.6705198 ]\n",
      " [0.39966926 0.2978959  0.40610084 0.31805584]\n",
      " [0.39966938 0.29789695 0.4061024  0.3180615 ]\n",
      " [0.39966926 0.2978959  0.40610084 0.3180558 ]\n",
      " [0.3996693  0.297896   0.406101   0.31805637]\n",
      " [0.472995   0.60470027 0.51828516 0.7161951 ]\n",
      " [0.3996693  0.29789594 0.4061009  0.31805596]\n",
      " [0.39966926 0.2978959  0.40610084 0.3180558 ]]\n",
      "[[0.72894734 0.21315789 1.         0.35789475]\n",
      " [0.40263158 0.29473683 0.8736842  0.6       ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.34473684 0.5        0.4236842  0.54473686]\n",
      " [0.35263157 0.41315788 0.58157897 0.49473685]\n",
      " [0.48421052 0.55263156 0.6078947  0.70789474]\n",
      " [0.54473686 0.6657895  0.6105263  0.70526314]\n",
      " [0.5947368  0.58157897 0.8        0.7868421 ]\n",
      " [0.66842103 0.72105265 0.80263156 0.7868421 ]\n",
      " [0.         0.28947368 0.4236842  0.46842104]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.67431915 0.4024542  0.78697723 0.4578351 ]\n",
      " [0.3983727  0.4448661  0.6952931  0.51407045]\n",
      " [0.21409354 0.33923233 0.23242691 0.35527414]\n",
      " [0.3715428  0.48581702 0.43792337 0.48316777]\n",
      " [0.36393887 0.45534834 0.57358766 0.5224258 ]\n",
      " [0.5284244  0.5233955  0.6289038  0.57405734]\n",
      " [0.5235046  0.55477357 0.5995045  0.5782605 ]\n",
      " [0.5298277  0.5314484  0.62439704 0.59628165]\n",
      " [0.58273077 0.57348293 0.6388794  0.597315  ]\n",
      " [0.23806405 0.43828613 0.43049848 0.49643326]\n",
      " [0.21409312 0.33923116 0.2324208  0.35526788]\n",
      " [0.21409315 0.33923125 0.23242117 0.35526827]\n",
      " [0.21409312 0.33923116 0.2324208  0.35526788]\n",
      " [0.21409312 0.33923116 0.2324208  0.35526788]\n",
      " [0.21409312 0.33923116 0.2324208  0.35526788]\n",
      " [0.21409312 0.33923116 0.2324208  0.35526788]]\n",
      "[[0.2948718  0.         0.60084194 0.28731343]\n",
      " [0.30606583 0.21641791 0.7090509  0.9365672 ]\n",
      " [0.46278223 0.17537314 0.5971106  0.29850745]\n",
      " [0.36949867 0.60820895 0.60084194 0.8992537 ]\n",
      " [0.48143896 0.2835821  0.6754688  0.6828358 ]\n",
      " [0.33964792 0.8619403  0.44412553 0.98507464]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.41054344 0.9067164  0.6157673  1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.47448608 0.30322176 0.5131997  0.40301865]\n",
      " [0.47847593 0.39326203 0.53218013 0.6688566 ]\n",
      " [0.49577263 0.39926994 0.5096904  0.4312782 ]\n",
      " [0.4811082  0.57326925 0.52235854 0.6471782 ]\n",
      " [0.50173944 0.43227136 0.5277679  0.5769778 ]\n",
      " [0.47565633 0.6486742  0.49767616 0.68424433]\n",
      " [0.41857308 0.3072998  0.42379418 0.32652682]\n",
      " [0.418573   0.30729896 0.4237931  0.32652214]\n",
      " [0.418573   0.30729896 0.4237931  0.32652217]\n",
      " [0.418573   0.3072987  0.42379275 0.32652065]\n",
      " [0.47885275 0.59742224 0.5261639  0.6996653 ]\n",
      " [0.418573   0.30729872 0.42379278 0.32652077]\n",
      " [0.418573   0.3072987  0.42379275 0.32652065]\n",
      " [0.418573   0.3072987  0.42379275 0.32652068]\n",
      " [0.418573   0.3072987  0.42379275 0.32652065]\n",
      " [0.418573   0.3072987  0.42379275 0.32652065]]\n",
      "[[0.42045453 0.34469697 0.7083333  0.6439394 ]\n",
      " [0.28787878 0.1969697  0.6780303  0.7689394 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.56060606 0.7613636  0.6136364  0.95075756]\n",
      " [0.5681818  0.86742425 0.6136364  0.95075756]\n",
      " [0.42045453 0.75       0.54545456 1.        ]\n",
      " [0.4848485  0.9166667  0.54545456 1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.32954547 0.         0.5530303  0.21212122]\n",
      " [0.49621212 0.5681818  0.6212121  0.6439394 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.48983157 0.42681816 0.53277963 0.5654365 ]\n",
      " [0.4664716  0.42132217 0.52271026 0.61331826]\n",
      " [0.42066085 0.3150424  0.42574713 0.33349323]\n",
      " [0.49980342 0.5763428  0.5235481  0.6805153 ]\n",
      " [0.49955213 0.6533935  0.51740426 0.6866317 ]\n",
      " [0.4871131  0.58604294 0.50819516 0.68589616]\n",
      " [0.49508834 0.66010743 0.5071715  0.68958557]\n",
      " [0.42066133 0.3150476  0.4257537  0.33352116]\n",
      " [0.42066082 0.31504214 0.42574677 0.33349168]\n",
      " [0.42066082 0.31504214 0.4257468  0.33349183]\n",
      " [0.42066082 0.31504214 0.42574677 0.33349174]\n",
      " [0.47262627 0.33784238 0.51189864 0.39499602]\n",
      " [0.50150585 0.49488735 0.52256995 0.54389423]\n",
      " [0.42066082 0.31504214 0.42574677 0.33349168]\n",
      " [0.42066082 0.31504214 0.42574677 0.33349168]\n",
      " [0.42066082 0.31504214 0.42574677 0.33349168]]\n",
      "[[0.36298934 0.         0.76868325 0.26334518]\n",
      " [0.24911033 0.18505338 0.8754448  0.71530247]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.46619216 0.6548043  0.65124553 1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.12811388 0.54448396 0.38078293 0.9679715 ]\n",
      " [0.12811388 0.78647685 0.2597865  0.97153026]\n",
      " [0.6441281  0.52669036 0.86121    0.7010676 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.50889677 0.10676157 0.6797153  0.23843417]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.4664936  0.3024679  0.5838246  0.41540614]\n",
      " [0.42568558 0.38220128 0.6200601  0.6187483 ]\n",
      " [0.3498991  0.3031331  0.35952353 0.32277578]\n",
      " [0.46319097 0.5573462  0.5422294  0.6843767 ]\n",
      " [0.34990585 0.30317727 0.35960707 0.32298064]\n",
      " [0.3795596  0.5221319  0.45667875 0.6947162 ]\n",
      " [0.38745457 0.65211403 0.43620178 0.7001807 ]\n",
      " [0.5497881  0.5153062  0.61056674 0.6231463 ]\n",
      " [0.34989893 0.303132   0.35952103 0.32276976]\n",
      " [0.349899   0.30313247 0.35952213 0.32277238]\n",
      " [0.34989893 0.30313197 0.35952097 0.32276958]\n",
      " [0.34990063 0.30314264 0.3595457  0.32282835]\n",
      " [0.4770102  0.3490122  0.5376382  0.39607772]\n",
      " [0.34989893 0.30313197 0.35952097 0.32276958]\n",
      " [0.34989893 0.30313197 0.35952097 0.32276958]\n",
      " [0.34989893 0.30313197 0.35952097 0.32276958]]\n",
      "[[0.02803738 0.04672897 0.57320875 0.6448598 ]\n",
      " [0.10903427 0.2647975  1.         0.95327103]\n",
      " [0.10903427 0.2647975  0.635514   0.7538941 ]\n",
      " [0.         0.6137072  0.49221185 0.95327103]\n",
      " [0.         0.6137072  0.14018692 0.79750776]\n",
      " [0.0529595  0.58878505 0.1152648  0.6199377 ]\n",
      " [0.0529595  0.58878505 0.1152648  0.6199377 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.28383052 0.32476744 0.5220938  0.5599261 ]\n",
      " [0.3231331  0.37764645 0.73622954 0.69183314]\n",
      " [0.34726787 0.45607448 0.55039656 0.612398  ]\n",
      " [0.29661992 0.5512195  0.49020377 0.6774703 ]\n",
      " [0.2830053  0.5578574  0.34047684 0.6336783 ]\n",
      " [0.27924237 0.5349764  0.34772858 0.5718923 ]\n",
      " [0.28535768 0.5399972  0.33545476 0.5876982 ]\n",
      " [0.26452205 0.29076168 0.27961957 0.31163687]\n",
      " [0.26452184 0.2907609  0.2796169  0.31163254]\n",
      " [0.26452184 0.29076087 0.27961686 0.3116325 ]\n",
      " [0.26452184 0.29076087 0.27961686 0.3116325 ]\n",
      " [0.26452187 0.2907609  0.27961698 0.3116327 ]\n",
      " [0.26452184 0.29076087 0.27961686 0.3116325 ]\n",
      " [0.26452184 0.29076087 0.27961686 0.3116325 ]\n",
      " [0.26452184 0.29076087 0.27961686 0.3116325 ]\n",
      " [0.26452184 0.29076087 0.27961686 0.3116325 ]]\n",
      "[[0.11448008 0.         0.45733723 0.44489795]\n",
      " [0.06958212 0.3510204  0.6777454  1.        ]\n",
      " [0.18386783 0.35510203 0.23692906 0.4244898 ]\n",
      " [0.6777454  0.8122449  0.926725   0.96734697]\n",
      " [0.55121475 0.51836735 0.82468414 0.922449  ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.36716557 0.3107484  0.47945976 0.47995317]\n",
      " [0.35233542 0.48180163 0.5562098  0.70249915]\n",
      " [0.37532026 0.42546043 0.421807   0.4880351 ]\n",
      " [0.5770876  0.61502147 0.64568174 0.6767304 ]\n",
      " [0.5218746  0.53551525 0.60952854 0.65481466]\n",
      " [0.33409926 0.31028828 0.34474117 0.32922512]\n",
      " [0.3340988  0.31028554 0.34473392 0.32921007]\n",
      " [0.33409876 0.31028545 0.34473366 0.32920954]\n",
      " [0.33409876 0.31028545 0.34473366 0.32920948]\n",
      " [0.33409876 0.31028545 0.34473366 0.32920948]\n",
      " [0.33409876 0.31028545 0.34473366 0.32920948]\n",
      " [0.33409876 0.31028545 0.34473366 0.32920948]\n",
      " [0.33409876 0.31028545 0.34473366 0.32920948]\n",
      " [0.33409876 0.31028545 0.34473366 0.32920948]\n",
      " [0.33409876 0.31028545 0.34473366 0.32920948]\n",
      " [0.33409876 0.31028545 0.34473366 0.32920948]]\n",
      "[[0.         0.54019314 0.14805195 0.73240095]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.06493507 0.27006328 1.         0.73499835]\n",
      " [0.07012987 0.26227108 0.33506495 0.63629705]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.9298701  0.38694638 1.         0.54279053]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.21636857 0.5007454  0.33755726 0.563485  ]\n",
      " [0.21757579 0.36914214 0.23571941 0.38222477]\n",
      " [0.21757294 0.3691357  0.23567787 0.38218966]\n",
      " [0.25587344 0.43489859 0.7720153  0.5592767 ]\n",
      " [0.25443175 0.45961478 0.36637646 0.54173666]\n",
      " [0.21757293 0.36913565 0.23567754 0.3821894 ]\n",
      " [0.21757293 0.36913565 0.23567754 0.3821894 ]\n",
      " [0.21757293 0.36913565 0.23567754 0.3821894 ]\n",
      " [0.21757293 0.36913565 0.23567754 0.3821894 ]\n",
      " [0.21757293 0.36913565 0.23567754 0.3821894 ]\n",
      " [0.21757293 0.36913565 0.23567754 0.3821894 ]\n",
      " [0.21757293 0.36913565 0.23567754 0.3821894 ]\n",
      " [0.21757293 0.36913565 0.23567754 0.3821894 ]\n",
      " [0.75000685 0.4726907  0.80267125 0.5407342 ]\n",
      " [0.21757293 0.36913565 0.23567754 0.3821894 ]\n",
      " [0.21757293 0.36913565 0.23567754 0.3821894 ]]\n",
      "[[0.1502146  0.         0.77253217 0.64377683]\n",
      " [0.12875536 0.24678111 0.8991416  0.88412017]\n",
      " [0.15236051 0.38412017 0.78755367 0.8133047 ]\n",
      " [0.59871244 0.6223176  0.8991416  1.        ]\n",
      " [0.6008584  0.77253217 0.8948498  1.        ]\n",
      " [0.09871244 0.6523605  0.2532189  0.7811159 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.26439726 0.16466135 0.66529334 0.63106513]\n",
      " [0.2653083  0.30349436 0.72636545 0.8037914 ]\n",
      " [0.333593   0.3759216  0.6018667  0.66215986]\n",
      " [0.56931686 0.57714224 0.7436365  0.83982015]\n",
      " [0.5378438  0.737258   0.6802112  0.8483135 ]\n",
      " [0.2688879  0.55197334 0.39861485 0.80468976]\n",
      " [0.2123439  0.1477471  0.2307846  0.18288611]\n",
      " [0.21234384 0.14774673 0.23078369 0.18288408]\n",
      " [0.21234384 0.14774671 0.23078366 0.18288401]\n",
      " [0.21234384 0.14774671 0.23078366 0.18288401]\n",
      " [0.21234384 0.14774671 0.23078366 0.18288401]\n",
      " [0.21234384 0.14774671 0.23078367 0.18288404]\n",
      " [0.21234384 0.14774673 0.2307837  0.1828841 ]\n",
      " [0.21234384 0.14774671 0.23078366 0.18288401]\n",
      " [0.21234384 0.14774671 0.23078366 0.18288401]\n",
      " [0.21234384 0.14774671 0.23078366 0.18288401]]\n",
      "[[0.19480519 0.         0.5194805  0.3116883 ]\n",
      " [0.23376623 0.06493507 0.8051948  0.5974026 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.37662336 0.5714286  0.5194805  1.        ]\n",
      " [0.37662336 0.96103895 0.45454547 1.        ]\n",
      " [0.22077923 0.54545456 0.35064936 0.97402596]\n",
      " [0.22077923 0.8831169  0.32467532 0.97402596]\n",
      " [0.6883117  0.42857143 0.77922076 0.8701299 ]\n",
      " [0.6883117  0.8051948  0.7532467  0.8701299 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.47943074 0.44748402 0.50299937 0.4816333 ]\n",
      " [0.48425388 0.45545802 0.5245586  0.5175787 ]\n",
      " [0.46640605 0.44605467 0.46856079 0.451439  ]\n",
      " [0.49246517 0.5063652  0.50515294 0.55420244]\n",
      " [0.49013594 0.54626936 0.49937117 0.55730695]\n",
      " [0.48426172 0.5038506  0.49279457 0.5474176 ]\n",
      " [0.48320624 0.540263   0.49177152 0.5514817 ]\n",
      " [0.5060052  0.49672127 0.5198169  0.54233384]\n",
      " [0.5060171  0.5319644  0.5143598  0.5423993 ]\n",
      " [0.46640596 0.44605395 0.46855947 0.45143518]\n",
      " [0.46640596 0.44605395 0.46855944 0.45143506]\n",
      " [0.46640596 0.44605413 0.4685598  0.45143607]\n",
      " [0.4664065  0.44605827 0.4685675  0.45145857]\n",
      " [0.46640596 0.44605395 0.46855944 0.45143506]\n",
      " [0.46640596 0.44605395 0.46855944 0.45143506]\n",
      " [0.46640596 0.44605395 0.46855944 0.45143506]]\n",
      "[[0.7356322  0.17816092 1.         0.7126437 ]\n",
      " [0.06896552 0.18965517 0.7816092  0.8045977 ]\n",
      " [0.59195405 0.1954023  0.75287354 0.6551724 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.21839081 0.7241379  0.3045977  0.8218391 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.5689655  0.11494253 0.6896552 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.5468198  0.44978675 0.642478   0.52529687]\n",
      " [0.38366845 0.44437551 0.5819781  0.5505852 ]\n",
      " [0.5079355  0.45220318 0.5812086  0.5248838 ]\n",
      " [0.36908478 0.4174032  0.3774769  0.42564225]\n",
      " [0.36908478 0.4174032  0.3774769  0.42564225]\n",
      " [0.36908597 0.4174069  0.3774943  0.42566225]\n",
      " [0.36908478 0.4174032  0.3774769  0.42564225]\n",
      " [0.41692293 0.54600716 0.44249618 0.5718217 ]\n",
      " [0.36908504 0.4174041  0.37748107 0.42564702]\n",
      " [0.37015778 0.4940897  0.41265345 0.5483145 ]\n",
      " [0.36908478 0.4174032  0.3774769  0.42564225]\n",
      " [0.36908478 0.4174032  0.3774769  0.42564225]\n",
      " [0.36908478 0.4174032  0.3774769  0.42564225]\n",
      " [0.36908478 0.4174032  0.3774769  0.42564225]\n",
      " [0.36908478 0.4174032  0.3774769  0.42564225]\n",
      " [0.36908478 0.4174032  0.3774769  0.42564225]]\n",
      "[[0.539267   0.45549738 1.         0.90052354]\n",
      " [0.08900524 0.13612565 0.61256546 0.7120419 ]\n",
      " [0.33507854 0.35078534 0.60732985 0.69109946]\n",
      " [0.41884816 0.10471204 0.85340315 0.5235602 ]\n",
      " [0.03141361 0.14136125 0.38743454 0.4764398 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.21989529 0.09947644 0.42408377]\n",
      " [0.0104712  0.30890054 0.09947644 0.41884816]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.4816873  0.515178   0.62855196 0.58373374]\n",
      " [0.39482275 0.41557446 0.5488861  0.54032516]\n",
      " [0.4453928  0.42727053 0.49411905 0.52910507]\n",
      " [0.5187489  0.39537087 0.6022179  0.49374065]\n",
      " [0.36962837 0.39173865 0.43409547 0.43145943]\n",
      " [0.35270306 0.38510776 0.3621664  0.39659497]\n",
      " [0.35270151 0.3851019  0.3621439  0.39656302]\n",
      " [0.36447528 0.44536084 0.4073191  0.5170717 ]\n",
      " [0.36923984 0.42287415 0.39264458 0.47665814]\n",
      " [0.35270151 0.3851019  0.3621439  0.39656302]\n",
      " [0.35270151 0.3851019  0.3621439  0.39656302]\n",
      " [0.35270151 0.3851019  0.3621439  0.39656302]\n",
      " [0.35270151 0.3851019  0.3621439  0.39656302]\n",
      " [0.35270151 0.3851019  0.3621439  0.39656302]\n",
      " [0.35270151 0.3851019  0.3621439  0.39656302]\n",
      " [0.35270151 0.3851019  0.3621439  0.39656302]]\n",
      "[[0.71910113 0.08988764 1.         0.3258427 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.11235955 0.8202247  0.76404494]\n",
      " [0.5730337  0.11235955 0.7977528  0.43820226]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.08988764 0.35955057 0.23595506 0.92134833]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.52471906 0.45323685 0.5600258  0.48329395]\n",
      " [0.43638575 0.4481558  0.4404638  0.45332748]\n",
      " [0.43638575 0.44815582 0.4404639  0.4533276 ]\n",
      " [0.43986872 0.4562886  0.5479635  0.53861666]\n",
      " [0.51932245 0.45744288 0.54313415 0.4819234 ]\n",
      " [0.43638572 0.44815573 0.44046363 0.4533272 ]\n",
      " [0.43638572 0.44815573 0.44046363 0.4533272 ]\n",
      " [0.43638572 0.44815573 0.44046363 0.4533272 ]\n",
      " [0.43638572 0.44815573 0.44046363 0.4533272 ]\n",
      " [0.43638572 0.44815573 0.44046363 0.4533272 ]\n",
      " [0.43638572 0.44815573 0.44046363 0.4533272 ]\n",
      " [0.43638572 0.44815573 0.44046363 0.4533272 ]\n",
      " [0.43638572 0.44815573 0.44046363 0.4533272 ]\n",
      " [0.4379089  0.4945009  0.45625716 0.5465667 ]\n",
      " [0.43638572 0.44815573 0.44046363 0.4533272 ]\n",
      " [0.43638572 0.44815573 0.44046363 0.4533272 ]]\n",
      "[[0.21698113 0.         0.7735849  0.8490566 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.5943396  0.8113208  0.7358491  1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.48958057 0.4275719  0.52443004 0.54070187]\n",
      " [0.45627514 0.4402984  0.46658647 0.4733382 ]\n",
      " [0.45560917 0.42182803 0.45845482 0.42962572]\n",
      " [0.45560923 0.42182833 0.45845532 0.42962742]\n",
      " [0.45560965 0.4218321  0.45846182 0.4296481 ]\n",
      " [0.45560917 0.42182803 0.4584548  0.4296257 ]\n",
      " [0.4556092  0.42182812 0.45845497 0.42962623]\n",
      " [0.4632939  0.44932905 0.49840742 0.5351742 ]\n",
      " [0.4859146  0.54023373 0.50300956 0.56500316]\n",
      " [0.45560917 0.42182803 0.4584548  0.4296257 ]\n",
      " [0.45560917 0.42182803 0.4584548  0.4296257 ]\n",
      " [0.45561025 0.42183766 0.45847094 0.42967835]\n",
      " [0.45560917 0.42182803 0.4584548  0.4296257 ]\n",
      " [0.45560917 0.42182803 0.4584548  0.4296257 ]\n",
      " [0.45560917 0.42182803 0.4584548  0.4296257 ]\n",
      " [0.45560917 0.42182803 0.4584548  0.4296257 ]]\n",
      "[[0.         0.12366737 0.9701493  0.9040512 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.52025586 0.08528785 1.         0.91684437]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.18592587 0.26374036 0.7958565  0.72663873]\n",
      " [0.16477422 0.22676681 0.18626367 0.25402215]\n",
      " [0.1647745  0.22676797 0.18626797 0.25402856]\n",
      " [0.5616243  0.2839216  0.8438207  0.7301521 ]\n",
      " [0.16477422 0.22676684 0.18626373 0.25402227]\n",
      " [0.16477421 0.22676675 0.18626341 0.2540218 ]\n",
      " [0.16477421 0.22676675 0.18626341 0.2540218 ]\n",
      " [0.16477421 0.22676675 0.18626341 0.2540218 ]\n",
      " [0.16477421 0.22676675 0.18626341 0.2540218 ]\n",
      " [0.16477421 0.22676675 0.18626341 0.2540218 ]\n",
      " [0.16477421 0.22676675 0.18626341 0.2540218 ]\n",
      " [0.16477421 0.22676675 0.18626341 0.2540218 ]\n",
      " [0.16477421 0.22676675 0.18626341 0.2540218 ]\n",
      " [0.16477421 0.22676675 0.18626341 0.2540218 ]\n",
      " [0.16477421 0.22676675 0.18626341 0.2540218 ]\n",
      " [0.16477421 0.22676675 0.18626341 0.2540218 ]]\n",
      "[[0.5246479  0.         0.6690141  0.16901408]\n",
      " [0.35915494 0.15492958 0.6971831  0.693662  ]\n",
      " [0.52112675 0.12323944 0.60915494 0.17957747]\n",
      " [0.28521127 0.39084506 0.37676057 0.5739437 ]\n",
      " [0.30633804 0.18661971 0.42605633 0.44014084]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.6443662  0.3943662  0.6760563  0.471831  ]\n",
      " [0.6584507  0.2359155  0.71478873 0.471831  ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.34507042 0.86619717 0.46478873 1.        ]\n",
      " [0.35211268 0.6302817  0.48239437 0.9049296 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.45422536 0.87676054 0.62676054 1.        ]\n",
      " [0.471831   0.6619718  0.64084506 0.943662  ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.49978065 0.29152462 0.53233767 0.3607466 ]\n",
      " [0.4850794  0.3813626  0.53527796 0.58037275]\n",
      " [0.50383943 0.34331447 0.51140356 0.36609602]\n",
      " [0.47163627 0.44351965 0.48136437 0.53364086]\n",
      " [0.46763727 0.38249937 0.4878532  0.4814654 ]\n",
      " [0.410525   0.29594517 0.41643146 0.316915  ]\n",
      " [0.51770055 0.45942244 0.53096914 0.52341825]\n",
      " [0.52637386 0.38103533 0.544399   0.5038309 ]\n",
      " [0.41050392 0.29579735 0.41624492 0.31618127]\n",
      " [0.47100598 0.6554252  0.48750675 0.71977806]\n",
      " [0.47430575 0.5476055  0.49635705 0.6720406 ]\n",
      " [0.41050363 0.29579413 0.4162407  0.31616372]\n",
      " [0.50339013 0.6550207  0.5184596  0.7230309 ]\n",
      " [0.49759924 0.53755844 0.52703506 0.66350174]\n",
      " [0.41050363 0.29579416 0.4162407  0.31616378]\n",
      " [0.41050363 0.29579413 0.41624066 0.3161637 ]]\n",
      "[[0.23056994 0.05440414 0.7435233  0.7953368 ]\n",
      " [0.0880829  0.5699482  0.9222798  0.8937824 ]\n",
      " [0.4611399  0.6062176  0.6502591  0.7927461 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.8134715  0.80569947 1.         0.9430052 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.7150259  0.1373057  0.865285  ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.29740113 0.2806091  0.6872847  0.62356275]\n",
      " [0.30036956 0.551524   0.7172799  0.7131773 ]\n",
      " [0.44419205 0.5605766  0.5941326  0.64011025]\n",
      " [0.22409995 0.25969505 0.24178672 0.28366628]\n",
      " [0.68181455 0.6222191  0.7782849  0.711933  ]\n",
      " [0.22409993 0.259695   0.24178655 0.283666  ]\n",
      " [0.2240999  0.25969487 0.24178612 0.28366536]\n",
      " [0.2346692  0.59260446 0.31599358 0.7098259 ]\n",
      " [0.2240999  0.25969487 0.24178612 0.28366536]\n",
      " [0.2240999  0.25969487 0.24178612 0.28366536]\n",
      " [0.2240999  0.25969487 0.24178612 0.28366536]\n",
      " [0.2240999  0.25969487 0.24178612 0.28366536]\n",
      " [0.22409992 0.2596949  0.24178618 0.28366542]\n",
      " [0.2240999  0.25969487 0.24178612 0.28366536]\n",
      " [0.2240999  0.25969487 0.24178612 0.28366536]\n",
      " [0.2240999  0.25969487 0.24178612 0.28366536]]\n",
      "[[0.24509804 0.         0.5735294  0.42156863]\n",
      " [0.19607843 0.28431374 0.60539216 0.63480395]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.59068626 0.3872549  0.7107843  0.51715684]\n",
      " [0.5710784  0.23529412 0.6642157  0.45588234]\n",
      " [0.64215684 0.45588234 0.7745098  0.62009805]\n",
      " [0.06127451 0.4730392  0.24754901 0.6372549 ]\n",
      " [0.19362745 0.35049018 0.32843137 0.60784316]\n",
      " [0.0882353  0.4117647  0.25490198 0.5882353 ]\n",
      " [0.6495098  0.4117647  0.8872549  0.7352941 ]\n",
      " [0.5612745  0.3872549  0.74019605 0.5539216 ]\n",
      " [0.7769608  0.65686274 0.9387255  0.8039216 ]\n",
      " [0.29166666 0.7769608  0.5710784  1.        ]\n",
      " [0.25490198 0.61519605 0.47549018 0.8627451 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.3526547  0.20531514 0.5300857  0.45779568]\n",
      " [0.31821206 0.32839447 0.5748688  0.5541414 ]\n",
      " [0.23738047 0.20663922 0.25422907 0.23592691]\n",
      " [0.5640142  0.4409737  0.6296604  0.47913593]\n",
      " [0.56790507 0.38386568 0.6042402  0.48094612]\n",
      " [0.5680051  0.43697685 0.65682435 0.5215955 ]\n",
      " [0.29508743 0.44179007 0.37671643 0.5431231 ]\n",
      " [0.30963057 0.40784466 0.4054252  0.57622564]\n",
      " [0.31630802 0.4721012  0.39030907 0.5804705 ]\n",
      " [0.60547787 0.4814934  0.72506595 0.661741  ]\n",
      " [0.5295108  0.46788535 0.67560637 0.59072495]\n",
      " [0.6521395  0.63238657 0.75220233 0.7640498 ]\n",
      " [0.40239012 0.639925   0.519897   0.7594255 ]\n",
      " [0.40352815 0.57544476 0.5690385  0.7486516 ]\n",
      " [0.23738077 0.20664091 0.25423357 0.23593612]\n",
      " [0.2373795  0.20663385 0.25421458 0.23589742]]\n",
      "[[0.         0.14114833 0.47368422 0.53588516]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.21052632 0.2799043  1.         0.8588517 ]\n",
      " [0.21052632 0.28229666 0.62918663 0.8588517 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.19741629 0.34611967 0.4766718  0.50708956]\n",
      " [0.19336566 0.2842921  0.2130324  0.30582082]\n",
      " [0.19336502 0.28428987 0.21302278 0.30580848]\n",
      " [0.31105262 0.39206088 0.81912553 0.67894614]\n",
      " [0.31857127 0.40060526 0.59147274 0.634676  ]\n",
      " [0.19336492 0.28428954 0.21302137 0.30580667]\n",
      " [0.19336492 0.28428954 0.21302137 0.30580667]\n",
      " [0.19336492 0.28428954 0.21302137 0.30580667]\n",
      " [0.19336492 0.28428954 0.21302137 0.30580667]\n",
      " [0.19336492 0.28428954 0.21302137 0.30580667]\n",
      " [0.19336492 0.28428954 0.21302137 0.30580667]\n",
      " [0.19336492 0.28428954 0.21302137 0.30580667]\n",
      " [0.19336492 0.28428954 0.21302137 0.30580667]\n",
      " [0.19336492 0.28428954 0.21302137 0.30580667]\n",
      " [0.19336492 0.28428954 0.21302137 0.30580667]\n",
      " [0.19336492 0.28428954 0.21302137 0.30580667]]\n",
      "[[0.24748491 0.         0.55130786 0.65995973]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.1891348  0.3360161  0.81086516 0.7565392 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.32394367 0.7183099  0.4607646  0.96378267]\n",
      " [0.31589538 0.93963784 0.39034206 0.9839034 ]\n",
      " [0.1971831  0.6458753  0.2917505  0.88531184]\n",
      " [0.1971831  0.89134806 0.2615694  0.97384304]\n",
      " [0.6760563  0.55331993 0.8028169  0.832998  ]\n",
      " [0.63983905 0.8148893  0.7183099  0.9939638 ]\n",
      " [0.5694165  0.71227366 0.63179076 0.8470825 ]\n",
      " [0.51307845 0.8148893  0.62575454 0.9979879 ]\n",
      " [0.6358149  0.6418511  0.6841046  0.8390342 ]\n",
      " [0.637827   0.94567406 0.7022133  0.9939638 ]\n",
      " [0.5070422  0.9356137  0.5895372  1.        ]] [[0.4032221  0.13901229 0.60261136 0.5270809 ]\n",
      " [0.26752275 0.13356936 0.28254196 0.17041977]\n",
      " [0.26751426 0.13349043 0.28244123 0.17011091]\n",
      " [0.3939765  0.21854667 0.5895251  0.717704  ]\n",
      " [0.2675126  0.13347669 0.282416   0.1700377 ]\n",
      " [0.43272185 0.6050901  0.501364   0.7916509 ]\n",
      " [0.4143262  0.7525739  0.45005998 0.87385195]\n",
      " [0.34824035 0.6637312  0.40116423 0.80242026]\n",
      " [0.3460191  0.7440601  0.37749463 0.86794007]\n",
      " [0.58182347 0.6393168  0.6367425  0.76414007]\n",
      " [0.56696486 0.71788377 0.6180643  0.8342776 ]\n",
      " [0.51704055 0.6905751  0.5334896  0.7753682 ]\n",
      " [0.4968965  0.74109495 0.5456756  0.8367432 ]\n",
      " [0.47028878 0.5979029  0.56260955 0.82667106]\n",
      " [0.56699854 0.7561044  0.6028428  0.8200397 ]\n",
      " [0.4982431  0.7740992  0.51819706 0.79745585]]\n",
      "[[0.         0.12366737 0.9701493  0.9040512 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.52025586 0.08528785 1.         0.91684437]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.16094589 0.24444649 0.81942046 0.74393445]\n",
      " [0.13830905 0.20519583 0.16149502 0.23460291]\n",
      " [0.13830936 0.20519711 0.16149977 0.23460999]\n",
      " [0.56513    0.26623482 0.8711351  0.74785274]\n",
      " [0.13830905 0.20519584 0.16149509 0.23460303]\n",
      " [0.13830902 0.20519575 0.16149475 0.23460251]\n",
      " [0.13830902 0.20519575 0.16149475 0.23460251]\n",
      " [0.13830902 0.20519575 0.16149475 0.23460251]\n",
      " [0.13830902 0.20519575 0.16149475 0.23460251]\n",
      " [0.13830902 0.20519575 0.16149475 0.23460251]\n",
      " [0.13830902 0.20519575 0.16149475 0.23460251]\n",
      " [0.13830902 0.20519575 0.16149475 0.23460251]\n",
      " [0.13830902 0.20519575 0.16149475 0.23460251]\n",
      " [0.13830902 0.20519575 0.16149475 0.23460251]\n",
      " [0.13830902 0.20519575 0.16149475 0.23460251]\n",
      " [0.13830902 0.20519575 0.16149475 0.23460251]]\n",
      "[[0.46293727 0.         0.81245184 0.3462783 ]\n",
      " [0.07135152 0.26537216 0.9160117  1.        ]\n",
      " [0.5082447  0.2815534  0.84157807 0.43365696]\n",
      " [0.708892   0.47572815 0.9224842  0.8899676 ]\n",
      " [0.81245184 0.74757284 0.89983046 0.8996764 ]\n",
      " [0.5276622  0.39158577 0.8027431  0.9514563 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.31730622 0.5566343  0.92572045 0.98381877]\n",
      " [0.8448143  0.6569579  0.9095392  0.7378641 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.47324777 0.25599524 0.64597    0.39801508]\n",
      " [0.33980626 0.32923695 0.6538261  0.7082122 ]\n",
      " [0.47266364 0.3459281  0.6273438  0.4669111 ]\n",
      " [0.5667505  0.5300451  0.6612979  0.6799284 ]\n",
      " [0.6062877  0.6082176  0.6690231  0.68040013]\n",
      " [0.4753047  0.5263256  0.6386664  0.6941131 ]\n",
      " [0.29143852 0.26072744 0.3048083  0.28459534]\n",
      " [0.29143855 0.2607276  0.30480874 0.28459626]\n",
      " [0.2914385  0.26072735 0.30480805 0.2845948 ]\n",
      " [0.54130006 0.5564127  0.677572   0.71606994]\n",
      " [0.6160406  0.65998733 0.68651617 0.7095798 ]\n",
      " [0.29145142 0.26081318 0.3050064  0.28503004]\n",
      " [0.29143852 0.26072744 0.30480832 0.28459534]\n",
      " [0.2914385  0.26072735 0.30480805 0.2845948 ]\n",
      " [0.2914385  0.26072735 0.30480805 0.2845948 ]\n",
      " [0.2914385  0.26072735 0.30480805 0.2845948 ]]\n",
      "[[0.72894734 0.21315789 1.         0.35789475]\n",
      " [0.40263158 0.29473683 0.8736842  0.6       ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.34473684 0.5        0.4236842  0.54473686]\n",
      " [0.35263157 0.41315788 0.58157897 0.49473685]\n",
      " [0.48421052 0.55263156 0.6078947  0.70789474]\n",
      " [0.54473686 0.6657895  0.6105263  0.70526314]\n",
      " [0.5947368  0.58157897 0.8        0.7868421 ]\n",
      " [0.66842103 0.72105265 0.80263156 0.7868421 ]\n",
      " [0.         0.28947368 0.4236842  0.46842104]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.67007756 0.40487412 0.77980566 0.45887068]\n",
      " [0.40088227 0.44619945 0.6904205  0.5137763 ]\n",
      " [0.22124125 0.34325153 0.2391163  0.35889232]\n",
      " [0.37506592 0.486152   0.43947107 0.48346877]\n",
      " [0.36744285 0.4562249  0.5719241  0.5216588 ]\n",
      " [0.52797854 0.5228635  0.6258383  0.57217324]\n",
      " [0.5232575  0.5533512  0.59725213 0.5762444 ]\n",
      " [0.52915305 0.5306031  0.62122405 0.59382164]\n",
      " [0.5807036  0.571584   0.63540226 0.59481794]\n",
      " [0.2446622  0.4398529  0.43233156 0.4966194 ]\n",
      " [0.22124085 0.34325042 0.23911034 0.3588862 ]\n",
      " [0.22124086 0.34325048 0.2391107  0.35888657]\n",
      " [0.22124085 0.34325042 0.23911034 0.3588862 ]\n",
      " [0.22124085 0.34325042 0.23911034 0.3588862 ]\n",
      " [0.22124085 0.34325042 0.23911034 0.3588862 ]\n",
      " [0.22124085 0.34325042 0.23911034 0.3588862 ]]\n",
      "[[0.34653464 0.         0.6707921  0.42821783]\n",
      " [0.36881188 0.27722773 0.7648515  0.62623763]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.7326733  0.41831684 0.9158416  0.59653467]\n",
      " [0.6509901  0.31930694 0.7648515  0.5866337 ]\n",
      " [0.7178218  0.36633664 0.8861386  0.5470297 ]\n",
      " [0.2549505  0.43069306 0.37376237 0.5569307 ]\n",
      " [0.28465346 0.27227724 0.36633664 0.49257424]\n",
      " [0.21782178 0.5        0.33168316 0.6732673 ]\n",
      " [0.48514852 0.7648515  0.7351485  1.        ]\n",
      " [0.5371287  0.6039604  0.7574257  0.8589109 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.11138614 0.46534654 0.32425743 0.8019802 ]\n",
      " [0.21782178 0.43316832 0.41831684 0.5841584 ]\n",
      " [0.08168317 0.7351485  0.24009901 0.87623763]\n",
      " [0.         0.         0.         0.        ]] [[0.4570018  0.21220236 0.5919619  0.39424565]\n",
      " [0.44556195 0.40974414 0.62222886 0.6033348 ]\n",
      " [0.2591254  0.21696936 0.27459255 0.2452518 ]\n",
      " [0.6095723  0.5128926  0.6846615  0.5317248 ]\n",
      " [0.5900327  0.39356172 0.6732581  0.5173371 ]\n",
      " [0.5723222  0.4331707  0.65012693 0.52714986]\n",
      " [0.33110982 0.4508595  0.42824772 0.5183488 ]\n",
      " [0.3317811  0.3815946  0.41409057 0.5301944 ]\n",
      " [0.3594979  0.4716286  0.42459607 0.5220753 ]\n",
      " [0.53239775 0.6570993  0.61675704 0.7655182 ]\n",
      " [0.47664997 0.5700418  0.633169   0.7080289 ]\n",
      " [0.25912398 0.21696137 0.27457213 0.24520808]\n",
      " [0.3271781  0.57023335 0.39712456 0.70432943]\n",
      " [0.34678558 0.48579586 0.48639202 0.62072885]\n",
      " [0.2869173  0.6630595  0.3258586  0.713783  ]\n",
      " [0.25912347 0.21695839 0.27456453 0.24519181]]\n",
      "[[0.33451957 0.01779359 0.85409254 0.46263346]\n",
      " [0.14590748 0.         0.40213522 0.5871886 ]\n",
      " [0.24199288 0.0569395  0.3985765  0.46263346]\n",
      " [0.27046263 0.42348754 0.54448396 1.        ]\n",
      " [0.3594306  0.9181495  0.54448396 1.        ]\n",
      " [0.14590748 0.45907474 0.3772242  0.9501779 ]\n",
      " [0.21708184 0.87188613 0.3772242  0.9501779 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.4633977  0.30366835 0.59010017 0.4994566 ]\n",
      " [0.379155   0.3322626  0.49509186 0.5696286 ]\n",
      " [0.41714978 0.34948877 0.48619992 0.48288274]\n",
      " [0.4391157  0.47646615 0.5308213  0.7059537 ]\n",
      " [0.44469798 0.62268126 0.4930055  0.6951038 ]\n",
      " [0.39615345 0.45002264 0.4683332  0.6907753 ]\n",
      " [0.4157784  0.62099826 0.45995367 0.68289137]\n",
      " [0.3540183  0.2979518  0.36337742 0.3181087 ]\n",
      " [0.3540182  0.29795125 0.36337623 0.3181057 ]\n",
      " [0.3540182  0.29795122 0.3633762  0.3181056 ]\n",
      " [0.3540182  0.29795122 0.3633762  0.3181056 ]\n",
      " [0.3540182  0.29795122 0.3633762  0.3181056 ]\n",
      " [0.3540182  0.29795122 0.3633762  0.3181056 ]\n",
      " [0.3540182  0.29795122 0.3633762  0.3181056 ]\n",
      " [0.3540182  0.29795122 0.3633762  0.3181056 ]\n",
      " [0.3540182  0.29795122 0.3633762  0.3181056 ]]\n",
      "[[0.6911315  0.13455658 1.         0.62691134]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.10397553 0.71253824 0.70642203]\n",
      " [0.47400612 0.14984709 0.7217125  0.59938836]\n",
      " [0.39143732 0.7308869  0.50152904 0.89908254]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.25382262 0.6666667  0.36697248 0.88990825]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.11926606 0.6727829  0.17125382 0.78899086]\n",
      " [0.11620795 0.75535166 0.16207951 0.86850154]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.05504587 0.6819572  0.12844037 0.86850154]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.5618316  0.3284658  0.7588867  0.62457514]\n",
      " [0.25396964 0.30825743 0.2697412  0.32738394]\n",
      " [0.25397146 0.30826443 0.26976806 0.3274221 ]\n",
      " [0.28531957 0.38868752 0.6531501  0.625342  ]\n",
      " [0.50923806 0.39790988 0.61689854 0.5745232 ]\n",
      " [0.4416277  0.5874833  0.5010306  0.65397286]\n",
      " [0.2539697  0.30825767 0.2697421  0.32738522]\n",
      " [0.37088472 0.59183353 0.44500256 0.6439273 ]\n",
      " [0.25396964 0.3082575  0.26974148 0.3273843 ]\n",
      " [0.31547356 0.55147046 0.34411216 0.5955992 ]\n",
      " [0.30489385 0.56430405 0.3374891  0.61716473]\n",
      " [0.25397024 0.3082597  0.26975    0.3273964 ]\n",
      " [0.25400734 0.30852032 0.27026206 0.32821822]\n",
      " [0.26946047 0.567245   0.32970184 0.6935718 ]\n",
      " [0.25397167 0.30826524 0.26977125 0.32742664]\n",
      " [0.25396964 0.3082574  0.2697411  0.32738376]]\n",
      "[[0.         0.14529915 0.2991453  0.45299146]\n",
      " [0.12820514 0.13675214 0.25641027 0.2905983 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.0940171  0.20512821 1.         0.8034188 ]\n",
      " [0.13675214 0.1965812  0.4017094  0.4871795 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.33333334 0.6581197  0.9230769  0.8632479 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.41158134 0.4365347  0.500487   0.47760236]\n",
      " [0.46148646 0.43794513 0.45953658 0.45543325]\n",
      " [0.4098265  0.4358819  0.41611448 0.4428501 ]\n",
      " [0.41783702 0.45971733 0.5823511  0.5560867 ]\n",
      " [0.42244416 0.44863933 0.48923415 0.5107027 ]\n",
      " [0.40977007 0.43574777 0.41555414 0.44215694]\n",
      " [0.40977007 0.43574777 0.41555414 0.44215694]\n",
      " [0.40977007 0.43574777 0.41555414 0.44215694]\n",
      " [0.40977007 0.43574777 0.41555414 0.44215694]\n",
      " [0.40977007 0.4357478  0.41555423 0.44215706]\n",
      " [0.40977007 0.43574777 0.41555414 0.44215694]\n",
      " [0.40977007 0.43574777 0.41555414 0.44215694]\n",
      " [0.40977007 0.43574777 0.41555414 0.44215694]\n",
      " [0.572542   0.5185306  0.57895654 0.55274093]\n",
      " [0.40977007 0.43574777 0.41555414 0.44215694]\n",
      " [0.40977007 0.43574777 0.41555414 0.44215694]]\n",
      "[[0.01098901 0.21978022 0.7582418  0.7307692 ]\n",
      " [0.43406594 0.26923078 0.76373625 0.64835167]\n",
      " [0.         0.22527473 0.4010989  0.5769231 ]\n",
      " [0.03846154 0.06043956 1.         0.9395604 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.88461536 0.510989   0.99450547 0.72527474]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.37223744 0.4114799  0.5207338  0.56689966]\n",
      " [0.53023946 0.43623453 0.5473932  0.5032299 ]\n",
      " [0.41479418 0.4414779  0.4654498  0.46468025]\n",
      " [0.37676165 0.42612147 0.60746217 0.5571578 ]\n",
      " [0.366489   0.38495445 0.3750476  0.39643028]\n",
      " [0.36648902 0.38495445 0.37504765 0.3964304 ]\n",
      " [0.366489   0.38495442 0.37504753 0.39643022]\n",
      " [0.366489   0.38495442 0.37504753 0.39643022]\n",
      " [0.366489   0.38495442 0.37504753 0.39643022]\n",
      " [0.366489   0.38495442 0.37504756 0.39643028]\n",
      " [0.366489   0.38495442 0.37504753 0.39643022]\n",
      " [0.366489   0.38495442 0.37504753 0.39643022]\n",
      " [0.366489   0.38495442 0.37504753 0.39643022]\n",
      " [0.59444714 0.5316051  0.6143235  0.60732186]\n",
      " [0.366489   0.38495442 0.37504753 0.39643022]\n",
      " [0.366489   0.38495442 0.37504753 0.39643022]]\n",
      "[[0.         0.18461539 0.47076923 0.6707692 ]\n",
      " [0.19076923 0.12615384 1.         0.71076924]\n",
      " [0.23076923 0.48923078 0.31692308 0.58153844]\n",
      " [0.45846155 0.6523077  0.6430769  0.8738462 ]\n",
      " [0.49846154 0.7630769  0.6276923  0.8738462 ]\n",
      " [0.41846153 0.6923077  0.5353846  0.7784615 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.10769231 0.37846154 0.2830769  0.59384614]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.272222   0.40692806 0.45180076 0.57428163]\n",
      " [0.34549266 0.38886678 0.6989528  0.57627034]\n",
      " [0.3470982  0.4594154  0.44695485 0.5429971 ]\n",
      " [0.48546204 0.56481403 0.5843437  0.6264943 ]\n",
      " [0.50509644 0.5987746  0.5767362  0.63920426]\n",
      " [0.4872281  0.5693488  0.5408754  0.5971604 ]\n",
      " [0.26158762 0.32527485 0.27687192 0.34270513]\n",
      " [0.26158807 0.3252764  0.27687824 0.34271353]\n",
      " [0.26158756 0.32527456 0.27687067 0.34270343]\n",
      " [0.26158756 0.32527465 0.2768711  0.342704  ]\n",
      " [0.26158756 0.32527456 0.27687067 0.34270343]\n",
      " [0.2615876  0.32527465 0.27687111 0.34270403]\n",
      " [0.30776903 0.506033   0.38085294 0.5694335 ]\n",
      " [0.26158756 0.32527456 0.27687067 0.34270343]\n",
      " [0.26158756 0.32527456 0.27687067 0.34270343]\n",
      " [0.26158756 0.32527456 0.27687067 0.34270343]]\n",
      "[[0.08791209 0.17857143 0.22527473 0.39835164]\n",
      " [0.         0.6593407  0.04120879 0.6813187 ]\n",
      " [0.17582418 0.6565934  0.1978022  0.69505495]\n",
      " [0.15384616 0.2032967  0.7307692  0.5879121 ]\n",
      " [0.14285715 0.1923077  0.34615386 0.39285713]\n",
      " [0.10164835 0.510989   0.23351648 0.60164833]\n",
      " [0.00549451 0.5906593  0.11263736 0.6813187 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.15384616 0.5631868  0.1978022  0.6923077 ]\n",
      " [0.5824176  0.5192308  0.72252744 0.6978022 ]\n",
      " [0.66483516 0.6813187  0.72252744 0.8214286 ]\n",
      " [0.5192308  0.52472526 0.60164833 0.65384614]\n",
      " [0.3489011  0.63736266 0.5494506  0.771978  ]\n",
      " [0.739011   0.4478022  1.         0.7005494 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.3434066  0.739011   0.38186812 0.771978  ]] [[0.25029826 0.3846842  0.34376886 0.46488103]\n",
      " [0.26573664 0.6137378  0.29558918 0.5959877 ]\n",
      " [0.32326204 0.61596775 0.3334431  0.6080858 ]\n",
      " [0.30624223 0.41354218 0.6288476  0.538079  ]\n",
      " [0.28984353 0.36593816 0.3770032  0.4303755 ]\n",
      " [0.27995616 0.4661309  0.32684717 0.5077143 ]\n",
      " [0.26726502 0.5354234  0.29705805 0.5761661 ]\n",
      " [0.29824108 0.45159855 0.35651538 0.4894099 ]\n",
      " [0.32581112 0.5285697  0.37256286 0.58288646]\n",
      " [0.4734149  0.45283657 0.61292475 0.53859186]\n",
      " [0.5227166  0.54120743 0.5937139  0.587255  ]\n",
      " [0.3729635  0.41974887 0.43085432 0.46180174]\n",
      " [0.46188116 0.5623137  0.5104582  0.6108012 ]\n",
      " [0.68609023 0.46761405 0.7656205  0.53887236]\n",
      " [0.23982517 0.3360613  0.25650895 0.35241997]\n",
      " [0.4730282  0.59553486 0.48507887 0.60903716]]\n",
      "[[1.2950841e-01 1.0781671e-02 8.8152999e-01 7.6549864e-01]\n",
      " [1.2835323e-04 0.0000000e+00 9.9743295e-01 9.9191374e-01]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      " [3.2357848e-01 8.6792451e-01 4.4487229e-01 1.0000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      " [1.2681299e-01 8.1132078e-01 3.2627392e-01 9.7843665e-01]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]] [[0.3380165  0.21973576 0.7584061  0.6497679 ]\n",
      " [0.24747019 0.23715258 0.77615565 0.76593554]\n",
      " [0.20769788 0.21271814 0.22643617 0.24137558]\n",
      " [0.37506372 0.7215122  0.49233615 0.8140117 ]\n",
      " [0.20769784 0.21271794 0.2264355  0.24137436]\n",
      " [0.3061332  0.67309034 0.4690832  0.7933661 ]\n",
      " [0.20769785 0.212718   0.2264357  0.24137473]\n",
      " [0.20769784 0.21271794 0.2264355  0.24137436]\n",
      " [0.20769784 0.21271794 0.22643548 0.24137434]\n",
      " [0.20769784 0.21271794 0.2264355  0.24137437]\n",
      " [0.20769784 0.21271794 0.22643548 0.24137434]\n",
      " [0.20769791 0.21271825 0.22643647 0.2413761 ]\n",
      " [0.20769784 0.21271794 0.22643551 0.2413744 ]\n",
      " [0.20769784 0.21271794 0.22643548 0.24137434]\n",
      " [0.20769784 0.21271794 0.22643548 0.24137434]\n",
      " [0.20769784 0.21271794 0.22643548 0.24137434]]\n",
      "[[0.648855   0.         0.8778626  0.19847329]\n",
      " [0.14503817 0.05343511 0.78625953 0.79389316]\n",
      " [0.5038168  0.07633588 0.79389316 0.3129771 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.519084   0.6183206  0.70229006 0.7251908 ]\n",
      " [0.53435117 0.65648854 0.7480916  0.71755725]\n",
      " [0.41984734 0.60305345 0.6870229  0.8015267 ]\n",
      " [0.4274809  0.7251908  0.6946565  0.8015267 ]\n",
      " [0.1221374  0.71755725 0.35877863 1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.5189135  0.40731266 0.56120396 0.4433097 ]\n",
      " [0.4437012  0.42368928 0.5452618  0.554108  ]\n",
      " [0.50711477 0.42487976 0.54505    0.46021235]\n",
      " [0.42923805 0.40822166 0.4337742  0.41737664]\n",
      " [0.42923805 0.4082217  0.43377423 0.4173767 ]\n",
      " [0.5027496  0.522415   0.52867043 0.5420642 ]\n",
      " [0.51220024 0.520187   0.52964854 0.5355062 ]\n",
      " [0.49014065 0.5231848  0.5209516  0.5648999 ]\n",
      " [0.4969591  0.545544   0.5170582  0.5631074 ]\n",
      " [0.44125256 0.55158174 0.47945473 0.59626865]\n",
      " [0.42923805 0.40822166 0.43377417 0.41737655]\n",
      " [0.42923805 0.40822166 0.43377417 0.41737658]\n",
      " [0.42923805 0.40822166 0.43377417 0.41737655]\n",
      " [0.42923805 0.40822166 0.43377417 0.41737655]\n",
      " [0.42923805 0.40822166 0.43377417 0.41737655]\n",
      " [0.42923805 0.40822166 0.43377417 0.41737655]]\n",
      "[[0.3431085  0.         0.627566   0.6480938 ]\n",
      " [0.3431085  0.627566   0.40175954 0.71260995]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.49560118 0.68328446 0.6598241  0.86510265]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.3431085  0.60703814 0.5073314  1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.46307078 0.24780725 0.5463216  0.53635657]\n",
      " [0.44809645 0.5614513  0.4762718  0.73329556]\n",
      " [0.4187426  0.24852726 0.4239561  0.2736348 ]\n",
      " [0.4187423  0.24852239 0.4239514  0.2736082 ]\n",
      " [0.41874242 0.24852434 0.4239533  0.2736189 ]\n",
      " [0.49464634 0.52106917 0.5226441  0.67845225]\n",
      " [0.41922343 0.25505978 0.42614478 0.29121146]\n",
      " [0.42676252 0.29064187 0.45184642 0.40900594]\n",
      " [0.47089875 0.6282569  0.5019111  0.74095607]\n",
      " [0.41874227 0.24852219 0.4239512  0.27360713]\n",
      " [0.4187423  0.24852242 0.42395142 0.2736084 ]\n",
      " [0.41874227 0.24852219 0.4239512  0.27360713]\n",
      " [0.41874227 0.24852219 0.4239512  0.27360713]\n",
      " [0.41874227 0.24852219 0.4239512  0.27360713]\n",
      " [0.41874227 0.24852219 0.4239512  0.27360713]\n",
      " [0.41874227 0.24852219 0.4239512  0.27360713]]\n",
      "[[0.38990825 0.01834862 0.66972476 0.2798165 ]\n",
      " [0.4678899  0.16513762 0.70642203 0.47706422]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.38073394 0.06422018 0.5321101  0.20183486]\n",
      " [0.49541286 0.15137614 0.67431194 0.26605505]\n",
      " [0.35779816 0.         0.4266055  0.09633028]\n",
      " [0.34862384 0.2614679  0.47706422 0.3440367 ]\n",
      " [0.4266055  0.2522936  0.5        0.33027524]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.5642202  0.7155963  0.70642203 0.9724771 ]\n",
      " [0.51376146 0.42201835 0.6788991  0.7522936 ]\n",
      " [0.5733945  0.96330273 0.6972477  1.        ]\n",
      " [0.33486238 0.6834862  0.48623854 0.95412844]\n",
      " [0.41284403 0.44954127 0.55504584 0.72477067]\n",
      " [0.28899083 0.91284406 0.3761468  0.94495416]\n",
      " [0.         0.         0.         0.        ]] [[0.4950148  0.33342287 0.5254078  0.40484592]\n",
      " [0.49797225 0.40027848 0.5271534  0.51118314]\n",
      " [0.42982173 0.3352186  0.43432835 0.3516859 ]\n",
      " [0.48765314 0.3826225  0.50385374 0.4271824 ]\n",
      " [0.50063133 0.38980976 0.5164302  0.44367334]\n",
      " [0.4576639  0.3516991  0.46936324 0.37400046]\n",
      " [0.49049872 0.4043135  0.5044802  0.4411141 ]\n",
      " [0.48923805 0.40883338 0.50139517 0.4534492 ]\n",
      " [0.4298216  0.3352171  0.4343264  0.3516776 ]\n",
      " [0.5210053  0.57861775 0.53655183 0.6427134 ]\n",
      " [0.5069338  0.47484833 0.5307151  0.57346267]\n",
      " [0.5171759  0.63785917 0.53235555 0.6680408 ]\n",
      " [0.4975615  0.57780224 0.50908136 0.64605576]\n",
      " [0.49599537 0.498778   0.5126775  0.5777141 ]\n",
      " [0.48668346 0.62594813 0.50133175 0.6574835 ]\n",
      " [0.42982116 0.33521205 0.43431997 0.35165003]]\n",
      "[[0.1986755  0.06622516 0.75165564 0.7086093 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.23178808 0.         0.8013245  0.6986755 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.615894   0.51655626 0.7450331  0.821192  ]\n",
      " [0.6258278  0.8178808  0.70198673 1.        ]\n",
      " [0.44039735 0.6258278  0.5397351  0.80463576]\n",
      " [0.3807947  0.8013245  0.49668875 0.99006623]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.28807947 0.69536424 0.37086093 0.90066224]\n",
      " [0.21192053 0.5264901  0.29139072 0.7350993 ]\n",
      " [0.20198676 0.72516555 0.24503312 0.8741722 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.43273425 0.28208852 0.57838404 0.5963921 ]\n",
      " [0.356219   0.26614797 0.36543626 0.28947574]\n",
      " [0.35621896 0.2661478  0.36543596 0.28947484]\n",
      " [0.39929402 0.26847106 0.5890587  0.59347093]\n",
      " [0.35621908 0.2661488  0.3654378  0.2894803 ]\n",
      " [0.54386413 0.52201754 0.5877763  0.6351932 ]\n",
      " [0.55535847 0.6459322  0.58660513 0.73781323]\n",
      " [0.45964378 0.5284624  0.50905615 0.6612531 ]\n",
      " [0.48495567 0.64578784 0.51916224 0.745996  ]\n",
      " [0.35621986 0.26615462 0.36544836 0.28951156]\n",
      " [0.45919964 0.59202844 0.4847033  0.69058895]\n",
      " [0.41511244 0.5278677  0.4456421  0.61674845]\n",
      " [0.40875947 0.609565   0.43576878 0.70396554]\n",
      " [0.35621896 0.2661478  0.36543593 0.2894747 ]\n",
      " [0.35621896 0.26614776 0.36543587 0.28947455]\n",
      " [0.35621905 0.26614836 0.36543697 0.28947783]]\n",
      "[[0.02572347 0.         0.6752412  0.5176849 ]\n",
      " [0.08360129 0.02893891 0.99356914 0.7620579 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.33440515 0.68167204 0.62700963 1.        ]\n",
      " [0.34083602 0.7813505  0.52411574 0.9903537 ]\n",
      " [0.0192926  0.562701   0.33440515 0.8424437 ]\n",
      " [0.00643087 0.75884247 0.20578778 0.829582  ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.29503918 0.26499438 0.5769273  0.49652928]\n",
      " [0.30757663 0.30480585 0.71661687 0.65105367]\n",
      " [0.26324293 0.2649132  0.27842188 0.2883663 ]\n",
      " [0.40700218 0.57049084 0.61571705 0.74091434]\n",
      " [0.44000006 0.6733396  0.54941714 0.7348143 ]\n",
      " [0.30252564 0.55374926 0.40937522 0.70328736]\n",
      " [0.30192354 0.619064   0.37953523 0.6978998 ]\n",
      " [0.263243   0.2649135  0.27842286 0.28836805]\n",
      " [0.26324278 0.26491252 0.2784198  0.2883625 ]\n",
      " [0.26324278 0.26491252 0.2784198  0.2883625 ]\n",
      " [0.26324278 0.26491252 0.2784198  0.2883625 ]\n",
      " [0.26324278 0.26491252 0.2784198  0.2883625 ]\n",
      " [0.2632428  0.26491252 0.27841985 0.2883626 ]\n",
      " [0.26324278 0.26491252 0.2784198  0.2883625 ]\n",
      " [0.26324278 0.26491252 0.2784198  0.2883625 ]\n",
      " [0.26324278 0.26491252 0.2784198  0.2883625 ]]\n",
      "[[0.38985386 0.         0.70974636 0.38709676]\n",
      " [0.40060657 0.26344085 0.79576784 0.7258065 ]\n",
      " [0.5914668  0.26881722 0.66942376 0.36827958]\n",
      " [0.6398539  0.62903225 0.8656603  0.87903225]\n",
      " [0.72049904 0.31989247 0.8683485  0.65591395]\n",
      " [0.48125172 0.8091398  0.68824095 0.9677419 ]\n",
      " [0.27695066 0.5913978  0.41404742 0.77956986]\n",
      " [0.30652055 0.34946236 0.45437    0.63709676]\n",
      " [0.2608216  0.7607527  0.36566034 0.91935486]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.5242625  0.6989247  0.78770334 1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.13447753 0.64784944 0.5699614  1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.4895399  0.22697397 0.60639954 0.40024263]\n",
      " [0.45960236 0.36022854 0.61123633 0.6334907 ]\n",
      " [0.5346881  0.35554203 0.58378375 0.4360264 ]\n",
      " [0.5543153  0.5721352  0.62843204 0.6763895 ]\n",
      " [0.5847776  0.43814296 0.63116324 0.6059743 ]\n",
      " [0.5551944  0.67949665 0.5996829  0.7332273 ]\n",
      " [0.39674744 0.57000244 0.45983958 0.675665  ]\n",
      " [0.39956474 0.41186434 0.4579665  0.5559409 ]\n",
      " [0.40123546 0.67856526 0.44937962 0.74117815]\n",
      " [0.29973358 0.23251924 0.31257173 0.25920123]\n",
      " [0.48696417 0.5910723  0.599661   0.7434762 ]\n",
      " [0.29973358 0.23251925 0.31257176 0.25920126]\n",
      " [0.29973376 0.23252048 0.3125745  0.25920796]\n",
      " [0.41321325 0.5634801  0.50058496 0.7493356 ]\n",
      " [0.29973376 0.23252031 0.31257418 0.25920713]\n",
      " [0.29973355 0.23251908 0.31257138 0.25920033]]\n",
      "[[0.12631579 0.         0.83157897 0.5824561 ]\n",
      " [0.10526316 0.3614035  0.8701754  0.7614035 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.4350877  0.645614   0.90877193 1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.09122807 0.55789477 0.4631579  0.9122807 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.37302628 0.28615698 0.6243212  0.5376193 ]\n",
      " [0.36065388 0.3530465  0.65070975 0.6858134 ]\n",
      " [0.3159288  0.27931243 0.32773033 0.3013298 ]\n",
      " [0.46329543 0.5578805  0.6327071  0.7236129 ]\n",
      " [0.31592867 0.27931166 0.32772836 0.30132544]\n",
      " [0.36916792 0.5598212  0.47399318 0.6956132 ]\n",
      " [0.31592965 0.27931756 0.32774317 0.3013578 ]\n",
      " [0.31592864 0.27931163 0.3277283  0.30132532]\n",
      " [0.31592864 0.27931163 0.3277283  0.30132532]\n",
      " [0.31592864 0.27931163 0.3277283  0.30132532]\n",
      " [0.31592864 0.27931163 0.3277283  0.30132532]\n",
      " [0.31592864 0.27931163 0.3277283  0.30132532]\n",
      " [0.31592867 0.27931178 0.32772866 0.30132613]\n",
      " [0.31592864 0.27931163 0.3277283  0.30132532]\n",
      " [0.31592864 0.27931163 0.3277283  0.30132532]\n",
      " [0.31592864 0.27931163 0.3277283  0.30132532]]\n",
      "[[0.27756655 0.33079848 0.581749   0.6501901 ]\n",
      " [0.3041825  0.20152092 0.72243345 0.7718631 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.4068441  0.7528517  0.5513308  0.9961977 ]\n",
      " [0.4068441  0.9163498  0.4714829  1.        ]\n",
      " [0.34600762 0.7528517  0.41064638 0.9429658 ]\n",
      " [0.34220532 0.8593156  0.39163497 0.9429658 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.47528517 0.         0.69961977 0.21673004]\n",
      " [0.36121672 0.56273764 0.486692   0.6425856 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.46870697 0.44397533 0.5142746  0.56895256]\n",
      " [0.4665399  0.39595413 0.5389651  0.609973  ]\n",
      " [0.41417155 0.31089422 0.41967395 0.32975924]\n",
      " [0.4787934  0.5848399  0.5162334  0.6952603 ]\n",
      " [0.48268613 0.66260874 0.50095737 0.69730973]\n",
      " [0.4712165  0.5867098  0.4928586  0.6776271 ]\n",
      " [0.4755422  0.65337646 0.4900643  0.6846634 ]\n",
      " [0.41417316 0.31090897 0.41969368 0.32983768]\n",
      " [0.41417152 0.31089383 0.41967344 0.3297572 ]\n",
      " [0.41417152 0.31089383 0.41967344 0.3297572 ]\n",
      " [0.41417152 0.31089383 0.41967344 0.3297572 ]\n",
      " [0.49058783 0.32037666 0.5384957  0.37476483]\n",
      " [0.47205514 0.50654453 0.49640834 0.55557495]\n",
      " [0.41417152 0.31089383 0.41967344 0.3297572 ]\n",
      " [0.41417152 0.31089383 0.41967344 0.3297572 ]\n",
      " [0.41417152 0.31089383 0.41967344 0.3297572 ]]\n",
      "[[0.68202764 0.1797235  1.         0.38709676]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.686636   0.19815668 0.8663595  0.39631337]\n",
      " [0.         0.25345623 0.9124424  0.8202765 ]\n",
      " [0.58986175 0.24884793 0.9032258  0.6267281 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.5772451  0.42706496 0.67619675 0.46103412]\n",
      " [0.3326597  0.3949491  0.34348387 0.40552393]\n",
      " [0.61438394 0.4117159  0.6603013  0.4363626 ]\n",
      " [0.33386075 0.45164588 0.66551566 0.5903843 ]\n",
      " [0.5061431  0.44671947 0.6546425  0.5390004 ]\n",
      " [0.33265045 0.39492872 0.34337816 0.4054096 ]\n",
      " [0.33265045 0.39492872 0.34337816 0.4054096 ]\n",
      " [0.33265045 0.39492872 0.34337816 0.4054096 ]\n",
      " [0.33265045 0.39492872 0.34337816 0.4054096 ]\n",
      " [0.33265045 0.39492872 0.34337816 0.4054096 ]\n",
      " [0.33265045 0.39492872 0.34337816 0.4054096 ]\n",
      " [0.33265045 0.39492872 0.34337816 0.4054096 ]\n",
      " [0.33265045 0.39492872 0.34337816 0.4054096 ]\n",
      " [0.33265045 0.39492875 0.34337825 0.4054097 ]\n",
      " [0.33265045 0.39492872 0.34337816 0.4054096 ]\n",
      " [0.33265045 0.39492872 0.34337816 0.4054096 ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ww/3nytfk1917b8nz7dchtckxnm0000gn/T/ipykernel_76803/3945016058.py:87: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  result['obj_class'].replace(class_dict, inplace=True)\n",
      "/var/folders/ww/3nytfk1917b8nz7dchtckxnm0000gn/T/ipykernel_76803/3945016058.py:113: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['head' 'torso' 'neck' ... 'neck' 'lleg' 'tail']' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  result.loc[result['obj_class']==k, ['part_labels']] = result.loc[result['obj_class']==k,['part_labels']].replace(v).copy()\n"
     ]
    }
   ],
   "source": [
    "#testing loop\n",
    "model_path = ('/Users/amrutamuthal/Documents/training_runs/model/'+run_prefix+'/Obj-Box-'\n",
    "                        +str(learning_rate)\n",
    "                        +'-batch-'+str(batch_size)\n",
    "                        +'-h1-'+str(hidden1)\n",
    "                        +'-h2-'+str(hidden2)\n",
    "                        +'-h3-'+str(hidden3)+'-test')\n",
    "summary_path = ('/Users/amrutamuthal/Documents/training_runs/run/'+run_prefix+'/Obj-Box-'\n",
    "                        +str(learning_rate)\n",
    "                        +'-batch-'+str(batch_size)\n",
    "                        +'-h1-'+str(hidden1)\n",
    "                        +'-h2-'+str(hidden2)\n",
    "                        +'-h3-'+str(hidden3)+'-test')\n",
    "\n",
    "\n",
    "write_tensorboard = False\n",
    "if write_tensorboard:\n",
    "    writer = SummaryWriter(summary_path)\n",
    "\n",
    "vae = TwoStageAutoEncoder(latent_dims,\n",
    "                          num_nodes,\n",
    "                          bbx_size,\n",
    "                          num_classes,\n",
    "                          label_shape,\n",
    "                          hidden1,\n",
    "                          hidden2,\n",
    "                          hidden3,\n",
    "                          dense_hidden1,\n",
    "                          dense_hidden2,\n",
    "                          adaptive_margin,\n",
    "                          output_log,\n",
    "                          area_encoding,\n",
    "                          coupling,\n",
    "                          obj_bbx_conditioning,\n",
    "                          use_fft_on_bbx,\n",
    "                          use_gcn_in_decoder,\n",
    "                        )\n",
    "\n",
    "vae.load_state_dict(torch.load(model_path+ '/model_weights.pth'))\n",
    "\n",
    "# decoder = vae.decoder\n",
    "image_shape = [num_nodes, bbx_size]\n",
    "global_step = 250000\n",
    "class_dict = {0:'cow', 1:'sheep', 2:'bird', 3:'person', 4:'cat', 5:'dog', 6:'horse'}\n",
    "res_dfs = []\n",
    "for i, val_data in enumerate(batch_val_loader, 0):\n",
    "    \n",
    "    val_data\n",
    "    node_data_true = val_data.x\n",
    "    label_true = node_data_true[:,:1]\n",
    "    y_true = val_data.y\n",
    "    class_true = y_true[:, :num_classes]\n",
    "    X_obj_true = y_true[:, num_classes:]\n",
    "    X_obj_true_transformed = X_obj_true[:,2:]-X_obj_true[:,:2]\n",
    "    adj_true = val_data.edge_index\n",
    "    class_true  = torch.flatten(class_true)\n",
    "    \n",
    "    output = vae(adj_true, node_data_true, X_obj_true_transformed, label_true , class_true, variational, coupling, refine_iter=2)\n",
    "    node_data_pred_test = output[0]\n",
    "    node_data_pred_test_refined = output[8]\n",
    "    X_obj_pred_test = output[1]\n",
    "#     node_data_pred_test, X_obj_pred_test, label_pred, z_mean, z_logvar, margin = output\n",
    "    X_obj_pred_test = torch.cat(((torch.tensor([1.0])-X_obj_pred_test)*torch.tensor([0.5]),\n",
    "                                (torch.tensor([1.0])+X_obj_pred_test)*torch.tensor([0.5])),\n",
    "                              -1)\n",
    "    res_dfs.append(metrics.get_metrics(node_data_true, X_obj_true, node_data_pred_test_refined,\n",
    "                               X_obj_pred_test,\n",
    "                               label_true,\n",
    "                               class_true,\n",
    "                               num_nodes,\n",
    "                               num_classes))\n",
    "    \n",
    "    if write_tensorboard:\n",
    "        \n",
    "        for j in range(int(len(node_data_true)/num_nodes)):\n",
    "            \n",
    "            image = plot_bbx(node_data_true[j].detach().to(\"cpu\").numpy().astype(float))/255\n",
    "            pred_image = plot_bbx(node_data_pred_test[j].detach().to(\"cpu\").numpy()).astype(float)/255\n",
    "\n",
    "            writer.add_image('test_result/images/'+str(j)+'-input/', image, global_step, dataformats='HWC')  \n",
    "            writer.add_image('test_result/images/'+str(j)+'-generated/', pred_image, global_step, dataformats='HWC')\n",
    "            \n",
    "\n",
    "result = pd.concat(res_dfs)\n",
    "result['obj_class'] = np.where(result['obj_class'].isna(), 0, result['obj_class'])\n",
    "result['obj_class'] = result['obj_class'].astype('int')\n",
    "result['obj_class'].replace(class_dict, inplace=True)\n",
    "result.where(result['part_labels']!=0, np.NaN, inplace=True)\n",
    "result['part_labels'] = np.where(result['part_labels'].isna(), 0, result['part_labels'])\n",
    "result['part_labels'] = result['part_labels'].astype('int')\n",
    "result['id'] = np.repeat(np.array(list(range(int(len(result)/num_nodes)))), 16)\n",
    "\n",
    "if write_tensorboard:\n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "\n",
    "bird_labels = {'head':1 , 'torso':2, 'neck':3, 'lwing':4, 'rwing':5, 'lleg':6, 'lfoot':7, 'rleg':8, 'rfoot':9, 'tail':10}\n",
    "cat_labels = {'head':1, 'torso':2, 'neck':3, 'lfleg':4, 'lfpa':5, 'rfleg':6, 'rfpa':7, 'lbleg':8, 'lbpa':9, 'rbleg':10, 'rbpa':11, 'tail':12}\n",
    "cow_labels = {'head':1,'lhorn':2, 'rhorn':3, 'torso':4, 'neck':5, 'lfuleg':6, 'lflleg':7, 'rfuleg':8, 'rflleg':9, 'lbuleg':10, 'lblleg':11, 'rbuleg':12, 'rblleg':13, 'tail':14}\n",
    "dog_labels = {'head':1,'torso':2, 'neck':3, 'lfleg':4, 'lfpa':5, 'rfleg':6, 'rfpa':7, 'lbleg':8, 'lbpa':9, 'rbleg':10, 'rbpa':11, 'tail':12, 'muzzle':13}\n",
    "horse_labels = {'head':1,'lfho':2, 'rfho':3, 'torso':4, 'neck':5, 'lfuleg':6, 'lflleg':7, 'rfuleg':8, 'rflleg':9, 'lbuleg':10, 'lblleg':11, 'rbuleg':12, 'rblleg':13, 'tail':14, 'lbho':15, 'rbho':16}\n",
    "person_labels = {'head':1, 'torso':2, 'neck': 3, 'llarm': 4, 'luarm': 5, 'lhand': 6, 'rlarm':7, 'ruarm':8, 'rhand': 9, 'llleg': 10, 'luleg':11, 'lfoot':12, 'rlleg':13, 'ruleg':14, 'rfoot':15}\n",
    "sheep_labels = cow_labels\n",
    "part_labels_combined_parts = {'bird': bird_labels, 'cat': cat_labels, 'cow': cow_labels, 'dog': dog_labels, 'sheep': sheep_labels, 'horse':horse_labels,'person':person_labels}\n",
    "\n",
    "for k, v in part_labels_combined_parts.items():\n",
    "    new_map = {}\n",
    "    for pk, pv in v.items():\n",
    "        new_map[pv]=pk\n",
    "    part_labels_combined_parts[k] = new_map\n",
    "    \n",
    "for k, v in part_labels_combined_parts.items():\n",
    "    result.loc[result['obj_class']==k, ['part_labels']] = result.loc[result['obj_class']==k,['part_labels']].replace(v).copy()\n",
    "\n",
    "# rel_metrics.csv')sult.to_csv(model_path+'/raw_metrics.csv')\n",
    "# res_obj_level = result.groupby(['obj_class', 'id']).mean().reset_index()\n",
    "# res_obj_level.groupby(['obj_class']).mean().reset_index()[['obj_class', 'IOU', 'MSE']].to_csv(model_path+'/obj_level_metrics.csv')\n",
    "# result.groupby(['obj_class', 'part_labels']).mean().reset_index()[['obj_class', 'part_labels',  'IOU', 'MSE']].to_csv(model_path+'/part_leve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78220c71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>obj_class</th>\n",
       "      <th>part_labels</th>\n",
       "      <th>IOU</th>\n",
       "      <th>MSE</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bird</td>\n",
       "      <td>head</td>\n",
       "      <td>0.562659</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>3336.324104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bird</td>\n",
       "      <td>lfoot</td>\n",
       "      <td>0.229764</td>\n",
       "      <td>0.000377</td>\n",
       "      <td>3333.079602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bird</td>\n",
       "      <td>lleg</td>\n",
       "      <td>0.388557</td>\n",
       "      <td>0.000338</td>\n",
       "      <td>3367.192810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bird</td>\n",
       "      <td>lwing</td>\n",
       "      <td>0.475419</td>\n",
       "      <td>0.000725</td>\n",
       "      <td>3406.471092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bird</td>\n",
       "      <td>neck</td>\n",
       "      <td>0.510384</td>\n",
       "      <td>0.000362</td>\n",
       "      <td>3236.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>sheep</td>\n",
       "      <td>lhorn</td>\n",
       "      <td>0.246529</td>\n",
       "      <td>0.000673</td>\n",
       "      <td>3578.155405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>sheep</td>\n",
       "      <td>neck</td>\n",
       "      <td>0.650131</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>3469.604167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>sheep</td>\n",
       "      <td>rhorn</td>\n",
       "      <td>0.290096</td>\n",
       "      <td>0.000686</td>\n",
       "      <td>3459.985915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>sheep</td>\n",
       "      <td>tail</td>\n",
       "      <td>0.231392</td>\n",
       "      <td>0.001076</td>\n",
       "      <td>3548.680769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>sheep</td>\n",
       "      <td>torso</td>\n",
       "      <td>0.832399</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>3399.291411</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>86 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   obj_class part_labels       IOU       MSE           id\n",
       "0       bird        head  0.562659  0.000200  3336.324104\n",
       "1       bird       lfoot  0.229764  0.000377  3333.079602\n",
       "2       bird        lleg  0.388557  0.000338  3367.192810\n",
       "3       bird       lwing  0.475419  0.000725  3406.471092\n",
       "4       bird        neck  0.510384  0.000362  3236.571429\n",
       "..       ...         ...       ...       ...          ...\n",
       "81     sheep       lhorn  0.246529  0.000673  3578.155405\n",
       "82     sheep        neck  0.650131  0.000176  3469.604167\n",
       "83     sheep       rhorn  0.290096  0.000686  3459.985915\n",
       "84     sheep        tail  0.231392  0.001076  3548.680769\n",
       "85     sheep       torso  0.832399  0.000156  3399.291411\n",
       "\n",
       "[86 rows x 5 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[['obj_class', 'part_labels', 'IOU', 'MSE', 'id']].groupby(['obj_class', 'part_labels']).mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02e6f6eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>obj_class</th>\n",
       "      <th>IOU</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bird</td>\n",
       "      <td>0.542668</td>\n",
       "      <td>0.000299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cat</td>\n",
       "      <td>0.604837</td>\n",
       "      <td>0.000918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cow</td>\n",
       "      <td>0.556325</td>\n",
       "      <td>0.000330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dog</td>\n",
       "      <td>0.479702</td>\n",
       "      <td>0.000749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>horse</td>\n",
       "      <td>0.412508</td>\n",
       "      <td>0.001222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>person</td>\n",
       "      <td>0.503938</td>\n",
       "      <td>0.000692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sheep</td>\n",
       "      <td>0.656498</td>\n",
       "      <td>0.000299</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  obj_class       IOU       MSE\n",
       "0      bird  0.542668  0.000299\n",
       "1       cat  0.604837  0.000918\n",
       "2       cow  0.556325  0.000330\n",
       "3       dog  0.479702  0.000749\n",
       "4     horse  0.412508  0.001222\n",
       "5    person  0.503938  0.000692\n",
       "6     sheep  0.656498  0.000299"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_obj_level = result[['obj_class', 'IOU', 'MSE', 'id']].groupby(['obj_class', 'id']).mean().reset_index()\n",
    "res_obj_level.groupby(['obj_class']).mean().reset_index()[['obj_class', 'IOU', 'MSE']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d9367e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(X_obj, part_decoder, obj_class, nodes, batch_size, latent_dims):\n",
    "    \n",
    "    nodes = torch.reshape(nodes,(batch_size,part_decoder.num_nodes))\n",
    "    obj_class = torch.reshape(obj_class, \n",
    "                              (batch_size, num_classes))\n",
    "    \n",
    "    # obj sampling\n",
    "    z_latent_obj = X_obj\n",
    "    \n",
    "    print(z_latent_obj.shape, obj_class.shape)\n",
    "    conditioned_obj_latent = torch.cat([obj_class, z_latent_obj],dim=-1)\n",
    "    conditioned_obj_latent = torch.cat([nodes, conditioned_obj_latent],dim=-1)\n",
    "\n",
    "    # part sampling\n",
    "    z_latent_part = torch.normal(torch.zeros([batch_size,latent_dims]))\n",
    "    conditioned_part_latent = torch.cat([conditioned_obj_latent, z_latent_part],dim=-1)\n",
    "    \n",
    "    x_bbx, _, _, _ = part_decoder(conditioned_part_latent)\n",
    "    \n",
    "    return x_bbx, X_obj\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c39d671",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#testing loop\n",
    "model_path = ('D:/meronym_data/model/'+run_prefix+'/Obj-Box-'\n",
    "                        +str(learning_rate)\n",
    "                        +'-batch-'+str(batch_size)\n",
    "                        +'-h1-'+str(hidden1)\n",
    "                        +'-h2-'+str(hidden2)\n",
    "                        +'-h3-'+str(hidden3)+'-test')\n",
    "summary_path = ('D:/meronym_data/runs/'+run_prefix+'/Obj-Box-'\n",
    "                        +str(learning_rate)\n",
    "                        +'-batch-'+str(batch_size)\n",
    "                        +'-h1-'+str(hidden1)\n",
    "                        +'-h2-'+str(hidden2)\n",
    "                        +'-h3-'+str(hidden3)+'-test')\n",
    "\n",
    "\n",
    "write_tensorboard = True\n",
    "if write_tensorboard:\n",
    "    writer = SummaryWriter(summary_path)\n",
    "\n",
    "vae = TwoStageAutoEncoder(latent_dims,\n",
    "                          num_nodes,\n",
    "                          bbx_size,\n",
    "                          num_classes,\n",
    "                          label_shape,\n",
    "                          hidden1,\n",
    "                          hidden2,\n",
    "                          hidden3,\n",
    "                          dense_hidden1,\n",
    "                          dense_hidden2,\n",
    "                          adaptive_margin,\n",
    "                          output_log,\n",
    "                          area_encoding,\n",
    "                          coupling,\n",
    "                          obj_bbx_conditioning,\n",
    "                        )\n",
    "\n",
    "\n",
    "vae.load_state_dict(torch.load(model_path+ '/model_weights.pth'))\n",
    "\n",
    "part_decoder = vae.gcn_decoder\n",
    "image_shape = [num_nodes, bbx_size]\n",
    "global_step = 250000\n",
    "class_dict = {0:'cow', 1:'sheep', 2:'bird', 3:'person', 4:'cat', 5:'dog', 6:'horse'}\n",
    "res_dfs = []\n",
    "for i, val_data in enumerate(batch_val_loader, 0):\n",
    "    \n",
    "    val_data\n",
    "    node_data_true = val_data.x\n",
    "    label_true = node_data_true[:,:1]\n",
    "    y_true = val_data.y\n",
    "    class_true = y_true[:, :num_classes]\n",
    "    X_obj_true = y_true[:, num_classes:]\n",
    "    adj_true = val_data.edge_index\n",
    "    class_true  = torch.flatten(class_true)\n",
    "    \n",
    "    output = inference(X_obj_true, part_decoder, class_true, label_true, batch_size, latent_dims)\n",
    "    node_data_pred_test = output[0]\n",
    "    X_obj_pred_test = output[1]\n",
    "#     node_data_pred_test, X_obj_pred_test, label_pred, z_mean, z_logvar, margin = output\n",
    "    \n",
    "#     res_dfs.append(metrics.get_metrics(node_data_true, X_obj_true, node_data_pred_test,\n",
    "#                                X_obj_pred_test,\n",
    "#                                label_true,\n",
    "#                                class_true,\n",
    "#                                num_nodes,\n",
    "#                                num_classes))\n",
    "    \n",
    "    if write_tensorboard:\n",
    "        \n",
    "        for j in range(int(len(node_data_true)/num_nodes)):\n",
    "            image = plot_utils.plot_bbx((node_data_true[num_nodes*j:num_nodes*(j+1)\n",
    "                                                       ,1:5]*label_true[num_nodes*j:num_nodes*(j+1)]).detach().to(\"cpu\").numpy().astype(float))/255\n",
    "            pred_image = plot_utils.plot_bbx((node_data_pred_test[j]*label_true[num_nodes*j:num_nodes*(j+1)]).detach().to(\"cpu\").numpy()).astype(float)/255\n",
    "\n",
    "            writer.add_image('test_result/images/'+str(j)+'-input/', image, global_step, dataformats='HWC')  \n",
    "            writer.add_image('test_result/images/'+str(j)+'-generated/', pred_image, global_step, dataformats='HWC')\n",
    "            \n",
    "\n",
    "result = pd.concat(res_dfs)\n",
    "result['obj_class'] = np.where(result['obj_class'].isna(), 0, result['obj_class'])\n",
    "result['obj_class'] = result['obj_class'].astype('int')\n",
    "result['obj_class'].replace(class_dict, inplace=True)\n",
    "result.where(result['part_labels']!=0, np.NaN, inplace=True)\n",
    "result['part_labels'] = np.where(result['part_labels'].isna(), 0, result['part_labels'])\n",
    "result['part_labels'] = result['part_labels'].astype('int')\n",
    "result['id'] = np.repeat(np.array(list(range(int(len(result)/num_nodes)))), 16)\n",
    "\n",
    "if write_tensorboard:\n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "\n",
    "bird_labels = {'head':1 , 'torso':2, 'neck':3, 'lwing':4, 'rwing':5, 'lleg':6, 'lfoot':7, 'rleg':8, 'rfoot':9, 'tail':10}\n",
    "cat_labels = {'head':1, 'torso':2, 'neck':3, 'lfleg':4, 'lfpa':5, 'rfleg':6, 'rfpa':7, 'lbleg':8, 'lbpa':9, 'rbleg':10, 'rbpa':11, 'tail':12}\n",
    "cow_labels = {'head':1,'lhorn':2, 'rhorn':3, 'torso':4, 'neck':5, 'lfuleg':6, 'lflleg':7, 'rfuleg':8, 'rflleg':9, 'lbuleg':10, 'lblleg':11, 'rbuleg':12, 'rblleg':13, 'tail':14}\n",
    "dog_labels = {'head':1,'torso':2, 'neck':3, 'lfleg':4, 'lfpa':5, 'rfleg':6, 'rfpa':7, 'lbleg':8, 'lbpa':9, 'rbleg':10, 'rbpa':11, 'tail':12, 'muzzle':13}\n",
    "horse_labels = {'head':1,'lfho':2, 'rfho':3, 'torso':4, 'neck':5, 'lfuleg':6, 'lflleg':7, 'rfuleg':8, 'rflleg':9, 'lbuleg':10, 'lblleg':11, 'rbuleg':12, 'rblleg':13, 'tail':14, 'lbho':15, 'rbho':16}\n",
    "person_labels = {'head':1, 'torso':2, 'neck': 3, 'llarm': 4, 'luarm': 5, 'lhand': 6, 'rlarm':7, 'ruarm':8, 'rhand': 9, 'llleg': 10, 'luleg':11, 'lfoot':12, 'rlleg':13, 'ruleg':14, 'rfoot':15}\n",
    "sheep_labels = cow_labels\n",
    "part_labels_combined_parts = {'bird': bird_labels, 'cat': cat_labels, 'cow': cow_labels, 'dog': dog_labels, 'sheep': sheep_labels, 'horse':horse_labels,'person':person_labels}\n",
    "\n",
    "for k, v in part_labels_combined_parts.items():\n",
    "    new_map = {}\n",
    "    for pk, pv in v.items():\n",
    "        new_map[pv]=pk\n",
    "    part_labels_combined_parts[k] = new_map\n",
    "    \n",
    "for k, v in part_labels_combined_parts.items():\n",
    "    result.loc[result['obj_class']==k, ['part_labels']] = result.loc[result['obj_class']==k,['part_labels']].replace(v).copy()\n",
    "\n",
    "result.to_csv(model_path+'/raw_metrics.csv')\n",
    "res_obj_level = result.groupby(['obj_class', 'id']).mean().reset_index()\n",
    "res_obj_level.groupby(['obj_class']).mean().reset_index()[['obj_class', 'IOU', 'MSE']].to_csv(model_path+'/obj_level_metrics.csv')\n",
    "result.groupby(['obj_class', 'part_labels']).mean().reset_index()[['obj_class', 'part_labels',  'IOU', 'MSE']].to_csv(model_path+'/part_level_metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a379c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = ('D:/meronym_data/model/'+run_prefix+'/Obj-Box-'\n",
    "                        +str(learning_rate)\n",
    "                        +'-batch-'+str(batch_size)\n",
    "                        +'-h1-'+str(hidden1)\n",
    "                        +'-h2-'+str(hidden2)\n",
    "                        +'-h3-'+str(hidden3)+'-test')\n",
    "summary_path = ('D:/meronym_data/runs/'+run_prefix+'/Obj-Box-'\n",
    "                        +str(learning_rate)\n",
    "                        +'-batch-'+str(batch_size)\n",
    "                        +'-h1-'+str(hidden1)\n",
    "                        +'-h2-'+str(hidden2)\n",
    "                        +'-h3-'+str(hidden3)+'-test')\n",
    "\n",
    "result = pd.read_csv(model_path+'/raw_metrics.csv')\n",
    "result.groupby(['obj_class', 'part_labels']).mean().reset_index()[['obj_class', 'part_labels',\n",
    "                                                                   'IOU', 'MSE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2d2cbb0",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "agg function failed [how->mean,dtype->object]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/groupby/groupby.py:1942\u001b[0m, in \u001b[0;36mGroupBy._agg_py_fallback\u001b[0;34m(self, how, values, ndim, alt)\u001b[0m\n\u001b[1;32m   1941\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1942\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_grouper\u001b[38;5;241m.\u001b[39magg_series(ser, alt, preserve_dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1943\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/groupby/ops.py:864\u001b[0m, in \u001b[0;36mBaseGrouper.agg_series\u001b[0;34m(self, obj, func, preserve_dtype)\u001b[0m\n\u001b[1;32m    862\u001b[0m     preserve_dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 864\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aggregate_series_pure_python(obj, func)\n\u001b[1;32m    866\u001b[0m npvalues \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mmaybe_convert_objects(result, try_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/groupby/ops.py:885\u001b[0m, in \u001b[0;36mBaseGrouper._aggregate_series_pure_python\u001b[0;34m(self, obj, func)\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(splitter):\n\u001b[0;32m--> 885\u001b[0m     res \u001b[38;5;241m=\u001b[39m func(group)\n\u001b[1;32m    886\u001b[0m     res \u001b[38;5;241m=\u001b[39m extract_result(res)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/groupby/groupby.py:2454\u001b[0m, in \u001b[0;36mGroupBy.mean.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   2451\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2452\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cython_agg_general(\n\u001b[1;32m   2453\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m-> 2454\u001b[0m         alt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: Series(x, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mmean(numeric_only\u001b[38;5;241m=\u001b[39mnumeric_only),\n\u001b[1;32m   2455\u001b[0m         numeric_only\u001b[38;5;241m=\u001b[39mnumeric_only,\n\u001b[1;32m   2456\u001b[0m     )\n\u001b[1;32m   2457\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroupby\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/series.py:6549\u001b[0m, in \u001b[0;36mSeries.mean\u001b[0;34m(self, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m   6541\u001b[0m \u001b[38;5;129m@doc\u001b[39m(make_doc(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m, ndim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m   6542\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmean\u001b[39m(\n\u001b[1;32m   6543\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   6547\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   6548\u001b[0m ):\n\u001b[0;32m-> 6549\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m NDFrame\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;28mself\u001b[39m, axis, skipna, numeric_only, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/generic.py:12420\u001b[0m, in \u001b[0;36mNDFrame.mean\u001b[0;34m(self, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m  12413\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmean\u001b[39m(\n\u001b[1;32m  12414\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m  12415\u001b[0m     axis: Axis \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  12418\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m  12419\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[0;32m> 12420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stat_function(\n\u001b[1;32m  12421\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m, nanops\u001b[38;5;241m.\u001b[39mnanmean, axis, skipna, numeric_only, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m  12422\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/generic.py:12377\u001b[0m, in \u001b[0;36mNDFrame._stat_function\u001b[0;34m(self, name, func, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m  12375\u001b[0m validate_bool_kwarg(skipna, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskipna\u001b[39m\u001b[38;5;124m\"\u001b[39m, none_allowed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m> 12377\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reduce(\n\u001b[1;32m  12378\u001b[0m     func, name\u001b[38;5;241m=\u001b[39mname, axis\u001b[38;5;241m=\u001b[39maxis, skipna\u001b[38;5;241m=\u001b[39mskipna, numeric_only\u001b[38;5;241m=\u001b[39mnumeric_only\n\u001b[1;32m  12379\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/series.py:6457\u001b[0m, in \u001b[0;36mSeries._reduce\u001b[0;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[0m\n\u001b[1;32m   6453\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   6454\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeries.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not allow \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwd_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnumeric_only\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   6455\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith non-numeric dtypes.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   6456\u001b[0m     )\n\u001b[0;32m-> 6457\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op(delegate, skipna\u001b[38;5;241m=\u001b[39mskipna, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/nanops.py:147\u001b[0m, in \u001b[0;36mbottleneck_switch.__call__.<locals>.f\u001b[0;34m(values, axis, skipna, **kwds)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 147\u001b[0m     result \u001b[38;5;241m=\u001b[39m alt(values, axis\u001b[38;5;241m=\u001b[39maxis, skipna\u001b[38;5;241m=\u001b[39mskipna, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/nanops.py:404\u001b[0m, in \u001b[0;36m_datetimelike_compat.<locals>.new_func\u001b[0;34m(values, axis, skipna, mask, **kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m     mask \u001b[38;5;241m=\u001b[39m isna(values)\n\u001b[0;32m--> 404\u001b[0m result \u001b[38;5;241m=\u001b[39m func(values, axis\u001b[38;5;241m=\u001b[39maxis, skipna\u001b[38;5;241m=\u001b[39mskipna, mask\u001b[38;5;241m=\u001b[39mmask, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m datetimelike:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/nanops.py:720\u001b[0m, in \u001b[0;36mnanmean\u001b[0;34m(values, axis, skipna, mask)\u001b[0m\n\u001b[1;32m    719\u001b[0m the_sum \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39msum(axis, dtype\u001b[38;5;241m=\u001b[39mdtype_sum)\n\u001b[0;32m--> 720\u001b[0m the_sum \u001b[38;5;241m=\u001b[39m _ensure_numeric(the_sum)\n\u001b[1;32m    722\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(the_sum, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/nanops.py:1701\u001b[0m, in \u001b[0;36m_ensure_numeric\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1699\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   1700\u001b[0m     \u001b[38;5;66;03m# GH#44008, GH#36703 avoid casting e.g. strings to numeric\u001b[39;00m\n\u001b[0;32m-> 1701\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not convert string \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to numeric\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1702\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: Could not convert string 'headtorsoneck' to numeric",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m res_obj_level \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mgroupby([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobj_class\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[1;32m      2\u001b[0m res_obj_level\u001b[38;5;241m.\u001b[39mgroupby([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobj_class\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mreset_index()[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobj_class\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIOU\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMSE\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/groupby/groupby.py:2452\u001b[0m, in \u001b[0;36mGroupBy.mean\u001b[0;34m(self, numeric_only, engine, engine_kwargs)\u001b[0m\n\u001b[1;32m   2445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_numba_agg_general(\n\u001b[1;32m   2446\u001b[0m         grouped_mean,\n\u001b[1;32m   2447\u001b[0m         executor\u001b[38;5;241m.\u001b[39mfloat_dtype_mapping,\n\u001b[1;32m   2448\u001b[0m         engine_kwargs,\n\u001b[1;32m   2449\u001b[0m         min_periods\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   2450\u001b[0m     )\n\u001b[1;32m   2451\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2452\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cython_agg_general(\n\u001b[1;32m   2453\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2454\u001b[0m         alt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: Series(x, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mmean(numeric_only\u001b[38;5;241m=\u001b[39mnumeric_only),\n\u001b[1;32m   2455\u001b[0m         numeric_only\u001b[38;5;241m=\u001b[39mnumeric_only,\n\u001b[1;32m   2456\u001b[0m     )\n\u001b[1;32m   2457\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroupby\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/groupby/groupby.py:1998\u001b[0m, in \u001b[0;36mGroupBy._cython_agg_general\u001b[0;34m(self, how, alt, numeric_only, min_count, **kwargs)\u001b[0m\n\u001b[1;32m   1995\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agg_py_fallback(how, values, ndim\u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39mndim, alt\u001b[38;5;241m=\u001b[39malt)\n\u001b[1;32m   1996\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m-> 1998\u001b[0m new_mgr \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mgrouped_reduce(array_func)\n\u001b[1;32m   1999\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_agged_manager(new_mgr)\n\u001b[1;32m   2000\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m how \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midxmin\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midxmax\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/internals/managers.py:1469\u001b[0m, in \u001b[0;36mBlockManager.grouped_reduce\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m   1465\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m blk\u001b[38;5;241m.\u001b[39mis_object:\n\u001b[1;32m   1466\u001b[0m     \u001b[38;5;66;03m# split on object-dtype blocks bc some columns may raise\u001b[39;00m\n\u001b[1;32m   1467\u001b[0m     \u001b[38;5;66;03m#  while others do not.\u001b[39;00m\n\u001b[1;32m   1468\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sb \u001b[38;5;129;01min\u001b[39;00m blk\u001b[38;5;241m.\u001b[39m_split():\n\u001b[0;32m-> 1469\u001b[0m         applied \u001b[38;5;241m=\u001b[39m sb\u001b[38;5;241m.\u001b[39mapply(func)\n\u001b[1;32m   1470\u001b[0m         result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[1;32m   1471\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/internals/blocks.py:393\u001b[0m, in \u001b[0;36mBlock.apply\u001b[0;34m(self, func, **kwargs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Block]:\n\u001b[1;32m    389\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;124;03m    apply the function to my values; return a block if we are not\u001b[39;00m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;124;03m    one\u001b[39;00m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 393\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    395\u001b[0m     result \u001b[38;5;241m=\u001b[39m maybe_coerce_values(result)\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split_op_result(result)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/groupby/groupby.py:1995\u001b[0m, in \u001b[0;36mGroupBy._cython_agg_general.<locals>.array_func\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m   1992\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m   1994\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m alt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1995\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agg_py_fallback(how, values, ndim\u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39mndim, alt\u001b[38;5;241m=\u001b[39malt)\n\u001b[1;32m   1996\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/groupby/groupby.py:1946\u001b[0m, in \u001b[0;36mGroupBy._agg_py_fallback\u001b[0;34m(self, how, values, ndim, alt)\u001b[0m\n\u001b[1;32m   1944\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magg function failed [how->\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhow\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,dtype->\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mser\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1945\u001b[0m     \u001b[38;5;66;03m# preserve the kind of exception that raised\u001b[39;00m\n\u001b[0;32m-> 1946\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(err)(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   1948\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ser\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[1;32m   1949\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m res_values\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: agg function failed [how->mean,dtype->object]"
     ]
    }
   ],
   "source": [
    "res_obj_level = result.groupby(['obj_class', 'id']).mean().reset_index()\n",
    "res_obj_level.groupby(['obj_class']).mean().reset_index()[['obj_class', 'IOU', 'MSE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a0a03e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IOU    0.491406\n",
       "MSE    0.001316\n",
       "dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_obj_level[['IOU', 'MSE']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e231a317",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_obj_pred[:10,2:]-X_obj_pred[:10,:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90832eb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_obj_true[:10, 2:]-X_obj_true[:10, :2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b789243a",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_obj_level.to_csv(model_path+'/obj_metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838534e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7e5b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = 'D:/meronym_data/generate_boxes.npy'\n",
    "with open(outfile, 'wb') as pickle_file:\n",
    "    pred_boxes = np.concatenate(pred_boxes)\n",
    "    pickle.dump(pred_boxes, pickle_file)\n",
    "outfile = 'D:/meronym_data/generate_boxesobj_class.npy'\n",
    "with open(outfile, 'wb') as pickle_file:\n",
    "    pickle.dump(classes,pickle_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
