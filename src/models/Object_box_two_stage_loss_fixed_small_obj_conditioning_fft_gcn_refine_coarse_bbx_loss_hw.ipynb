{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cfd1439",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5671166e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_LAUNCH_BLOCKING=1\n"
     ]
    }
   ],
   "source": [
    "%set_env CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd5cb46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21434d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import collections\n",
    "import cv2\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e88c7fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "from torch_geometric.data import DataLoader\n",
    "import torch.utils.data as data_utils\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4e46b5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from losses import BoxVAE_losses as loss\n",
    "from evaluation import metrics\n",
    "from utils import plot_utils\n",
    "from utils import data_utils as data_loading\n",
    "from components.DenseAutoencoder import DenseAutoencoder\n",
    "from components.DenseAutoencoder import Decoder\n",
    "from components.TwoStageAutoEncoder import TwoStageAutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d19349cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b59f5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mask_generation import masked_sketch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9cd8d4a6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "latent_dims = 64\n",
    "num_nodes = 16\n",
    "bbx_size = 4\n",
    "num_classes = 7\n",
    "label_shape = 1\n",
    "nb_epochs = 1000\n",
    "klw = loss.frange_cycle_linear(nb_epochs)\n",
    "learning_rate = 0.0002\n",
    "hidden1 = 32\n",
    "hidden2 = 16\n",
    "hidden3 = 128\n",
    "dense_hidden1=8\n",
    "dense_hidden2=4\n",
    "adaptive_margin = True\n",
    "fine_tune_box = False\n",
    "output_log = False\n",
    "area_encoding = False\n",
    "run_prefix = \"two_stage_small_obj_conditioning_fft_gcn_reccurent_refine_coarse_bbx_loss_hw\"\n",
    "variational=False\n",
    "coupling = True\n",
    "obj_bbx_conditioning = True\n",
    "use_fft_on_bbx = True\n",
    "use_gcn_in_decoder = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d7f3f906",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "batch_train_loader, batch_val_loader = data_loading.load_data(obj_data_postfix = '_obj_boundary',\n",
    "                                                              part_data_post_fix = '_scaled',\n",
    "                                                              file_postfix = '_combined',\n",
    "                                                              seed=345,\n",
    "                                                              batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "361b90d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73f4529b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e573aa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "del vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5d41ca05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "[1,   399] loss: 352.657\n",
      "200\n",
      "[2,   399] loss: 202.959\n",
      "200\n",
      "[3,   399] loss: 189.408\n",
      "200\n",
      "[4,   399] loss: 180.985\n",
      "200\n",
      "[5,   399] loss: 173.102\n",
      "200\n",
      "[6,   399] loss: 165.697\n",
      "200\n",
      "[7,   399] loss: 158.846\n",
      "200\n",
      "[8,   399] loss: 152.590\n",
      "200\n",
      "[9,   399] loss: 146.753\n",
      "200\n",
      "[10,   399] loss: 141.461\n",
      "200\n",
      "[11,   399] loss: 136.712\n",
      "200\n",
      "[12,   399] loss: 132.273\n",
      "200\n",
      "[13,   399] loss: 127.930\n",
      "200\n",
      "[14,   399] loss: 123.356\n",
      "200\n",
      "[15,   399] loss: 118.625\n",
      "200\n",
      "[16,   399] loss: 113.681\n",
      "200\n",
      "[17,   399] loss: 108.161\n",
      "200\n",
      "[18,   399] loss: 102.804\n",
      "200\n",
      "[19,   399] loss: 97.799\n",
      "200\n",
      "[20,   399] loss: 92.226\n",
      "200\n",
      "[21,   399] loss: 86.462\n",
      "200\n",
      "[22,   399] loss: 79.794\n",
      "200\n",
      "[23,   399] loss: 72.928\n",
      "200\n",
      "[24,   399] loss: 65.335\n",
      "200\n",
      "[25,   399] loss: 58.689\n",
      "200\n",
      "[26,   399] loss: 54.259\n",
      "200\n",
      "[27,   399] loss: 51.397\n",
      "200\n",
      "[28,   399] loss: 49.125\n",
      "200\n",
      "[29,   399] loss: 47.163\n",
      "200\n",
      "[30,   399] loss: 45.140\n",
      "200\n",
      "[31,   399] loss: 43.390\n",
      "200\n",
      "[32,   399] loss: 41.957\n",
      "200\n",
      "[33,   399] loss: 40.502\n",
      "200\n",
      "[34,   399] loss: 39.111\n",
      "200\n",
      "[35,   399] loss: 38.063\n",
      "200\n",
      "[36,   399] loss: 37.153\n",
      "200\n",
      "[37,   399] loss: 36.092\n",
      "200\n",
      "[38,   399] loss: 35.151\n",
      "200\n",
      "[39,   399] loss: 34.365\n",
      "200\n",
      "[40,   399] loss: 33.621\n",
      "200\n",
      "[41,   399] loss: 32.898\n",
      "200\n",
      "[42,   399] loss: 32.191\n",
      "200\n",
      "[43,   399] loss: 31.454\n",
      "200\n",
      "[44,   399] loss: 30.606\n",
      "200\n",
      "[45,   399] loss: 29.741\n",
      "200\n",
      "[46,   399] loss: 29.004\n",
      "200\n",
      "[47,   399] loss: 28.350\n",
      "200\n",
      "[48,   399] loss: 27.770\n",
      "200\n",
      "[49,   399] loss: 27.239\n",
      "200\n",
      "[50,   399] loss: 26.748\n",
      "200\n",
      "[51,   399] loss: 26.287\n",
      "200\n",
      "[52,   399] loss: 26.317\n",
      "200\n",
      "[53,   399] loss: 25.842\n",
      "200\n",
      "[54,   399] loss: 25.405\n",
      "200\n",
      "[55,   399] loss: 24.962\n",
      "200\n",
      "[56,   399] loss: 24.513\n",
      "200\n",
      "[57,   399] loss: 24.120\n",
      "200\n",
      "[58,   399] loss: 23.762\n",
      "200\n",
      "[59,   399] loss: 23.429\n",
      "200\n",
      "[60,   399] loss: 23.113\n",
      "200\n",
      "[61,   399] loss: 22.811\n",
      "200\n",
      "[62,   399] loss: 22.520\n",
      "200\n",
      "[63,   399] loss: 22.242\n",
      "200\n",
      "[64,   399] loss: 21.972\n",
      "200\n",
      "[65,   399] loss: 21.713\n",
      "200\n",
      "[66,   399] loss: 21.463\n",
      "200\n",
      "[67,   399] loss: 21.222\n",
      "200\n",
      "[68,   399] loss: 20.988\n",
      "200\n",
      "[69,   399] loss: 20.760\n",
      "200\n",
      "[70,   399] loss: 20.536\n",
      "200\n",
      "[71,   399] loss: 20.318\n",
      "200\n",
      "[72,   399] loss: 20.106\n",
      "200\n",
      "[73,   399] loss: 19.899\n",
      "200\n",
      "[74,   399] loss: 19.697\n",
      "200\n",
      "[75,   399] loss: 19.499\n",
      "200\n",
      "[76,   399] loss: 19.306\n",
      "200\n",
      "[77,   399] loss: 19.115\n",
      "200\n",
      "[78,   399] loss: 18.928\n",
      "200\n",
      "[79,   399] loss: 18.742\n",
      "200\n",
      "[80,   399] loss: 18.554\n",
      "200\n",
      "[81,   399] loss: 18.361\n",
      "200\n",
      "[82,   399] loss: 18.171\n",
      "200\n",
      "[83,   399] loss: 17.988\n",
      "200\n",
      "[84,   399] loss: 17.804\n",
      "200\n",
      "[85,   399] loss: 17.621\n",
      "200\n",
      "[86,   399] loss: 17.447\n",
      "200\n",
      "[87,   399] loss: 17.278\n",
      "200\n",
      "[88,   399] loss: 17.114\n",
      "200\n",
      "[89,   399] loss: 16.953\n",
      "200\n",
      "[90,   399] loss: 16.796\n",
      "200\n",
      "[91,   399] loss: 16.642\n",
      "200\n",
      "[92,   399] loss: 16.491\n",
      "200\n",
      "[93,   399] loss: 16.342\n",
      "200\n",
      "[94,   399] loss: 16.196\n",
      "200\n",
      "[95,   399] loss: 16.054\n",
      "200\n",
      "[96,   399] loss: 15.914\n",
      "200\n",
      "[97,   399] loss: 15.778\n",
      "200\n",
      "[98,   399] loss: 15.645\n",
      "200\n",
      "[99,   399] loss: 15.516\n",
      "200\n",
      "[100,   399] loss: 15.390\n",
      "200\n",
      "[101,   399] loss: 15.265\n",
      "200\n",
      "[102,   399] loss: 15.145\n",
      "200\n",
      "[103,   399] loss: 15.024\n",
      "200\n",
      "[104,   399] loss: 14.907\n",
      "200\n",
      "[105,   399] loss: 14.795\n",
      "200\n",
      "[106,   399] loss: 14.684\n",
      "200\n",
      "[107,   399] loss: 14.574\n",
      "200\n",
      "[108,   399] loss: 14.465\n",
      "200\n",
      "[109,   399] loss: 14.361\n",
      "200\n",
      "[110,   399] loss: 14.258\n",
      "200\n",
      "[111,   399] loss: 14.157\n",
      "200\n",
      "[112,   399] loss: 14.059\n",
      "200\n",
      "[113,   399] loss: 13.962\n",
      "200\n",
      "[114,   399] loss: 13.867\n",
      "200\n",
      "[115,   399] loss: 13.774\n",
      "200\n",
      "[116,   399] loss: 13.683\n",
      "200\n",
      "[117,   399] loss: 13.592\n",
      "200\n",
      "[118,   399] loss: 13.505\n",
      "200\n",
      "[119,   399] loss: 13.417\n",
      "200\n",
      "[120,   399] loss: 13.332\n",
      "200\n",
      "[121,   399] loss: 13.248\n",
      "200\n",
      "[122,   399] loss: 13.164\n",
      "200\n",
      "[123,   399] loss: 13.079\n",
      "200\n",
      "[124,   399] loss: 12.998\n",
      "200\n",
      "[125,   399] loss: 12.917\n",
      "200\n",
      "[126,   399] loss: 12.837\n",
      "200\n",
      "[127,   399] loss: 12.757\n",
      "200\n",
      "[128,   399] loss: 12.683\n",
      "200\n",
      "[129,   399] loss: 12.607\n",
      "200\n",
      "[130,   399] loss: 12.532\n",
      "200\n",
      "[131,   399] loss: 12.456\n",
      "200\n",
      "[132,   399] loss: 12.383\n",
      "200\n",
      "[133,   399] loss: 12.313\n",
      "200\n",
      "[134,   399] loss: 12.242\n",
      "200\n",
      "[135,   399] loss: 12.174\n",
      "200\n",
      "[136,   399] loss: 12.104\n",
      "200\n",
      "[137,   399] loss: 12.036\n",
      "200\n",
      "[138,   399] loss: 11.965\n",
      "200\n",
      "[139,   399] loss: 11.895\n",
      "200\n",
      "[140,   399] loss: 11.822\n",
      "200\n",
      "[141,   399] loss: 11.753\n",
      "200\n",
      "[142,   399] loss: 11.687\n",
      "200\n",
      "[143,   399] loss: 11.621\n",
      "200\n",
      "[144,   399] loss: 11.556\n",
      "200\n",
      "[145,   399] loss: 11.492\n",
      "200\n",
      "[146,   399] loss: 11.428\n",
      "200\n",
      "[147,   399] loss: 11.366\n",
      "200\n",
      "[148,   399] loss: 11.306\n",
      "200\n",
      "[149,   399] loss: 11.245\n",
      "200\n",
      "[150,   399] loss: 11.187\n",
      "200\n",
      "[151,   399] loss: 11.126\n",
      "200\n",
      "[152,   399] loss: 11.065\n",
      "200\n",
      "[153,   399] loss: 11.007\n",
      "200\n",
      "[154,   399] loss: 10.949\n",
      "200\n",
      "[155,   399] loss: 10.891\n",
      "200\n",
      "[156,   399] loss: 10.833\n",
      "200\n",
      "[157,   399] loss: 10.778\n",
      "200\n",
      "[158,   399] loss: 10.723\n",
      "200\n",
      "[159,   399] loss: 10.668\n",
      "200\n",
      "[160,   399] loss: 10.613\n",
      "200\n",
      "[161,   399] loss: 10.559\n",
      "200\n",
      "[162,   399] loss: 10.505\n",
      "200\n",
      "[163,   399] loss: 10.454\n",
      "200\n",
      "[164,   399] loss: 10.400\n",
      "200\n",
      "[165,   399] loss: 10.348\n",
      "200\n",
      "[166,   399] loss: 10.296\n",
      "200\n",
      "[167,   399] loss: 10.245\n",
      "200\n",
      "[168,   399] loss: 10.193\n",
      "200\n",
      "[169,   399] loss: 10.144\n",
      "200\n",
      "[170,   399] loss: 10.094\n",
      "200\n",
      "[171,   399] loss: 10.046\n",
      "200\n",
      "[172,   399] loss: 9.998\n",
      "200\n",
      "[173,   399] loss: 9.949\n",
      "200\n",
      "[174,   399] loss: 9.901\n",
      "200\n",
      "[175,   399] loss: 9.852\n",
      "200\n",
      "[176,   399] loss: 9.804\n",
      "200\n",
      "[177,   399] loss: 9.755\n",
      "200\n",
      "[178,   399] loss: 9.709\n",
      "200\n",
      "[179,   399] loss: 9.662\n",
      "200\n",
      "[180,   399] loss: 9.615\n",
      "200\n",
      "[181,   399] loss: 9.568\n",
      "200\n",
      "[182,   399] loss: 9.522\n",
      "200\n",
      "[183,   399] loss: 9.477\n",
      "200\n",
      "[184,   399] loss: 9.433\n",
      "200\n",
      "[185,   399] loss: 9.389\n",
      "200\n",
      "[186,   399] loss: 9.344\n",
      "200\n",
      "[187,   399] loss: 9.302\n",
      "200\n",
      "[188,   399] loss: 9.258\n",
      "200\n",
      "[189,   399] loss: 9.214\n",
      "200\n",
      "[190,   399] loss: 9.171\n",
      "200\n",
      "[191,   399] loss: 9.129\n",
      "200\n",
      "[192,   399] loss: 9.087\n",
      "200\n",
      "[193,   399] loss: 9.045\n",
      "200\n",
      "[194,   399] loss: 9.002\n",
      "200\n",
      "[195,   399] loss: 8.961\n",
      "200\n",
      "[196,   399] loss: 8.920\n",
      "200\n",
      "[197,   399] loss: 8.881\n",
      "200\n",
      "[198,   399] loss: 8.841\n",
      "200\n",
      "[199,   399] loss: 8.802\n",
      "200\n",
      "[200,   399] loss: 8.762\n",
      "200\n",
      "[201,   399] loss: 8.723\n",
      "200\n",
      "[202,   399] loss: 8.683\n",
      "200\n",
      "[203,   399] loss: 8.644\n",
      "200\n",
      "[204,   399] loss: 8.604\n",
      "200\n",
      "[205,   399] loss: 8.565\n",
      "200\n",
      "[206,   399] loss: 8.526\n",
      "200\n",
      "[207,   399] loss: 8.487\n",
      "200\n",
      "[208,   399] loss: 8.450\n",
      "200\n",
      "[209,   399] loss: 8.412\n",
      "200\n",
      "[210,   399] loss: 8.375\n",
      "200\n",
      "[211,   399] loss: 8.336\n",
      "200\n",
      "[212,   399] loss: 8.297\n",
      "200\n",
      "[213,   399] loss: 8.258\n",
      "200\n",
      "[214,   399] loss: 8.220\n",
      "200\n",
      "[215,   399] loss: 8.183\n",
      "200\n",
      "[216,   399] loss: 8.146\n",
      "200\n",
      "[217,   399] loss: 8.109\n",
      "200\n",
      "[218,   399] loss: 8.072\n",
      "200\n",
      "[219,   399] loss: 8.035\n",
      "200\n",
      "[220,   399] loss: 7.998\n",
      "200\n",
      "[221,   399] loss: 7.962\n",
      "200\n",
      "[222,   399] loss: 7.925\n",
      "200\n",
      "[223,   399] loss: 7.888\n",
      "200\n",
      "[224,   399] loss: 7.854\n",
      "200\n",
      "[225,   399] loss: 7.818\n",
      "200\n",
      "[226,   399] loss: 7.784\n",
      "200\n",
      "[227,   399] loss: 7.750\n",
      "200\n",
      "[228,   399] loss: 7.718\n",
      "200\n",
      "[229,   399] loss: 7.685\n",
      "200\n",
      "[230,   399] loss: 7.648\n",
      "200\n",
      "[231,   399] loss: 7.610\n",
      "200\n",
      "[232,   399] loss: 7.574\n",
      "200\n",
      "[233,   399] loss: 7.538\n",
      "200\n",
      "[234,   399] loss: 7.503\n",
      "200\n",
      "[235,   399] loss: 7.468\n",
      "200\n",
      "[236,   399] loss: 7.433\n",
      "200\n",
      "[237,   399] loss: 7.400\n",
      "200\n",
      "[238,   399] loss: 7.366\n",
      "200\n",
      "[239,   399] loss: 7.332\n",
      "200\n",
      "[240,   399] loss: 7.299\n",
      "200\n",
      "[241,   399] loss: 7.265\n",
      "200\n",
      "[242,   399] loss: 7.232\n",
      "200\n",
      "[243,   399] loss: 7.200\n",
      "200\n",
      "[244,   399] loss: 7.166\n",
      "200\n",
      "[245,   399] loss: 7.133\n",
      "200\n",
      "[246,   399] loss: 7.101\n",
      "200\n",
      "[247,   399] loss: 7.069\n",
      "200\n",
      "[248,   399] loss: 7.037\n",
      "200\n",
      "[249,   399] loss: 7.006\n",
      "200\n",
      "[250,   399] loss: 6.975\n",
      "200\n",
      "[251,   399] loss: 6.943\n",
      "200\n",
      "[252,   399] loss: 6.912\n",
      "200\n",
      "[253,   399] loss: 6.881\n",
      "200\n",
      "[254,   399] loss: 6.850\n",
      "200\n",
      "[255,   399] loss: 6.818\n",
      "200\n",
      "[256,   399] loss: 6.787\n",
      "200\n",
      "[257,   399] loss: 6.757\n",
      "200\n",
      "[258,   399] loss: 6.726\n",
      "200\n",
      "[259,   399] loss: 6.696\n",
      "200\n",
      "[260,   399] loss: 6.664\n",
      "200\n",
      "[261,   399] loss: 6.634\n",
      "200\n",
      "[262,   399] loss: 6.604\n",
      "200\n",
      "[263,   399] loss: 6.574\n",
      "200\n",
      "[264,   399] loss: 6.544\n",
      "200\n",
      "[265,   399] loss: 6.514\n",
      "200\n",
      "[266,   399] loss: 6.486\n",
      "200\n",
      "[267,   399] loss: 6.457\n",
      "200\n",
      "[268,   399] loss: 6.429\n",
      "200\n",
      "[269,   399] loss: 6.400\n",
      "200\n",
      "[270,   399] loss: 6.371\n",
      "200\n",
      "[271,   399] loss: 6.342\n",
      "200\n",
      "[272,   399] loss: 6.314\n",
      "200\n",
      "[273,   399] loss: 6.285\n",
      "200\n",
      "[274,   399] loss: 6.258\n",
      "200\n",
      "[275,   399] loss: 6.229\n",
      "200\n",
      "[276,   399] loss: 6.202\n",
      "200\n",
      "[277,   399] loss: 6.175\n",
      "200\n",
      "[278,   399] loss: 6.148\n",
      "200\n",
      "[279,   399] loss: 6.121\n",
      "200\n",
      "[280,   399] loss: 6.096\n",
      "200\n",
      "[281,   399] loss: 6.070\n",
      "200\n",
      "[282,   399] loss: 6.044\n",
      "200\n",
      "[283,   399] loss: 6.019\n",
      "200\n",
      "[284,   399] loss: 5.993\n",
      "200\n",
      "[285,   399] loss: 5.967\n",
      "200\n",
      "[286,   399] loss: 5.944\n",
      "200\n",
      "[287,   399] loss: 5.918\n",
      "200\n",
      "[288,   399] loss: 5.892\n",
      "200\n",
      "[289,   399] loss: 5.866\n",
      "200\n",
      "[290,   399] loss: 5.842\n",
      "200\n",
      "[291,   399] loss: 5.815\n",
      "200\n",
      "[292,   399] loss: 5.791\n",
      "200\n",
      "[293,   399] loss: 5.769\n",
      "200\n",
      "[294,   399] loss: 5.746\n",
      "200\n",
      "[295,   399] loss: 5.723\n",
      "200\n",
      "[296,   399] loss: 5.701\n",
      "200\n",
      "[297,   399] loss: 5.676\n",
      "200\n",
      "[298,   399] loss: 5.651\n",
      "200\n",
      "[299,   399] loss: 5.626\n",
      "200\n",
      "[300,   399] loss: 5.602\n",
      "200\n",
      "[301,   399] loss: 5.577\n",
      "200\n",
      "[302,   399] loss: 5.554\n",
      "200\n",
      "[303,   399] loss: 5.530\n",
      "200\n",
      "[304,   399] loss: 5.510\n",
      "200\n",
      "[305,   399] loss: 5.487\n",
      "200\n",
      "[306,   399] loss: 5.465\n",
      "200\n",
      "[307,   399] loss: 5.445\n",
      "200\n",
      "[308,   399] loss: 5.423\n",
      "200\n",
      "[309,   399] loss: 5.401\n",
      "200\n",
      "[310,   399] loss: 5.380\n",
      "200\n",
      "[311,   399] loss: 5.355\n",
      "200\n",
      "[312,   399] loss: 5.332\n",
      "200\n",
      "[313,   399] loss: 5.308\n",
      "200\n",
      "[314,   399] loss: 5.286\n",
      "200\n",
      "[315,   399] loss: 5.264\n",
      "200\n",
      "[316,   399] loss: 5.243\n",
      "200\n",
      "[317,   399] loss: 5.221\n",
      "200\n",
      "[318,   399] loss: 5.201\n",
      "200\n",
      "[319,   399] loss: 5.181\n",
      "200\n",
      "[320,   399] loss: 5.159\n",
      "200\n",
      "[321,   399] loss: 5.138\n",
      "200\n",
      "[322,   399] loss: 5.120\n",
      "200\n",
      "[323,   399] loss: 5.098\n",
      "200\n",
      "[324,   399] loss: 5.075\n",
      "200\n",
      "[325,   399] loss: 5.054\n",
      "200\n",
      "[326,   399] loss: 5.033\n",
      "200\n",
      "[327,   399] loss: 5.013\n",
      "200\n",
      "[328,   399] loss: 4.992\n",
      "200\n",
      "[329,   399] loss: 4.970\n",
      "200\n",
      "[330,   399] loss: 4.952\n",
      "200\n",
      "[331,   399] loss: 4.932\n",
      "200\n",
      "[332,   399] loss: 4.912\n",
      "200\n",
      "[333,   399] loss: 4.890\n",
      "200\n",
      "[334,   399] loss: 4.870\n",
      "200\n",
      "[335,   399] loss: 4.850\n",
      "200\n",
      "[336,   399] loss: 4.830\n",
      "200\n",
      "[337,   399] loss: 4.811\n",
      "200\n",
      "[338,   399] loss: 4.793\n",
      "200\n",
      "[339,   399] loss: 4.773\n",
      "200\n",
      "[340,   399] loss: 4.755\n",
      "200\n",
      "[341,   399] loss: 4.735\n",
      "200\n",
      "[342,   399] loss: 4.719\n",
      "200\n",
      "[343,   399] loss: 4.701\n",
      "200\n",
      "[344,   399] loss: 4.682\n",
      "200\n",
      "[345,   399] loss: 4.669\n",
      "200\n",
      "[346,   399] loss: 4.651\n",
      "200\n",
      "[347,   399] loss: 4.633\n",
      "200\n",
      "[348,   399] loss: 4.611\n",
      "200\n",
      "[349,   399] loss: 4.591\n",
      "200\n",
      "[350,   399] loss: 4.571\n",
      "200\n",
      "[351,   399] loss: 4.553\n",
      "200\n",
      "[352,   399] loss: 4.534\n",
      "200\n",
      "[353,   399] loss: 4.517\n",
      "200\n",
      "[354,   399] loss: 4.498\n",
      "200\n",
      "[355,   399] loss: 4.479\n",
      "200\n",
      "[356,   399] loss: 4.462\n",
      "200\n",
      "[357,   399] loss: 4.445\n",
      "200\n",
      "[358,   399] loss: 4.430\n",
      "200\n",
      "[359,   399] loss: 4.412\n",
      "200\n",
      "[360,   399] loss: 4.397\n",
      "200\n",
      "[361,   399] loss: 4.380\n",
      "200\n",
      "[362,   399] loss: 4.362\n",
      "200\n",
      "[363,   399] loss: 4.346\n",
      "200\n",
      "[364,   399] loss: 4.327\n",
      "200\n",
      "[365,   399] loss: 4.310\n",
      "200\n",
      "[366,   399] loss: 4.293\n",
      "200\n",
      "[367,   399] loss: 4.276\n",
      "200\n",
      "[368,   399] loss: 4.259\n",
      "200\n",
      "[369,   399] loss: 4.243\n",
      "200\n",
      "[370,   399] loss: 4.227\n",
      "200\n",
      "[371,   399] loss: 4.210\n",
      "200\n",
      "[372,   399] loss: 4.195\n",
      "200\n",
      "[373,   399] loss: 4.177\n",
      "200\n",
      "[374,   399] loss: 4.161\n",
      "200\n",
      "[375,   399] loss: 4.145\n",
      "200\n",
      "[376,   399] loss: 4.129\n",
      "200\n",
      "[377,   399] loss: 4.111\n",
      "200\n",
      "[378,   399] loss: 4.095\n",
      "200\n",
      "[379,   399] loss: 4.081\n",
      "200\n",
      "[380,   399] loss: 4.065\n",
      "200\n",
      "[381,   399] loss: 4.049\n",
      "200\n",
      "[382,   399] loss: 4.037\n",
      "200\n",
      "[383,   399] loss: 4.023\n",
      "200\n",
      "[384,   399] loss: 4.010\n",
      "200\n",
      "[385,   399] loss: 3.995\n",
      "200\n",
      "[386,   399] loss: 3.981\n",
      "200\n",
      "[387,   399] loss: 3.964\n",
      "200\n",
      "[388,   399] loss: 3.948\n",
      "200\n",
      "[389,   399] loss: 3.931\n",
      "200\n",
      "[390,   399] loss: 3.917\n",
      "200\n",
      "[391,   399] loss: 3.901\n",
      "200\n",
      "[392,   399] loss: 3.889\n",
      "200\n",
      "[393,   399] loss: 3.877\n",
      "200\n",
      "[394,   399] loss: 3.860\n",
      "200\n",
      "[395,   399] loss: 3.847\n",
      "200\n",
      "[396,   399] loss: 3.833\n",
      "200\n",
      "[397,   399] loss: 3.821\n",
      "200\n",
      "[398,   399] loss: 3.807\n",
      "200\n",
      "[399,   399] loss: 3.796\n",
      "200\n",
      "[400,   399] loss: 3.781\n",
      "200\n",
      "[401,   399] loss: 3.767\n",
      "200\n",
      "[402,   399] loss: 3.753\n",
      "200\n",
      "[403,   399] loss: 3.741\n",
      "200\n",
      "[404,   399] loss: 3.728\n",
      "200\n",
      "[405,   399] loss: 3.717\n",
      "200\n",
      "[406,   399] loss: 3.701\n",
      "200\n",
      "[407,   399] loss: 3.689\n",
      "200\n",
      "[408,   399] loss: 3.678\n",
      "200\n",
      "[409,   399] loss: 3.663\n",
      "200\n",
      "[410,   399] loss: 3.652\n",
      "200\n",
      "[411,   399] loss: 3.639\n",
      "200\n",
      "[412,   399] loss: 3.625\n",
      "200\n",
      "[413,   399] loss: 3.613\n",
      "200\n",
      "[414,   399] loss: 3.601\n",
      "200\n",
      "[415,   399] loss: 3.590\n",
      "200\n",
      "[416,   399] loss: 3.576\n",
      "200\n",
      "[417,   399] loss: 3.564\n",
      "200\n",
      "[418,   399] loss: 3.552\n",
      "200\n",
      "[419,   399] loss: 3.540\n",
      "200\n",
      "[420,   399] loss: 3.527\n",
      "200\n",
      "[421,   399] loss: 3.519\n",
      "200\n",
      "[422,   399] loss: 3.507\n",
      "200\n",
      "[423,   399] loss: 3.493\n",
      "200\n",
      "[424,   399] loss: 3.484\n",
      "200\n",
      "[425,   399] loss: 3.472\n",
      "200\n",
      "[426,   399] loss: 3.459\n",
      "200\n",
      "[427,   399] loss: 3.447\n",
      "200\n",
      "[428,   399] loss: 3.436\n",
      "200\n",
      "[429,   399] loss: 3.425\n",
      "200\n",
      "[430,   399] loss: 3.417\n",
      "200\n",
      "[431,   399] loss: 3.403\n",
      "200\n",
      "[432,   399] loss: 3.393\n",
      "200\n",
      "[433,   399] loss: 3.384\n",
      "200\n",
      "[434,   399] loss: 3.371\n",
      "200\n",
      "[435,   399] loss: 3.359\n",
      "200\n",
      "[436,   399] loss: 3.348\n",
      "200\n",
      "[437,   399] loss: 3.338\n",
      "200\n",
      "[438,   399] loss: 3.328\n",
      "200\n",
      "[439,   399] loss: 3.317\n",
      "200\n",
      "[440,   399] loss: 3.308\n",
      "200\n",
      "[441,   399] loss: 3.297\n",
      "200\n",
      "[442,   399] loss: 3.288\n",
      "200\n",
      "[443,   399] loss: 3.278\n",
      "200\n",
      "[444,   399] loss: 3.267\n",
      "200\n",
      "[445,   399] loss: 3.257\n",
      "200\n",
      "[446,   399] loss: 3.249\n",
      "200\n",
      "[447,   399] loss: 3.237\n",
      "200\n",
      "[448,   399] loss: 3.226\n",
      "200\n",
      "[449,   399] loss: 3.217\n",
      "200\n",
      "[450,   399] loss: 3.208\n",
      "200\n",
      "[451,   399] loss: 3.198\n",
      "200\n",
      "[452,   399] loss: 3.188\n",
      "200\n",
      "[453,   399] loss: 3.178\n",
      "200\n",
      "[454,   399] loss: 3.169\n",
      "200\n",
      "[455,   399] loss: 3.160\n",
      "200\n",
      "[456,   399] loss: 3.150\n",
      "200\n",
      "[457,   399] loss: 3.141\n",
      "200\n",
      "[458,   399] loss: 3.131\n",
      "200\n",
      "[459,   399] loss: 3.123\n",
      "200\n",
      "[460,   399] loss: 3.115\n",
      "200\n",
      "[461,   399] loss: 3.106\n",
      "200\n",
      "[462,   399] loss: 3.096\n",
      "200\n",
      "[463,   399] loss: 3.087\n",
      "200\n",
      "[464,   399] loss: 3.078\n",
      "200\n",
      "[465,   399] loss: 3.070\n",
      "200\n",
      "[466,   399] loss: 3.062\n",
      "200\n",
      "[467,   399] loss: 3.050\n",
      "200\n",
      "[468,   399] loss: 3.043\n",
      "200\n",
      "[469,   399] loss: 3.032\n",
      "200\n",
      "[470,   399] loss: 3.026\n",
      "200\n",
      "[471,   399] loss: 3.016\n",
      "200\n",
      "[472,   399] loss: 3.009\n",
      "200\n",
      "[473,   399] loss: 3.000\n",
      "200\n",
      "[474,   399] loss: 2.992\n",
      "200\n",
      "[475,   399] loss: 2.983\n",
      "200\n",
      "[476,   399] loss: 2.975\n",
      "200\n",
      "[477,   399] loss: 2.969\n",
      "200\n",
      "[478,   399] loss: 2.965\n",
      "200\n",
      "[479,   399] loss: 2.954\n",
      "200\n",
      "[480,   399] loss: 2.946\n",
      "200\n",
      "[481,   399] loss: 2.939\n",
      "200\n",
      "[482,   399] loss: 2.932\n",
      "200\n",
      "[483,   399] loss: 2.926\n",
      "200\n",
      "[484,   399] loss: 2.915\n",
      "200\n",
      "[485,   399] loss: 2.908\n",
      "200\n",
      "[486,   399] loss: 2.900\n",
      "200\n",
      "[487,   399] loss: 2.890\n",
      "200\n",
      "[488,   399] loss: 2.881\n",
      "200\n",
      "[489,   399] loss: 2.874\n",
      "200\n",
      "[490,   399] loss: 2.866\n",
      "200\n",
      "[491,   399] loss: 2.858\n",
      "200\n",
      "[492,   399] loss: 2.850\n",
      "200\n",
      "[493,   399] loss: 2.845\n",
      "200\n",
      "[494,   399] loss: 2.837\n",
      "200\n",
      "[495,   399] loss: 2.830\n",
      "200\n",
      "[496,   399] loss: 2.824\n",
      "200\n",
      "[497,   399] loss: 2.817\n",
      "200\n",
      "[498,   399] loss: 2.809\n",
      "200\n",
      "[499,   399] loss: 2.805\n",
      "200\n",
      "[500,   399] loss: 2.797\n",
      "200\n",
      "[501,   399] loss: 2.790\n",
      "200\n",
      "[502,   399] loss: 2.782\n",
      "200\n",
      "[503,   399] loss: 2.775\n",
      "200\n",
      "[504,   399] loss: 2.769\n",
      "200\n",
      "[505,   399] loss: 2.764\n",
      "200\n",
      "[506,   399] loss: 2.758\n",
      "200\n",
      "[507,   399] loss: 2.751\n",
      "200\n",
      "[508,   399] loss: 2.744\n",
      "200\n",
      "[509,   399] loss: 2.735\n",
      "200\n",
      "[510,   399] loss: 2.728\n",
      "200\n",
      "[511,   399] loss: 2.721\n",
      "200\n",
      "[512,   399] loss: 2.713\n",
      "200\n",
      "[513,   399] loss: 2.707\n",
      "200\n",
      "[514,   399] loss: 2.700\n",
      "200\n",
      "[515,   399] loss: 2.692\n",
      "200\n",
      "[516,   399] loss: 2.686\n",
      "200\n",
      "[517,   399] loss: 2.681\n",
      "200\n",
      "[518,   399] loss: 2.674\n",
      "200\n",
      "[519,   399] loss: 2.669\n",
      "200\n",
      "[520,   399] loss: 2.663\n",
      "200\n",
      "[521,   399] loss: 2.655\n",
      "200\n",
      "[522,   399] loss: 2.649\n",
      "200\n",
      "[523,   399] loss: 2.645\n",
      "200\n",
      "[524,   399] loss: 2.638\n",
      "200\n",
      "[525,   399] loss: 2.633\n",
      "200\n",
      "[526,   399] loss: 2.625\n",
      "200\n",
      "[527,   399] loss: 2.619\n",
      "200\n",
      "[528,   399] loss: 2.613\n",
      "200\n",
      "[529,   399] loss: 2.608\n",
      "200\n",
      "[530,   399] loss: 2.601\n",
      "200\n",
      "[531,   399] loss: 2.594\n",
      "200\n",
      "[532,   399] loss: 2.588\n",
      "200\n",
      "[533,   399] loss: 2.583\n",
      "200\n",
      "[534,   399] loss: 2.577\n",
      "200\n",
      "[535,   399] loss: 2.572\n",
      "200\n",
      "[536,   399] loss: 2.567\n",
      "200\n",
      "[537,   399] loss: 2.562\n",
      "200\n",
      "[538,   399] loss: 2.555\n",
      "200\n",
      "[539,   399] loss: 2.550\n",
      "200\n",
      "[540,   399] loss: 2.545\n",
      "200\n",
      "[541,   399] loss: 2.540\n",
      "200\n",
      "[542,   399] loss: 2.537\n",
      "200\n",
      "[543,   399] loss: 2.531\n",
      "200\n",
      "[544,   399] loss: 2.527\n",
      "200\n",
      "[545,   399] loss: 2.519\n",
      "200\n",
      "[546,   399] loss: 2.513\n",
      "200\n",
      "[547,   399] loss: 2.506\n",
      "200\n",
      "[548,   399] loss: 2.501\n",
      "200\n",
      "[549,   399] loss: 2.492\n",
      "200\n",
      "[550,   399] loss: 2.486\n",
      "200\n",
      "[551,   399] loss: 2.479\n",
      "200\n",
      "[552,   399] loss: 2.475\n",
      "200\n",
      "[553,   399] loss: 2.472\n",
      "200\n",
      "[554,   399] loss: 2.465\n",
      "200\n",
      "[555,   399] loss: 2.460\n",
      "200\n",
      "[556,   399] loss: 2.454\n",
      "200\n",
      "[557,   399] loss: 2.450\n",
      "200\n",
      "[558,   399] loss: 2.447\n",
      "200\n",
      "[559,   399] loss: 2.440\n",
      "200\n",
      "[560,   399] loss: 2.437\n",
      "200\n",
      "[561,   399] loss: 2.430\n",
      "200\n",
      "[562,   399] loss: 2.426\n",
      "200\n",
      "[563,   399] loss: 2.422\n",
      "200\n",
      "[564,   399] loss: 2.420\n",
      "200\n",
      "[565,   399] loss: 2.411\n",
      "200\n",
      "[566,   399] loss: 2.403\n",
      "200\n",
      "[567,   399] loss: 2.398\n",
      "200\n",
      "[568,   399] loss: 2.392\n",
      "200\n",
      "[569,   399] loss: 2.388\n",
      "200\n",
      "[570,   399] loss: 2.384\n",
      "200\n",
      "[571,   399] loss: 2.381\n",
      "200\n",
      "[572,   399] loss: 2.375\n",
      "200\n",
      "[573,   399] loss: 2.370\n",
      "200\n",
      "[574,   399] loss: 2.363\n",
      "200\n",
      "[575,   399] loss: 2.359\n",
      "200\n",
      "[576,   399] loss: 2.355\n",
      "200\n",
      "[577,   399] loss: 2.350\n",
      "200\n",
      "[578,   399] loss: 2.348\n",
      "200\n",
      "[579,   399] loss: 2.340\n",
      "200\n",
      "[580,   399] loss: 2.336\n",
      "200\n",
      "[581,   399] loss: 2.330\n",
      "200\n",
      "[582,   399] loss: 2.328\n",
      "200\n",
      "[583,   399] loss: 2.322\n",
      "200\n",
      "[584,   399] loss: 2.318\n",
      "200\n",
      "[585,   399] loss: 2.311\n",
      "200\n",
      "[586,   399] loss: 2.307\n",
      "200\n",
      "[587,   399] loss: 2.303\n",
      "200\n",
      "[588,   399] loss: 2.298\n",
      "200\n",
      "[589,   399] loss: 2.294\n",
      "200\n",
      "[590,   399] loss: 2.290\n",
      "200\n",
      "[591,   399] loss: 2.284\n",
      "200\n",
      "[592,   399] loss: 2.282\n",
      "200\n",
      "[593,   399] loss: 2.275\n",
      "200\n",
      "[594,   399] loss: 2.270\n",
      "200\n",
      "[595,   399] loss: 2.266\n",
      "200\n",
      "[596,   399] loss: 2.262\n",
      "200\n",
      "[597,   399] loss: 2.257\n",
      "200\n",
      "[598,   399] loss: 2.255\n",
      "200\n",
      "[599,   399] loss: 2.251\n",
      "200\n",
      "[600,   399] loss: 2.247\n",
      "200\n",
      "[601,   399] loss: 2.242\n",
      "200\n",
      "[602,   399] loss: 2.238\n",
      "200\n",
      "[603,   399] loss: 2.232\n",
      "200\n",
      "[604,   399] loss: 2.232\n",
      "200\n",
      "[605,   399] loss: 2.228\n",
      "200\n",
      "[606,   399] loss: 2.226\n",
      "200\n",
      "[607,   399] loss: 2.218\n",
      "200\n",
      "[608,   399] loss: 2.215\n",
      "200\n",
      "[609,   399] loss: 2.210\n",
      "200\n",
      "[610,   399] loss: 2.207\n",
      "200\n",
      "[611,   399] loss: 2.201\n",
      "200\n",
      "[612,   399] loss: 2.195\n",
      "200\n",
      "[613,   399] loss: 2.190\n",
      "200\n",
      "[614,   399] loss: 2.185\n",
      "200\n",
      "[615,   399] loss: 2.180\n",
      "200\n",
      "[616,   399] loss: 2.179\n",
      "200\n",
      "[617,   399] loss: 2.174\n",
      "200\n",
      "[618,   399] loss: 2.169\n",
      "200\n",
      "[619,   399] loss: 2.167\n",
      "200\n",
      "[620,   399] loss: 2.165\n",
      "200\n",
      "[621,   399] loss: 2.159\n",
      "200\n",
      "[622,   399] loss: 2.160\n",
      "200\n",
      "[623,   399] loss: 2.160\n",
      "200\n",
      "[624,   399] loss: 2.153\n",
      "200\n",
      "[625,   399] loss: 2.150\n",
      "200\n",
      "[626,   399] loss: 2.145\n",
      "200\n",
      "[627,   399] loss: 2.142\n",
      "200\n",
      "[628,   399] loss: 2.136\n",
      "200\n",
      "[629,   399] loss: 2.133\n",
      "200\n",
      "[630,   399] loss: 2.128\n",
      "200\n",
      "[631,   399] loss: 2.121\n",
      "200\n",
      "[632,   399] loss: 2.115\n",
      "200\n",
      "[633,   399] loss: 2.115\n",
      "200\n",
      "[634,   399] loss: 2.109\n",
      "200\n",
      "[635,   399] loss: 2.107\n",
      "200\n",
      "[636,   399] loss: 2.102\n",
      "200\n",
      "[637,   399] loss: 2.102\n",
      "200\n",
      "[638,   399] loss: 2.096\n",
      "200\n",
      "[639,   399] loss: 2.092\n",
      "200\n",
      "[640,   399] loss: 2.089\n",
      "200\n",
      "[641,   399] loss: 2.088\n",
      "200\n",
      "[642,   399] loss: 2.083\n",
      "200\n",
      "[643,   399] loss: 2.078\n",
      "200\n",
      "[644,   399] loss: 2.078\n",
      "200\n",
      "[645,   399] loss: 2.074\n",
      "200\n",
      "[646,   399] loss: 2.075\n",
      "200\n",
      "[647,   399] loss: 2.072\n",
      "200\n",
      "[648,   399] loss: 2.067\n",
      "200\n",
      "[649,   399] loss: 2.061\n",
      "200\n",
      "[650,   399] loss: 2.061\n",
      "200\n",
      "[651,   399] loss: 2.053\n",
      "200\n",
      "[652,   399] loss: 2.049\n",
      "200\n",
      "[653,   399] loss: 2.040\n",
      "200\n",
      "[654,   399] loss: 2.036\n",
      "200\n",
      "[655,   399] loss: 2.033\n",
      "200\n",
      "[656,   399] loss: 2.031\n",
      "200\n",
      "[657,   399] loss: 2.030\n",
      "200\n",
      "[658,   399] loss: 2.026\n",
      "200\n",
      "[659,   399] loss: 2.026\n",
      "200\n",
      "[660,   399] loss: 2.022\n",
      "200\n",
      "[661,   399] loss: 2.017\n",
      "200\n",
      "[662,   399] loss: 2.017\n",
      "200\n",
      "[663,   399] loss: 2.013\n",
      "200\n",
      "[664,   399] loss: 2.009\n",
      "200\n",
      "[665,   399] loss: 2.006\n",
      "200\n",
      "[666,   399] loss: 2.003\n",
      "200\n",
      "[667,   399] loss: 2.000\n",
      "200\n",
      "[668,   399] loss: 1.996\n",
      "200\n",
      "[669,   399] loss: 1.995\n",
      "200\n",
      "[670,   399] loss: 1.990\n",
      "200\n",
      "[671,   399] loss: 1.984\n",
      "200\n",
      "[672,   399] loss: 1.980\n",
      "200\n",
      "[673,   399] loss: 1.979\n",
      "200\n",
      "[674,   399] loss: 1.972\n",
      "200\n",
      "[675,   399] loss: 1.970\n",
      "200\n",
      "[676,   399] loss: 1.966\n",
      "200\n",
      "[677,   399] loss: 1.964\n",
      "200\n",
      "[678,   399] loss: 1.964\n",
      "200\n",
      "[679,   399] loss: 1.962\n",
      "200\n",
      "[680,   399] loss: 1.956\n",
      "200\n",
      "[681,   399] loss: 1.956\n",
      "200\n",
      "[682,   399] loss: 1.955\n",
      "200\n",
      "[683,   399] loss: 1.954\n",
      "200\n",
      "[684,   399] loss: 1.949\n",
      "200\n",
      "[685,   399] loss: 1.944\n",
      "200\n",
      "[686,   399] loss: 1.941\n",
      "200\n",
      "[687,   399] loss: 1.938\n",
      "200\n",
      "[688,   399] loss: 1.937\n",
      "200\n",
      "[689,   399] loss: 1.931\n",
      "200\n",
      "[690,   399] loss: 1.927\n",
      "200\n",
      "[691,   399] loss: 1.924\n",
      "200\n",
      "[692,   399] loss: 1.916\n",
      "200\n",
      "[693,   399] loss: 1.919\n",
      "200\n",
      "[694,   399] loss: 1.910\n",
      "200\n",
      "[695,   399] loss: 1.909\n",
      "200\n",
      "[696,   399] loss: 1.908\n",
      "200\n",
      "[697,   399] loss: 1.903\n",
      "200\n",
      "[698,   399] loss: 1.902\n",
      "200\n",
      "[699,   399] loss: 1.901\n",
      "200\n",
      "[700,   399] loss: 1.901\n",
      "200\n",
      "[701,   399] loss: 1.898\n",
      "200\n",
      "[702,   399] loss: 1.890\n",
      "200\n",
      "[703,   399] loss: 1.886\n",
      "200\n",
      "[704,   399] loss: 1.888\n",
      "200\n",
      "[705,   399] loss: 1.881\n",
      "200\n",
      "[706,   399] loss: 1.878\n",
      "200\n",
      "[707,   399] loss: 1.873\n",
      "200\n",
      "[708,   399] loss: 1.872\n",
      "200\n",
      "[709,   399] loss: 1.868\n",
      "200\n",
      "[710,   399] loss: 1.867\n",
      "200\n",
      "[711,   399] loss: 1.866\n",
      "200\n",
      "[712,   399] loss: 1.860\n",
      "200\n",
      "[713,   399] loss: 1.863\n",
      "200\n",
      "[714,   399] loss: 1.861\n",
      "200\n",
      "[715,   399] loss: 1.861\n",
      "200\n",
      "[716,   399] loss: 1.859\n",
      "200\n",
      "[717,   399] loss: 1.853\n",
      "200\n",
      "[718,   399] loss: 1.853\n",
      "200\n",
      "[719,   399] loss: 1.849\n",
      "200\n",
      "[720,   399] loss: 1.843\n",
      "200\n",
      "[721,   399] loss: 1.841\n",
      "200\n",
      "[722,   399] loss: 1.839\n",
      "200\n",
      "[723,   399] loss: 1.829\n",
      "200\n",
      "[724,   399] loss: 1.825\n",
      "200\n",
      "[725,   399] loss: 1.824\n",
      "200\n",
      "[726,   399] loss: 1.822\n",
      "200\n",
      "[727,   399] loss: 1.819\n",
      "200\n",
      "[728,   399] loss: 1.819\n",
      "200\n",
      "[729,   399] loss: 1.813\n",
      "200\n",
      "[730,   399] loss: 1.811\n",
      "200\n",
      "[731,   399] loss: 1.810\n",
      "200\n",
      "[732,   399] loss: 1.809\n",
      "200\n",
      "[733,   399] loss: 1.807\n",
      "200\n",
      "[734,   399] loss: 1.804\n",
      "200\n",
      "[735,   399] loss: 1.802\n",
      "200\n",
      "[736,   399] loss: 1.800\n",
      "200\n",
      "[737,   399] loss: 1.797\n",
      "200\n",
      "[738,   399] loss: 1.798\n",
      "200\n",
      "[739,   399] loss: 1.799\n",
      "200\n",
      "[740,   399] loss: 1.792\n",
      "200\n",
      "[741,   399] loss: 1.785\n",
      "200\n",
      "[742,   399] loss: 1.785\n",
      "200\n",
      "[743,   399] loss: 1.786\n",
      "200\n",
      "[744,   399] loss: 1.777\n",
      "200\n",
      "[745,   399] loss: 1.778\n",
      "200\n",
      "[746,   399] loss: 1.774\n",
      "200\n",
      "[747,   399] loss: 1.772\n",
      "200\n",
      "[748,   399] loss: 1.771\n",
      "200\n",
      "[749,   399] loss: 1.763\n",
      "200\n",
      "[750,   399] loss: 1.764\n",
      "200\n",
      "[751,   399] loss: 1.758\n",
      "200\n",
      "[752,   399] loss: 1.759\n",
      "200\n",
      "[753,   399] loss: 1.757\n",
      "200\n",
      "[754,   399] loss: 1.754\n",
      "200\n",
      "[755,   399] loss: 1.752\n",
      "200\n",
      "[756,   399] loss: 1.749\n",
      "200\n",
      "[757,   399] loss: 1.748\n",
      "200\n",
      "[758,   399] loss: 1.745\n",
      "200\n",
      "[759,   399] loss: 1.745\n",
      "200\n",
      "[760,   399] loss: 1.740\n",
      "200\n",
      "[761,   399] loss: 1.740\n",
      "200\n",
      "[762,   399] loss: 1.738\n",
      "200\n",
      "[763,   399] loss: 1.735\n",
      "200\n",
      "[764,   399] loss: 1.733\n",
      "200\n",
      "[765,   399] loss: 1.731\n",
      "200\n",
      "[766,   399] loss: 1.729\n",
      "200\n",
      "[767,   399] loss: 1.731\n",
      "200\n",
      "[768,   399] loss: 1.731\n",
      "200\n",
      "[769,   399] loss: 1.733\n",
      "200\n",
      "[770,   399] loss: 1.731\n",
      "200\n",
      "[771,   399] loss: 1.724\n",
      "200\n",
      "[772,   399] loss: 1.718\n",
      "200\n",
      "[773,   399] loss: 1.714\n",
      "200\n",
      "[774,   399] loss: 1.709\n",
      "200\n",
      "[775,   399] loss: 1.705\n",
      "200\n",
      "[776,   399] loss: 1.700\n",
      "200\n",
      "[777,   399] loss: 1.699\n",
      "200\n",
      "[778,   399] loss: 1.697\n",
      "200\n",
      "[779,   399] loss: 1.695\n",
      "200\n",
      "[780,   399] loss: 1.694\n",
      "200\n",
      "[781,   399] loss: 1.696\n",
      "200\n",
      "[782,   399] loss: 1.692\n",
      "200\n",
      "[783,   399] loss: 1.690\n",
      "200\n",
      "[784,   399] loss: 1.689\n",
      "200\n",
      "[785,   399] loss: 1.688\n",
      "200\n",
      "[786,   399] loss: 1.683\n",
      "200\n",
      "[787,   399] loss: 1.682\n",
      "200\n",
      "[788,   399] loss: 1.677\n",
      "200\n",
      "[789,   399] loss: 1.678\n",
      "200\n",
      "[790,   399] loss: 1.673\n",
      "200\n",
      "[791,   399] loss: 1.670\n",
      "200\n",
      "[792,   399] loss: 1.670\n",
      "200\n",
      "[793,   399] loss: 1.668\n",
      "200\n",
      "[794,   399] loss: 1.669\n",
      "200\n",
      "[795,   399] loss: 1.665\n",
      "200\n",
      "[796,   399] loss: 1.666\n",
      "200\n",
      "[797,   399] loss: 1.660\n",
      "200\n",
      "[798,   399] loss: 1.659\n",
      "200\n",
      "[799,   399] loss: 1.660\n",
      "200\n",
      "[800,   399] loss: 1.658\n",
      "200\n",
      "[801,   399] loss: 1.657\n",
      "200\n",
      "[802,   399] loss: 1.658\n",
      "200\n",
      "[803,   399] loss: 1.650\n",
      "200\n",
      "[804,   399] loss: 1.651\n",
      "200\n",
      "[805,   399] loss: 1.645\n",
      "200\n",
      "[806,   399] loss: 1.639\n",
      "200\n",
      "[807,   399] loss: 1.637\n",
      "200\n",
      "[808,   399] loss: 1.635\n",
      "200\n",
      "[809,   399] loss: 1.634\n",
      "200\n",
      "[810,   399] loss: 1.634\n",
      "200\n",
      "[811,   399] loss: 1.633\n",
      "200\n",
      "[812,   399] loss: 1.628\n",
      "200\n",
      "[813,   399] loss: 1.627\n",
      "200\n",
      "[814,   399] loss: 1.626\n",
      "200\n",
      "[815,   399] loss: 1.625\n",
      "200\n",
      "[816,   399] loss: 1.623\n",
      "200\n",
      "[817,   399] loss: 1.620\n",
      "200\n",
      "[818,   399] loss: 1.617\n",
      "200\n",
      "[819,   399] loss: 1.616\n",
      "200\n",
      "[820,   399] loss: 1.616\n",
      "200\n",
      "[821,   399] loss: 1.612\n",
      "200\n",
      "[822,   399] loss: 1.610\n",
      "200\n",
      "[823,   399] loss: 1.608\n",
      "200\n",
      "[824,   399] loss: 1.606\n",
      "200\n",
      "[825,   399] loss: 1.606\n",
      "200\n",
      "[826,   399] loss: 1.602\n",
      "200\n",
      "[827,   399] loss: 1.601\n",
      "200\n",
      "[828,   399] loss: 1.596\n",
      "200\n",
      "[829,   399] loss: 1.595\n",
      "200\n",
      "[830,   399] loss: 1.594\n",
      "200\n",
      "[831,   399] loss: 1.594\n",
      "200\n",
      "[832,   399] loss: 1.592\n",
      "200\n",
      "[833,   399] loss: 1.593\n",
      "200\n",
      "[834,   399] loss: 1.595\n",
      "200\n",
      "[835,   399] loss: 1.593\n",
      "200\n",
      "[836,   399] loss: 1.596\n",
      "200\n",
      "[837,   399] loss: 1.595\n",
      "200\n",
      "[838,   399] loss: 1.592\n",
      "200\n",
      "[839,   399] loss: 1.583\n",
      "200\n",
      "[840,   399] loss: 1.577\n",
      "200\n",
      "[841,   399] loss: 1.579\n",
      "200\n",
      "[842,   399] loss: 1.577\n",
      "200\n",
      "[843,   399] loss: 1.573\n",
      "200\n",
      "[844,   399] loss: 1.568\n",
      "200\n",
      "[845,   399] loss: 1.564\n",
      "200\n",
      "[846,   399] loss: 1.563\n",
      "200\n",
      "[847,   399] loss: 1.563\n",
      "200\n",
      "[848,   399] loss: 1.561\n",
      "200\n",
      "[849,   399] loss: 1.561\n",
      "200\n",
      "[850,   399] loss: 1.557\n",
      "200\n",
      "[851,   399] loss: 1.558\n",
      "200\n",
      "[852,   399] loss: 1.555\n",
      "200\n",
      "[853,   399] loss: 1.556\n",
      "200\n",
      "[854,   399] loss: 1.553\n",
      "200\n",
      "[855,   399] loss: 1.551\n",
      "200\n",
      "[856,   399] loss: 1.549\n",
      "200\n",
      "[857,   399] loss: 1.546\n",
      "200\n",
      "[858,   399] loss: 1.551\n",
      "200\n",
      "[859,   399] loss: 1.546\n",
      "200\n",
      "[860,   399] loss: 1.546\n",
      "200\n",
      "[861,   399] loss: 1.548\n",
      "200\n",
      "[862,   399] loss: 1.551\n",
      "200\n",
      "[863,   399] loss: 1.548\n",
      "200\n",
      "[864,   399] loss: 1.542\n",
      "200\n",
      "[865,   399] loss: 1.536\n",
      "200\n",
      "[866,   399] loss: 1.532\n",
      "200\n",
      "[867,   399] loss: 1.529\n",
      "200\n",
      "[868,   399] loss: 1.528\n",
      "200\n",
      "[869,   399] loss: 1.527\n",
      "200\n",
      "[870,   399] loss: 1.525\n",
      "200\n",
      "[871,   399] loss: 1.524\n",
      "200\n",
      "[872,   399] loss: 1.521\n",
      "200\n",
      "[873,   399] loss: 1.519\n",
      "200\n",
      "[874,   399] loss: 1.515\n",
      "200\n",
      "[875,   399] loss: 1.515\n",
      "200\n",
      "[876,   399] loss: 1.514\n",
      "200\n",
      "[877,   399] loss: 1.517\n",
      "200\n",
      "[878,   399] loss: 1.514\n",
      "200\n",
      "[879,   399] loss: 1.513\n",
      "200\n",
      "[880,   399] loss: 1.508\n",
      "200\n",
      "[881,   399] loss: 1.506\n",
      "200\n",
      "[882,   399] loss: 1.506\n",
      "200\n",
      "[883,   399] loss: 1.506\n",
      "200\n",
      "[884,   399] loss: 1.502\n",
      "200\n",
      "[885,   399] loss: 1.505\n",
      "200\n",
      "[886,   399] loss: 1.507\n",
      "200\n",
      "[887,   399] loss: 1.507\n",
      "200\n",
      "[888,   399] loss: 1.505\n",
      "200\n",
      "[889,   399] loss: 1.501\n",
      "200\n",
      "[890,   399] loss: 1.504\n",
      "200\n",
      "[891,   399] loss: 1.500\n",
      "200\n",
      "[892,   399] loss: 1.502\n",
      "200\n",
      "[893,   399] loss: 1.496\n",
      "200\n",
      "[894,   399] loss: 1.492\n",
      "200\n",
      "[895,   399] loss: 1.488\n",
      "200\n",
      "[896,   399] loss: 1.489\n",
      "200\n",
      "[897,   399] loss: 1.486\n",
      "200\n",
      "[898,   399] loss: 1.480\n",
      "200\n",
      "[899,   399] loss: 1.477\n",
      "200\n",
      "[900,   399] loss: 1.478\n",
      "200\n",
      "[901,   399] loss: 1.475\n",
      "200\n",
      "[902,   399] loss: 1.474\n",
      "200\n",
      "[903,   399] loss: 1.471\n",
      "200\n",
      "[904,   399] loss: 1.471\n",
      "200\n",
      "[905,   399] loss: 1.469\n",
      "200\n",
      "[906,   399] loss: 1.467\n",
      "200\n",
      "[907,   399] loss: 1.468\n",
      "200\n",
      "[908,   399] loss: 1.464\n",
      "200\n",
      "[909,   399] loss: 1.461\n",
      "200\n",
      "[910,   399] loss: 1.463\n",
      "200\n",
      "[911,   399] loss: 1.458\n",
      "200\n",
      "[912,   399] loss: 1.460\n",
      "200\n",
      "[913,   399] loss: 1.459\n",
      "200\n",
      "[914,   399] loss: 1.457\n",
      "200\n",
      "[915,   399] loss: 1.459\n",
      "200\n",
      "[916,   399] loss: 1.463\n",
      "200\n",
      "[917,   399] loss: 1.459\n",
      "200\n",
      "[918,   399] loss: 1.457\n",
      "200\n",
      "[919,   399] loss: 1.453\n",
      "200\n",
      "[920,   399] loss: 1.448\n",
      "200\n",
      "[921,   399] loss: 1.452\n",
      "200\n",
      "[922,   399] loss: 1.443\n",
      "200\n",
      "[923,   399] loss: 1.445\n",
      "200\n",
      "[924,   399] loss: 1.440\n",
      "200\n",
      "[925,   399] loss: 1.438\n",
      "200\n",
      "[926,   399] loss: 1.438\n",
      "200\n",
      "[927,   399] loss: 1.438\n",
      "200\n",
      "[928,   399] loss: 1.436\n",
      "200\n",
      "[929,   399] loss: 1.433\n",
      "200\n",
      "[930,   399] loss: 1.433\n",
      "200\n",
      "[931,   399] loss: 1.433\n",
      "200\n",
      "[932,   399] loss: 1.432\n",
      "200\n",
      "[933,   399] loss: 1.431\n",
      "200\n",
      "[934,   399] loss: 1.428\n",
      "200\n",
      "[935,   399] loss: 1.430\n",
      "200\n",
      "[936,   399] loss: 1.427\n",
      "200\n",
      "[937,   399] loss: 1.423\n",
      "200\n",
      "[938,   399] loss: 1.425\n",
      "200\n",
      "[939,   399] loss: 1.425\n",
      "200\n",
      "[940,   399] loss: 1.423\n",
      "200\n",
      "[941,   399] loss: 1.424\n",
      "200\n",
      "[942,   399] loss: 1.423\n",
      "200\n",
      "[943,   399] loss: 1.424\n",
      "200\n",
      "[944,   399] loss: 1.423\n",
      "200\n",
      "[945,   399] loss: 1.423\n",
      "200\n",
      "[946,   399] loss: 1.423\n",
      "200\n",
      "[947,   399] loss: 1.418\n",
      "200\n",
      "[948,   399] loss: 1.417\n",
      "200\n",
      "[949,   399] loss: 1.412\n",
      "200\n",
      "[950,   399] loss: 1.406\n",
      "200\n",
      "[951,   399] loss: 1.405\n",
      "200\n",
      "[952,   399] loss: 1.407\n",
      "200\n",
      "[953,   399] loss: 1.402\n",
      "200\n",
      "[954,   399] loss: 1.401\n",
      "200\n",
      "[955,   399] loss: 1.398\n",
      "200\n",
      "[956,   399] loss: 1.395\n",
      "200\n",
      "[957,   399] loss: 1.392\n",
      "200\n",
      "[958,   399] loss: 1.394\n",
      "200\n",
      "[959,   399] loss: 1.393\n",
      "200\n",
      "[960,   399] loss: 1.395\n",
      "200\n",
      "[961,   399] loss: 1.392\n",
      "200\n",
      "[962,   399] loss: 1.390\n",
      "200\n",
      "[963,   399] loss: 1.390\n",
      "200\n",
      "[964,   399] loss: 1.391\n",
      "200\n",
      "[965,   399] loss: 1.388\n",
      "200\n",
      "[966,   399] loss: 1.385\n",
      "200\n",
      "[967,   399] loss: 1.384\n",
      "200\n",
      "[968,   399] loss: 1.384\n",
      "200\n",
      "[969,   399] loss: 1.385\n",
      "200\n",
      "[970,   399] loss: 1.383\n",
      "200\n",
      "[971,   399] loss: 1.381\n",
      "200\n",
      "[972,   399] loss: 1.375\n",
      "200\n",
      "[973,   399] loss: 1.372\n",
      "200\n",
      "[974,   399] loss: 1.374\n",
      "200\n",
      "[975,   399] loss: 1.374\n",
      "200\n",
      "[976,   399] loss: 1.371\n",
      "200\n",
      "[977,   399] loss: 1.369\n",
      "200\n",
      "[978,   399] loss: 1.368\n",
      "200\n",
      "[979,   399] loss: 1.367\n",
      "200\n",
      "[980,   399] loss: 1.368\n",
      "200\n",
      "[981,   399] loss: 1.366\n",
      "200\n",
      "[982,   399] loss: 1.370\n",
      "200\n",
      "[983,   399] loss: 1.366\n",
      "200\n",
      "[984,   399] loss: 1.364\n",
      "200\n",
      "[985,   399] loss: 1.362\n",
      "200\n",
      "[986,   399] loss: 1.360\n",
      "200\n",
      "[987,   399] loss: 1.358\n",
      "200\n",
      "[988,   399] loss: 1.360\n",
      "200\n",
      "[989,   399] loss: 1.363\n",
      "200\n",
      "[990,   399] loss: 1.362\n",
      "200\n",
      "[991,   399] loss: 1.358\n",
      "200\n",
      "[992,   399] loss: 1.363\n",
      "200\n",
      "[993,   399] loss: 1.359\n",
      "200\n",
      "[994,   399] loss: 1.357\n",
      "200\n",
      "[995,   399] loss: 1.354\n",
      "200\n",
      "[996,   399] loss: 1.352\n",
      "200\n",
      "[997,   399] loss: 1.349\n",
      "200\n",
      "[998,   399] loss: 1.343\n",
      "200\n",
      "[999,   399] loss: 1.341\n",
      "200\n",
      "[1000,   399] loss: 1.338\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "reconstruction_loss_arr = []\n",
    "kl_loss_obj_arr = []\n",
    "kl_loss_part_arr = []\n",
    "bbox_loss_arr = []\n",
    "refined_bbox_loss_arr = []\n",
    "adj_loss_arr = []\n",
    "node_loss_arr = []\n",
    "\n",
    "reconstruction_loss_val_arr = []\n",
    "kl_loss_val_arr = []\n",
    "bbox_loss_val_arr = []\n",
    "refined_bbox_loss_val_arr = []\n",
    "adj_loss_val_arr = []\n",
    "node_loss_val_arr = []\n",
    "\n",
    "bbox_loss_threshold = 1.0\n",
    "vae = TwoStageAutoEncoder(latent_dims,\n",
    "                          num_nodes,\n",
    "                          bbx_size,\n",
    "                          num_classes,\n",
    "                          label_shape,\n",
    "                          hidden1,\n",
    "                          hidden2,\n",
    "                          hidden3,\n",
    "                          dense_hidden1,\n",
    "                          dense_hidden2,\n",
    "                          adaptive_margin,\n",
    "                          output_log,\n",
    "                          area_encoding,\n",
    "                          coupling,\n",
    "                          obj_bbx_conditioning,\n",
    "                          use_fft_on_bbx,\n",
    "                          use_gcn_in_decoder\n",
    "                        )\n",
    "vae.to(device)\n",
    "optimizer = optim.Adam(vae.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100,300], gamma=0.75)\n",
    "model_path = ('/Users/amrutamuthal/Documents/training_runs/model/'+run_prefix+'/Obj-Box-'\n",
    "                        +str(learning_rate)\n",
    "                        +'-batch-'+str(batch_size)\n",
    "                        +'-h1-'+str(hidden1)\n",
    "                        +'-h2-'+str(hidden2)\n",
    "                        +'-h3-'+str(hidden3)+'-test')\n",
    "summary_path = ('/Users/amrutamuthal/Documents/training_runs/runs/'+run_prefix+'/Obj-Box-'\n",
    "                        +str(learning_rate)\n",
    "                        +'-batch-'+str(batch_size)\n",
    "                        +'-h1-'+str(hidden1)\n",
    "                        +'-h2-'+str(hidden2)\n",
    "                        +'-h3-'+str(hidden3)+'-test')\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "writer = SummaryWriter(summary_path)\n",
    "icoef = 0\n",
    "\n",
    "for epoch in range(nb_epochs):\n",
    "    \n",
    "    batch_loss = torch.tensor([0.0])\n",
    "    batch_kl_loss_part = torch.tensor([0.0])\n",
    "    batch_kl_loss_obj = torch.tensor([0.0])\n",
    "    batch_bbox_loss = torch.tensor([0.0])\n",
    "    batch_refined_bbox_loss = torch.tensor([0.0])\n",
    "    batch_obj_bbox_loss = torch.tensor([0.0])\n",
    "    batch_node_loss = torch.tensor([0.0])\n",
    "    IOU_weight_delta = torch.tensor([(1+epoch)/nb_epochs])\n",
    "    images = []\n",
    "    \n",
    "    vae.train()\n",
    "    i=0\n",
    "    for train_data in batch_train_loader:\n",
    "        \n",
    "        node_data_true = train_data.x\n",
    "        label_true = node_data_true[:,:1]\n",
    "        y_true = train_data.y\n",
    "        class_true = y_true[:, :num_classes]\n",
    "        X_obj_true = y_true[:, num_classes:]\n",
    "        X_obj_true_transformed = X_obj_true[:,2:]-X_obj_true[:,:2]\n",
    "        node_data_transformed = torch.cat(\n",
    "            [node_data_true[:, :-2],\n",
    "             node_data_true[:, -2:] -  node_data_true[:, -4:-2]], axis=-1)\n",
    "\n",
    "        adj_true = train_data.edge_index\n",
    "        batch = train_data.batch\n",
    "        \n",
    "        class_true  = torch.flatten(class_true)\n",
    "        \n",
    "\n",
    "        for param in vae.parameters():\n",
    "            param.grad=None\n",
    "        \n",
    "        output = vae(\n",
    "            adj_true,\n",
    "            node_data_transformed,\n",
    "            X_obj_true_transformed,\n",
    "            label_true,\n",
    "            class_true, variational, coupling, refine_iter=2\n",
    "            # training=(epoch>100)\n",
    "            )\n",
    "        \n",
    "        node_data_pred = output[0]\n",
    "        node_data_pred_new = torch.cat(\n",
    "            [node_data_pred[:, :, :-2],\n",
    "             node_data_pred[:, :, -2:] + node_data_pred[:, :, -4:-2]], axis=-1)\n",
    "        X_obj_pred = output[1]\n",
    "        X_obj_pred = torch.cat(((torch.tensor([1.0])-X_obj_pred)*torch.tensor([0.5]),\n",
    "                                (torch.tensor([1.0])+X_obj_pred)*torch.tensor([0.5])),\n",
    "                              -1)\n",
    "        label_pred = output[2]\n",
    "        z_mean_part = output[3]\n",
    "        z_logvar_part = output[4]\n",
    "        margin = output[5]\n",
    "        z_mean_obj = output[6]\n",
    "        z_logvar_obj = output[7]\n",
    "\n",
    "        node_data_pred_refined = output[8]\n",
    "        node_data_pred_refined_new = torch.cat(\n",
    "            [node_data_pred_refined[:, :, :-2],\n",
    "             node_data_pred_refined[:, :, -2:] + node_data_pred_refined[:, :, -4:-2]], axis=-1)\n",
    "\n",
    "        obj_bbox_loss = loss.obj_bbox_loss(pred_box=X_obj_pred, true_box=X_obj_true, weight=IOU_weight_delta, has_mse=False)\n",
    "        kl_loss_obj = loss.kl_loss(z_mean_obj, z_logvar_obj)\n",
    "        kl_loss_part = loss.kl_loss(z_mean_part, z_logvar_part)\n",
    "        bbox_loss = loss.weighted_bbox_loss(pred_box=node_data_pred_new, true_box=node_data_true[:,1:], weight=IOU_weight_delta, margin=margin)\n",
    "        bbox_loss_coarse = loss.coarse_bbx_loss(pred_box=node_data_pred_new, true_box=node_data_true[:,1:])\n",
    "        node_loss = loss.node_loss(label_pred,label_true)\n",
    "        \n",
    "        # reconstruction_loss = (node_loss)*num_nodes\n",
    "\n",
    "        if use_gcn_in_decoder:\n",
    "            refined_bbox_loss = loss.weighted_bbox_loss(\n",
    "                pred_box=node_data_pred_refined_new,\n",
    "                true_box=node_data_true[:, 1:],\n",
    "                weight=IOU_weight_delta,\n",
    "                margin=margin,\n",
    "            )\n",
    "            reconstruction_loss = (refined_bbox_loss + node_loss) * num_nodes # * epoch / nb_epochs\n",
    "        \n",
    "        else:\n",
    "            refined_bbox_loss = torch.tensor([0.0])\n",
    "        \n",
    "        kl_weight = klw[icoef]\n",
    "        if variational and (kl_weight>0):\n",
    "            reconstruction_loss += kl_loss_part*kl_weight   \n",
    "        \n",
    "        if epoch >50:\n",
    "            reconstruction_loss += bbox_loss_coarse\n",
    "\n",
    "        reconstruction_loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        i+=1\n",
    "      \n",
    "        batch_loss += reconstruction_loss\n",
    "        batch_kl_loss_part += kl_loss_part\n",
    "        batch_kl_loss_obj += kl_loss_obj\n",
    "        batch_bbox_loss += bbox_loss\n",
    "        batch_refined_bbox_loss += refined_bbox_loss\n",
    "        batch_obj_bbox_loss += obj_bbox_loss\n",
    "        batch_node_loss += node_loss\n",
    "    \n",
    "        if i%200==0:\n",
    "            print(i)\n",
    "            global_step = epoch*len(batch_train_loader)+i\n",
    "            \n",
    "            writer.add_scalar(\"Loss/train/reconstruction_loss\", batch_loss.item()/(i+1), global_step)\n",
    "            writer.add_scalar(\"Loss/train/kl_loss_part\", batch_kl_loss_part.item()/(i+1), global_step)\n",
    "            writer.add_scalar(\"Loss/train/kl_loss_obj\", batch_kl_loss_obj.item()/(i+1), global_step)\n",
    "            writer.add_scalar(\"Loss/train/bbox_loss\", batch_bbox_loss.item()/(i+1), global_step)\n",
    "            writer.add_scalar(\"Loss/train/refined_bbox_loss\", batch_refined_bbox_loss.item()/(i+1), global_step)\n",
    "            writer.add_scalar(\"Loss/train/obj_bbox_loss\", batch_obj_bbox_loss.item()/(i+1), global_step)\n",
    "            writer.add_scalar(\"Loss/train/node_loss\", batch_node_loss.item()/(i+1), global_step)\n",
    "            \n",
    "            \n",
    "#     scheduler.step()\n",
    "    global_step = epoch*len(batch_train_loader)+i\n",
    "    image_shape = [num_nodes, bbx_size]\n",
    "\n",
    "    image = plot_utils.plot_bbx(np.reshape((node_data_true[:num_nodes,1:5]*label_true[:num_nodes]).detach().to(\"cpu\").numpy(),\n",
    "                                image_shape)).astype(float)/255\n",
    "    writer.add_image('train/images/input', image, global_step, dataformats='HWC')\n",
    "    image = plot_utils.plot_bbx((node_data_pred_new[0]*label_true[:num_nodes]).detach().to(\"cpu\").numpy()).astype(float)/255\n",
    "    writer.add_image('train/images/generated', image, global_step, dataformats='HWC')\n",
    "    image = plot_utils.plot_bbx((node_data_pred_refined_new[0]*label_true[:num_nodes]).detach().to(\"cpu\").numpy()).astype(float)/255\n",
    "    writer.add_image('train/images/refined', image, global_step, dataformats='HWC')\n",
    "    \n",
    "    reconstruction_loss_arr.append(batch_loss.detach().item()/(i+1))\n",
    "    kl_loss_obj_arr.append(batch_kl_loss_obj.detach().item()/(i+1))\n",
    "    kl_loss_part_arr.append(batch_kl_loss_part.detach().item()/(i+1))\n",
    "    bbox_loss_arr.append(batch_bbox_loss.detach().item()/(i+1))\n",
    "    refined_bbox_loss_arr.append(batch_refined_bbox_loss.detach().item()/(i+1))\n",
    "    node_loss_arr.append(batch_node_loss.detach().item()/(i+1))\n",
    "    \n",
    "    print('[%d, %5d] loss: %.3f' %\n",
    "              (epoch + 1, i + 1, batch_loss/(i+1) ))\n",
    "    \n",
    "    \n",
    "    batch_loss = torch.tensor([0.0])\n",
    "    batch_kl_loss_part = torch.tensor([0.0])\n",
    "    batch_kl_loss_obj = torch.tensor([0.0])\n",
    "    batch_bbox_loss = torch.tensor([0.0])\n",
    "    batch_refined_bbox_loss = torch.tensor([0.0])\n",
    "    batch_obj_bbox_loss = torch.tensor([0.0])\n",
    "    batch_node_loss = torch.tensor([0.0])\n",
    "    images = []\n",
    "    vae.eval()\n",
    "    for i, val_data in enumerate(batch_val_loader, 0):\n",
    "        \n",
    "        node_data_true = val_data.x\n",
    "        label_true = node_data_true[:,:1]\n",
    "        y_true = val_data.y\n",
    "        class_true = torch.flatten(y_true[:, :num_classes])\n",
    "        X_obj_true = y_true[:, num_classes:]\n",
    "        X_obj_true_transformed = X_obj_true[:,2:]-X_obj_true[:,:2]\n",
    "        node_data_transformed = torch.cat(\n",
    "            [node_data_true[:, :-2],\n",
    "             node_data_true[:, -2:] -  node_data_true[:, -4:-2]], axis=-1)\n",
    "        adj_true = val_data.edge_index\n",
    "        batch = val_data.batch\n",
    "        \n",
    "        class_true  = torch.flatten(class_true)\n",
    "        \n",
    "        output = vae(adj_true, node_data_transformed, X_obj_true_transformed, label_true , class_true, variational, coupling)\n",
    "        \n",
    "        node_data_pred = output[0]\n",
    "        node_data_pred_new = node_data_pred\n",
    "        node_data_pred_new = torch.cat(\n",
    "            [node_data_pred[:, :, :-2],\n",
    "             node_data_pred[:, :, -2:] + node_data_pred[:, :, -4:-2]], axis=-1)\n",
    "\n",
    "        X_obj_pred = output[1]\n",
    "        X_obj_pred = torch.cat(((torch.tensor([1.0])-X_obj_pred)*torch.tensor([0.5]),\n",
    "                                (torch.tensor([1.0])+X_obj_pred)*torch.tensor([0.5])),\n",
    "                              -1)\n",
    "        label_pred = output[2]\n",
    "        z_mean_part = output[3]\n",
    "        z_logvar_part = output[4]\n",
    "        margin = output[5]\n",
    "        z_mean_obj = output[6]\n",
    "        z_logvar_obj = output[7]\n",
    "\n",
    "        node_data_pred_refined = output[8]\n",
    "        node_data_pred_refined_new = node_data_pred_refined\n",
    "        node_data_pred_refined_new = torch.cat(\n",
    "            [node_data_pred_refined_new[:, :, :-2],\n",
    "             node_data_pred_refined_new[:, :, -2:] + node_data_pred_refined_new[:, :, -4:-2]], axis=-1)\n",
    "        \n",
    "        \n",
    "        obj_bbox_loss = loss.obj_bbox_loss(pred_box=X_obj_pred, true_box=X_obj_true, weight=IOU_weight_delta, has_mse=False)\n",
    "        kl_loss_obj = loss.kl_loss(z_mean_obj, z_logvar_obj)\n",
    "        kl_loss_part = loss.kl_loss(z_mean_part, z_logvar_part)\n",
    "        bbox_loss = loss.weighted_bbox_loss(pred_box=node_data_pred_new, true_box=node_data_true[:,1:], weight=IOU_weight_delta, margin=margin)\n",
    "        bbox_loss_coarse = loss.coarse_bbx_loss(pred_box=node_data_pred_new, true_box=node_data_true[:,1:])\n",
    "        refined_bbox_loss = loss.weighted_bbox_loss(pred_box=node_data_pred_refined_new, true_box=node_data_true[:,1:], weight=IOU_weight_delta, margin=margin)\n",
    "        node_loss = loss.node_loss(label_pred,label_true)\n",
    "        \n",
    "        if kl_weight>0:\n",
    "            reconstruction_loss = kl_loss_part*kl_weight + (bbox_loss + node_loss)*num_nodes\n",
    "        else:\n",
    "            reconstruction_loss = (refined_bbox_loss + node_loss)*num_nodes\n",
    "\n",
    "        if epoch > 50:\n",
    "            reconstruction_loss +=  refined_bbox_loss\n",
    "            \n",
    "        batch_loss += reconstruction_loss\n",
    "        batch_kl_loss_part += kl_loss_part\n",
    "        batch_kl_loss_obj += kl_loss_obj\n",
    "        batch_bbox_loss += bbox_loss\n",
    "        batch_refined_bbox_loss += refined_bbox_loss\n",
    "        batch_obj_bbox_loss += obj_bbox_loss\n",
    "        batch_node_loss += node_loss\n",
    "    \n",
    "    image = plot_utils.plot_bbx(np.reshape((node_data_true[:num_nodes,1:5]*label_true[:num_nodes]).detach().to(\"cpu\").numpy(),\n",
    "                                image_shape)).astype(float)/255\n",
    "    writer.add_image('val/images/input', image, global_step, dataformats='HWC')\n",
    "    image = plot_utils.plot_bbx((node_data_pred_new[0]*label_true[:num_nodes]).detach().to(\"cpu\").numpy()).astype(float)/255\n",
    "    writer.add_image('val/images/generated', image, global_step, dataformats='HWC')\n",
    "    image = plot_utils.plot_bbx((node_data_pred_refined_new[0]*label_true[:num_nodes]).detach().to(\"cpu\").numpy()).astype(float)/255\n",
    "    writer.add_image('val/images/refined', image, global_step, dataformats='HWC')\n",
    "\n",
    "    reconstruction_loss_val_arr.append(batch_loss.detach().item()/(i+1))\n",
    "    bbox_loss_val_arr.append(batch_bbox_loss.detach().item()/(i+1))\n",
    "    node_loss_val_arr.append(batch_node_loss.detach().item()/(i+1))\n",
    "    \n",
    "    writer.add_scalar(\"Loss/val/reconstruction_loss\", batch_loss.detach()/(i+1), global_step)\n",
    "    writer.add_scalar(\"Loss/train/kl_loss_part\", batch_kl_loss_part.item()/(i+1), global_step)\n",
    "    writer.add_scalar(\"Loss/train/kl_loss_obj\", batch_kl_loss_obj.item()/(i+1), global_step)\n",
    "    writer.add_scalar(\"Loss/val/bbox_loss\", batch_bbox_loss.item()/(i+1), global_step)\n",
    "    writer.add_scalar(\"Loss/val/refined_bbox_loss\", batch_refined_bbox_loss.item()/(i+1), global_step)\n",
    "    writer.add_scalar(\"Loss/val/obj_bbox_loss\", batch_obj_bbox_loss.item()/(i+1), global_step)\n",
    "    writer.add_scalar(\"Loss/val/node_loss\", batch_node_loss.item()/(i+1), global_step)\n",
    "       \n",
    "    if epoch%50 == 0:\n",
    "        torch.save(vae.state_dict(), model_path + '/model_weights.pth')\n",
    "        \n",
    "    if ((kl_loss_part_arr[-1]>0.5) and \n",
    "        (abs(bbox_loss_arr[-1] - bbox_loss_val_arr[-1]) < 0.07) and \n",
    "        (bbox_loss_arr[-1]<bbox_loss_threshold) and (epoch>300)):\n",
    "        \n",
    "        icoef = icoef + 1\n",
    "        bbox_loss_threshold*=0.9\n",
    "\n",
    "torch.save(vae.state_dict(),model_path + '/model_weights.pth')\n",
    "\n",
    "for i in range(min(100,int(len(node_data_true)/num_nodes))):    \n",
    "    image = plot_utils.plot_bbx(np.reshape((node_data_true[num_nodes*i:num_nodes*(i+1),1:5]).detach().to(\"cpu\").numpy(),\n",
    "                                    image_shape)).astype(float)/255\n",
    "    writer.add_image('result/images/'+str(i)+'-input', image, global_step, dataformats='HWC')\n",
    "    image = plot_utils.plot_bbx((node_data_pred[i]*label_true[num_nodes*i:num_nodes*(i+1)]).detach().to(\"cpu\").numpy()).astype(float)/255\n",
    "    writer.add_image('result/images/'+str(i)+'-generated', image, global_step, dataformats='HWC')\n",
    "    \n",
    "writer.flush()\n",
    "writer.close()\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d9154e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 16, 4])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_data_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a642a867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "173005"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " sum(p.numel() for p in vae.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6b72cd01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ww/3nytfk1917b8nz7dchtckxnm0000gn/T/ipykernel_74047/4170770996.py:39: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  vae.load_state_dict(torch.load(model_path+ '/model_weights.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.06140351 0.         0.63596493 0.7816092 ]\n",
      " [0.0877193  0.40229884 1.         1.        ]\n",
      " [0.18421052 0.3218391  0.3508772  0.43678162]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.51754385 0.33333334 0.5833333  0.42528737]\n",
      " [0.6666667  0.55172414 0.8684211  0.7356322 ]\n",
      " [0.03947368 0.85823756 0.4473684  1.        ]\n",
      " [0.         0.47509578 0.19298245 0.97318006]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.33133826 0.29239422 0.57612807 0.5966601 ]\n",
      " [0.33200622 0.43697947 0.658074   0.7045162 ]\n",
      " [0.38630325 0.42256996 0.4681724  0.50294393]\n",
      " [0.3243219  0.29607248 0.33431372 0.29978934]\n",
      " [0.5789807  0.43745074 0.6496495  0.53728086]\n",
      " [0.48392308 0.48291713 0.6213629  0.6068866 ]\n",
      " [0.3269028  0.6842104  0.40128058 0.70761365]\n",
      " [0.32022294 0.49895346 0.3742542  0.69718045]\n",
      " [0.32431847 0.2960709  0.33430302 0.29978505]\n",
      " [0.32431838 0.29607087 0.33430278 0.29978496]\n",
      " [0.32431838 0.29607087 0.33430278 0.29978496]\n",
      " [0.32431838 0.29607087 0.33430278 0.29978496]\n",
      " [0.3243184  0.29607087 0.3343028  0.29978496]\n",
      " [0.32431838 0.29607087 0.33430278 0.29978496]\n",
      " [0.32431838 0.29607087 0.33430278 0.29978496]\n",
      " [0.32431838 0.29607087 0.33430278 0.29978496]]\n",
      "[[0.75       0.         1.         0.21875   ]\n",
      " [0.27083334 0.09375    0.9375     0.6875    ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.6041667  0.71875    0.6458333  1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.5208333  0.71875    0.5833333  1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.3125     0.27083334 0.53125   ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.5152401  0.47637656 0.5354546  0.48667407]\n",
      " [0.48520228 0.48084208 0.5316059  0.5086464 ]\n",
      " [0.46565628 0.47678316 0.46760824 0.47720602]\n",
      " [0.4656562  0.47678313 0.46760803 0.47720596]\n",
      " [0.4656562  0.47678313 0.46760803 0.47720596]\n",
      " [0.50687397 0.512717   0.5132161  0.52363634]\n",
      " [0.4656562  0.47678313 0.46760803 0.47720596]\n",
      " [0.5050008  0.51063496 0.51076394 0.52357   ]\n",
      " [0.4656562  0.47678313 0.46760803 0.47720596]\n",
      " [0.46573955 0.49240187 0.48371446 0.5017547 ]\n",
      " [0.4656562  0.47678313 0.46760803 0.47720596]\n",
      " [0.46565622 0.47678313 0.4676081  0.477206  ]\n",
      " [0.4656562  0.47678313 0.46760803 0.47720596]\n",
      " [0.4656562  0.47678313 0.46760803 0.47720596]\n",
      " [0.4656562  0.47678313 0.46760803 0.47720596]\n",
      " [0.4656562  0.47678313 0.46760803 0.47720596]]\n",
      "[[0.11409396 0.         0.97651005 0.70454544]\n",
      " [0.         0.3778409  1.         1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.35038203 0.24686259 0.689661   0.6026014 ]\n",
      " [0.28753754 0.45256072 0.7144697  0.75333333]\n",
      " [0.29224956 0.2511626  0.3040565  0.25569457]\n",
      " [0.29224956 0.2511626  0.3040565  0.25569457]\n",
      " [0.29224956 0.2511626  0.3040565  0.25569457]\n",
      " [0.29224956 0.2511626  0.3040565  0.25569457]\n",
      " [0.29224956 0.2511626  0.3040565  0.25569457]\n",
      " [0.29224956 0.2511626  0.3040565  0.25569457]\n",
      " [0.29224956 0.2511626  0.3040565  0.25569457]\n",
      " [0.29224962 0.25116262 0.3040567  0.25569463]\n",
      " [0.29224956 0.2511626  0.3040565  0.25569457]\n",
      " [0.29224956 0.2511626  0.3040565  0.25569457]\n",
      " [0.29224956 0.2511626  0.3040565  0.25569457]\n",
      " [0.29224956 0.2511626  0.3040565  0.25569457]\n",
      " [0.29224956 0.2511626  0.3040565  0.25569457]\n",
      " [0.29224956 0.2511626  0.3040565  0.25569457]]\n",
      "[[0.27522936 0.         1.         0.39930555]\n",
      " [0.07339449 0.18055555 0.8027523  0.7743056 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.2638889  0.19266056 0.6458333 ]\n",
      " [0.5321101  0.6631944  0.78899086 0.9097222 ]\n",
      " [0.5412844  0.7777778  0.7844037  0.9097222 ]\n",
      " [0.1146789  0.6354167  0.3119266  1.        ]\n",
      " [0.14678898 0.8159722  0.33486238 1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.43414426 0.29289797 0.65075374 0.45697826]\n",
      " [0.36159638 0.34219193 0.55663544 0.61755663]\n",
      " [0.34802157 0.29640582 0.35665914 0.30011392]\n",
      " [0.34802663 0.29640847 0.35667527 0.3001214 ]\n",
      " [0.344969   0.42278892 0.41259044 0.6030929 ]\n",
      " [0.50444275 0.5871774  0.55927247 0.6618431 ]\n",
      " [0.51663494 0.61612093 0.5642954  0.68443763]\n",
      " [0.37701574 0.5903907  0.46095467 0.7014159 ]\n",
      " [0.37970945 0.6215793  0.44796348 0.68215936]\n",
      " [0.34802186 0.296406   0.35666007 0.30011436]\n",
      " [0.34802145 0.29640576 0.35665873 0.30011374]\n",
      " [0.34802145 0.29640576 0.35665873 0.30011374]\n",
      " [0.34802145 0.29640576 0.35665873 0.30011374]\n",
      " [0.34802145 0.29640576 0.35665873 0.30011374]\n",
      " [0.34802145 0.29640576 0.35665873 0.30011374]\n",
      " [0.34802145 0.29640576 0.35665873 0.30011374]]\n",
      "[[0.6605505  0.         0.93119264 0.3257143 ]\n",
      " [0.16972478 0.03428571 1.         0.9142857 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.61926603 0.8857143  0.6788991  0.94857144]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.55504584 0.88       0.74311924 1.        ]\n",
      " [0.58715594 0.9257143  0.7477064  1.        ]\n",
      " [0.         0.6628571  0.33027524 0.85142857]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.5531295  0.36446097 0.66815925 0.4356838 ]\n",
      " [0.3938371  0.39025488 0.6617154  0.6111043 ]\n",
      " [0.33602318 0.3665215  0.34534237 0.36895248]\n",
      " [0.33602318 0.3665215  0.34534237 0.36895248]\n",
      " [0.33602318 0.36652148 0.34534237 0.36895248]\n",
      " [0.5360538  0.5979815  0.56451833 0.6277948 ]\n",
      " [0.33602327 0.3665215  0.34534264 0.36895257]\n",
      " [0.519196   0.5937001  0.5676943  0.6358901 ]\n",
      " [0.52465326 0.61496884 0.56081027 0.6358901 ]\n",
      " [0.33983842 0.5312314  0.42757136 0.59520113]\n",
      " [0.33602318 0.36652148 0.34534237 0.36895248]\n",
      " [0.33602324 0.3665215  0.34534252 0.3689525 ]\n",
      " [0.33602318 0.36652148 0.34534237 0.36895248]\n",
      " [0.33602318 0.36652148 0.34534237 0.36895248]\n",
      " [0.33602318 0.36652148 0.34534237 0.36895248]\n",
      " [0.33602318 0.36652148 0.34534237 0.36895248]]\n",
      "[[0.         0.23076923 0.2962963  0.7       ]\n",
      " [0.2361111  0.         0.8449074  0.75      ]\n",
      " [0.28935185 0.19615385 0.36342594 0.52692306]\n",
      " [0.42824075 0.4076923  0.5555556  1.        ]\n",
      " [0.4189815  0.76153845 0.5462963  1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.6018519  0.37692308 0.8287037  0.9115385 ]\n",
      " [0.5925926  0.7076923  0.7037037  0.9       ]\n",
      " [0.7175926  0.5846154  0.849537   0.9115385 ]\n",
      " [0.7175926  0.74615383 0.8078704  0.9269231 ]\n",
      " [0.8194444  0.11153846 1.         0.34615386]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.19088626 0.40841767 0.33528984 0.6379701 ]\n",
      " [0.372846   0.31918377 0.781664   0.58004856]\n",
      " [0.3092259  0.37164286 0.38439083 0.5162671 ]\n",
      " [0.4887189  0.44107127 0.5969945  0.66158485]\n",
      " [0.4285832  0.6003816  0.4870995  0.659613  ]\n",
      " [0.1988428  0.31620237 0.21598318 0.31955472]\n",
      " [0.19883177 0.3161997  0.21594837 0.31954727]\n",
      " [0.57088614 0.43391865 0.7006624  0.6501058 ]\n",
      " [0.61884737 0.60064924 0.66788304 0.6511471 ]\n",
      " [0.6799044  0.50884366 0.7681543  0.6602365 ]\n",
      " [0.6645904  0.59886265 0.7064224  0.6486429 ]\n",
      " [0.6589377  0.3473749  0.79001784 0.52242166]\n",
      " [0.19885337 0.3162053  0.21601719 0.31956252]\n",
      " [0.19883153 0.31619963 0.21594764 0.31954712]\n",
      " [0.19883153 0.31619963 0.21594764 0.31954712]\n",
      " [0.19883153 0.31619963 0.21594764 0.31954712]]\n",
      "[[0.         0.         0.27659574 0.2       ]\n",
      " [0.04255319 0.14285715 0.7234042  0.71428573]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.34042552 0.74285716 0.40425533 1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.27659574 0.71428573 0.34042552 0.9714286 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.70212764 0.4857143  1.         0.6571429 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.46624377 0.47482264 0.48192686 0.4849483 ]\n",
      " [0.4693773  0.48093218 0.51605946 0.50870246]\n",
      " [0.46723413 0.47525766 0.4690966  0.47570837]\n",
      " [0.46723443 0.47525775 0.46909755 0.47570863]\n",
      " [0.467234   0.47525764 0.46909615 0.47570825]\n",
      " [0.48580003 0.5129718  0.4937703  0.5251894 ]\n",
      " [0.46723402 0.47525764 0.46909627 0.47570828]\n",
      " [0.48132148 0.5127217  0.48690093 0.5224857 ]\n",
      " [0.467234   0.47525764 0.46909615 0.47570825]\n",
      " [0.51903856 0.4977452  0.53382576 0.5086139 ]\n",
      " [0.467234   0.47525764 0.46909615 0.47570825]\n",
      " [0.467234   0.47525764 0.46909615 0.47570825]\n",
      " [0.467234   0.47525764 0.46909615 0.47570825]\n",
      " [0.467234   0.47525764 0.46909615 0.47570825]\n",
      " [0.467234   0.47525764 0.46909615 0.47570825]\n",
      " [0.467234   0.47525764 0.46909615 0.47570825]]\n",
      "[[0.         0.         0.91056913 0.54910713]\n",
      " [0.5203252  0.35267857 1.         1.        ]\n",
      " [0.5691057  0.26339287 0.86178863 0.4955357 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.57723576 0.6785714  0.70731705 0.83928573]\n",
      " [0.46341464 0.50446427 0.6504065  0.6964286 ]\n",
      " [0.49593496 0.7901786  0.699187   0.95089287]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.4133827  0.3389455  0.56834286 0.5587293 ]\n",
      " [0.48975056 0.4602237  0.58852273 0.66121215]\n",
      " [0.47619724 0.45889944 0.51430184 0.5007553 ]\n",
      " [0.41425067 0.34164894 0.41912413 0.34453297]\n",
      " [0.41425064 0.3416489  0.419124   0.3445329 ]\n",
      " [0.41426247 0.34165788 0.41916344 0.34455982]\n",
      " [0.4617355  0.5552278  0.50475484 0.6109357 ]\n",
      " [0.50166255 0.5178064  0.5436609  0.5830251 ]\n",
      " [0.49959606 0.56759787 0.5260309  0.61267793]\n",
      " [0.41425064 0.3416489  0.41912398 0.34453288]\n",
      " [0.41425064 0.3416489  0.41912398 0.34453288]\n",
      " [0.41425064 0.3416489  0.41912398 0.34453288]\n",
      " [0.41425064 0.3416489  0.41912404 0.3445329 ]\n",
      " [0.41425064 0.3416489  0.41912398 0.34453288]\n",
      " [0.41425064 0.3416489  0.41912398 0.34453288]\n",
      " [0.41425064 0.3416489  0.41912398 0.34453288]]\n",
      "[[0.         0.05236908 0.7827869  0.7880299 ]\n",
      " [0.15163934 0.         1.         1.        ]\n",
      " [0.19467214 0.25685784 0.8217213  0.9501247 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.12304911 0.19168109 0.7061404  0.7399699 ]\n",
      " [0.22310047 0.2081136  0.8789394  0.8113826 ]\n",
      " [0.3214037  0.35334945 0.7335428  0.7868985 ]\n",
      " [0.13293262 0.19414353 0.15379396 0.19971398]\n",
      " [0.13293259 0.19414353 0.15379389 0.19971396]\n",
      " [0.1329326  0.19414353 0.15379392 0.19971398]\n",
      " [0.13293259 0.19414353 0.15379389 0.19971396]\n",
      " [0.13293259 0.19414353 0.15379389 0.19971396]\n",
      " [0.13293259 0.19414353 0.15379389 0.19971396]\n",
      " [0.1329326  0.19414353 0.1537939  0.19971396]\n",
      " [0.13293259 0.19414353 0.15379389 0.19971396]\n",
      " [0.13293259 0.19414353 0.15379389 0.19971396]\n",
      " [0.1329326  0.19414353 0.15379392 0.19971398]\n",
      " [0.13293259 0.19414353 0.15379389 0.19971396]\n",
      " [0.13293259 0.19414353 0.15379389 0.19971396]\n",
      " [0.13293259 0.19414353 0.15379389 0.19971396]]\n",
      "[[0.37223974 0.16783217 0.8548896  0.53613055]\n",
      " [0.21766561 0.17715618 0.9621451  0.7785548 ]\n",
      " [0.5457413  0.4055944  0.6940063  0.5011655 ]\n",
      " [0.2933754  0.74125874 0.60883284 1.        ]\n",
      " [0.29652998 0.8857809  0.42902207 0.98834497]\n",
      " [0.         0.5850816  0.29022083 0.86247087]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.79810727 0.         1.         0.1958042 ]\n",
      " [0.39747635 0.30769232 0.6971609  0.53613055]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.43256927 0.30615574 0.64206773 0.5128815 ]\n",
      " [0.36770377 0.2466018  0.66635185 0.63950884]\n",
      " [0.4477921  0.41232646 0.61243147 0.5364405 ]\n",
      " [0.3986976  0.6093469  0.6062833  0.7697705 ]\n",
      " [0.42986962 0.7266656  0.5339891  0.78442883]\n",
      " [0.29150623 0.5521171  0.43803447 0.70474696]\n",
      " [0.27900887 0.19673221 0.29157975 0.20226046]\n",
      " [0.27900392 0.19672951 0.29156414 0.20225306]\n",
      " [0.2790037  0.19672939 0.29156345 0.20225273]\n",
      " [0.27900493 0.19673003 0.2915673  0.20225453]\n",
      " [0.27900428 0.19672969 0.2915652  0.20225355]\n",
      " [0.54107594 0.19852014 0.67496276 0.27746975]\n",
      " [0.4127729  0.410855   0.5136453  0.52162206]\n",
      " [0.2790037  0.19672939 0.29156345 0.20225273]\n",
      " [0.2790037  0.19672939 0.29156345 0.20225273]\n",
      " [0.2790037  0.19672939 0.29156345 0.20225273]]\n",
      "[[0.04954955 0.         0.7522523  0.33333334]\n",
      " [0.03603604 0.2875817  1.         0.86056644]\n",
      " [0.1891892  0.23747277 0.7882883  0.3812636 ]\n",
      " [0.5585586  0.48801744 0.8828829  0.97167754]\n",
      " [0.6036036  0.90631807 0.8063063  0.9477124 ]\n",
      " [0.22972973 0.48801744 0.5855856  1.        ]\n",
      " [0.4009009  0.9106754  0.6081081  0.962963  ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.74509805 0.8918919  0.95642704]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.35297048 0.16244408 0.5675806  0.39809126]\n",
      " [0.34489843 0.31236118 0.66397727 0.7519737 ]\n",
      " [0.38371193 0.31084442 0.5817807  0.4472178 ]\n",
      " [0.5224044  0.49956855 0.62502986 0.82159895]\n",
      " [0.53334653 0.7882494  0.5950839  0.8390341 ]\n",
      " [0.42042208 0.5088324  0.5483942  0.81098455]\n",
      " [0.48662567 0.76344395 0.5603878  0.8320584 ]\n",
      " [0.3414079  0.16743627 0.35082564 0.17378876]\n",
      " [0.34116015 0.16698284 0.35018757 0.17304805]\n",
      " [0.3411602  0.16698289 0.35018772 0.17304818]\n",
      " [0.3411605  0.16698313 0.3501887  0.17304887]\n",
      " [0.38145125 0.58032775 0.5846454  0.7775613 ]\n",
      " [0.3411601  0.16698278 0.3501874  0.17304792]\n",
      " [0.34116006 0.16698277 0.3501873  0.17304787]\n",
      " [0.34116006 0.16698277 0.3501873  0.17304787]\n",
      " [0.34116006 0.16698277 0.3501873  0.17304787]]\n",
      "[[0.31858408 0.3244275  1.         0.63740456]\n",
      " [0.         0.19465649 0.9734513  0.76717556]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.7345133  0.7519084  0.88495576 0.94274807]\n",
      " [0.7699115  0.85877866 0.88495576 0.94274807]\n",
      " [0.39823008 0.7557252  0.7345133  1.        ]\n",
      " [0.5840708  0.9160305  0.7345133  1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.04424779 0.         0.57522124 0.21755725]\n",
      " [0.539823   0.55725193 0.8318584  0.63740456]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.4679298  0.41175574 0.5849405  0.5467404 ]\n",
      " [0.41524556 0.3796146  0.5877462  0.62702745]\n",
      " [0.41500285 0.3001636  0.41983342 0.30380315]\n",
      " [0.54424834 0.62144446 0.5744798  0.68738306]\n",
      " [0.54210514 0.66054654 0.57193077 0.6923826 ]\n",
      " [0.48691636 0.6008551  0.54857624 0.703447  ]\n",
      " [0.5056264  0.6557867  0.5380268  0.6954159 ]\n",
      " [0.415003   0.30016375 0.4198339  0.3038035 ]\n",
      " [0.41500285 0.3001636  0.41983342 0.30380315]\n",
      " [0.41500303 0.30016378 0.41983402 0.30380362]\n",
      " [0.41500297 0.30016372 0.41983378 0.3038034 ]\n",
      " [0.4261092  0.2999139  0.53015375 0.41564378]\n",
      " [0.51755315 0.48088777 0.5669527  0.53577155]\n",
      " [0.41500285 0.3001636  0.41983342 0.30380315]\n",
      " [0.41500285 0.3001636  0.41983342 0.30380315]\n",
      " [0.41500285 0.3001636  0.41983342 0.30380315]]\n",
      "[[0.08743169 0.07920792 1.         0.7260726 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.90710384 0.69636965]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.37704918 0.63366336 0.5355191  0.8151815 ]\n",
      " [0.431694   0.8118812  0.59562844 1.        ]\n",
      " [0.06557377 0.5049505  0.24590164 0.8151815 ]\n",
      " [0.07103825 0.8118812  0.2021858  0.990099  ]\n",
      " [0.8087432  0.55775577 0.91803277 0.76567656]\n",
      " [0.84153    0.7590759  0.91256833 0.90759075]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.6502732  0.71947193 0.76502734 0.9207921 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.38458824 0.293941   0.6267015  0.5791069 ]\n",
      " [0.36234987 0.26889163 0.37017325 0.2731009 ]\n",
      " [0.3623497  0.2688915  0.37017268 0.27310058]\n",
      " [0.36630476 0.28391814 0.603582   0.6122643 ]\n",
      " [0.36234972 0.26889154 0.3701728  0.27310064]\n",
      " [0.47833848 0.5318403  0.53138137 0.6440455 ]\n",
      " [0.4838401  0.6413038  0.51821977 0.7299696 ]\n",
      " [0.37684283 0.51805425 0.43609157 0.64009434]\n",
      " [0.38337582 0.63770604 0.4235481  0.7352841 ]\n",
      " [0.59288925 0.53389025 0.6320113  0.6233483 ]\n",
      " [0.6091878  0.6299485  0.6421023  0.70396143]\n",
      " [0.36236373 0.26890108 0.37021706 0.27312762]\n",
      " [0.5290806  0.6126621  0.5611179  0.6690405 ]\n",
      " [0.3623524  0.2688932  0.3701812  0.2731054 ]\n",
      " [0.3623497  0.2688915  0.37017268 0.27310058]\n",
      " [0.3623497  0.2688915  0.37017268 0.27310058]]\n",
      "[[0.57236844 0.2739726  0.9605263  0.98630136]\n",
      " [0.01973684 0.         0.93421054 0.79452056]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.92105263 0.6164383  1.         1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.28947368 0.5753425  0.4868421  0.7808219 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.12328767 0.25       0.8630137 ]\n",
      " [0.6315789  0.6438356  0.81578946 0.9589041 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.5319997  0.50238234 0.60392344 0.55530304]\n",
      " [0.39089012 0.4499532  0.5981093  0.5102222 ]\n",
      " [0.38845617 0.44567844 0.39479554 0.4466678 ]\n",
      " [0.5898613  0.4879753  0.6151515  0.5177362 ]\n",
      " [0.38845643 0.4456785  0.39479643 0.44666794]\n",
      " [0.38851008 0.44569463 0.39496517 0.44670588]\n",
      " [0.3884561  0.44567844 0.39479542 0.44666776]\n",
      " [0.3884712  0.4456813  0.39484274 0.4466762 ]\n",
      " [0.3884561  0.44567844 0.39479542 0.44666776]\n",
      " [0.4200374  0.51258796 0.45451924 0.5453528 ]\n",
      " [0.38850802 0.44570622 0.39494872 0.44671288]\n",
      " [0.38913622 0.46761084 0.46214554 0.5448739 ]\n",
      " [0.5504083  0.5266042  0.5811775  0.54478157]\n",
      " [0.3884561  0.44567844 0.39479542 0.44666776]\n",
      " [0.3884561  0.44567844 0.39479542 0.44666776]\n",
      " [0.3884561  0.44567844 0.39479542 0.44666776]]\n",
      "[[0.11646587 0.         0.5582329  0.5674157 ]\n",
      " [0.11445783 0.19662921 1.         1.        ]\n",
      " [0.19277108 0.49438202 0.31325302 0.5702247 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.66853935 0.12449799 0.76123595]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.00401606 0.6769663  0.6927711  0.8820225 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.73876405 0.7409639  0.994382  ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.4630142  0.21862127 0.7375623  0.55255616]\n",
      " [0.20117173 0.34864172 0.8708728  0.78318185]\n",
      " [0.2465856  0.35146007 0.40307885 0.49594527]\n",
      " [0.11630206 0.22185276 0.13817257 0.22693324]\n",
      " [0.11627515 0.221844   0.13808475 0.22691031]\n",
      " [0.14154    0.6574271  0.25662568 0.72115016]\n",
      " [0.11627445 0.22184381 0.13808256 0.22690976]\n",
      " [0.1736333  0.60032725 0.532676   0.7113589 ]\n",
      " [0.11627444 0.22184381 0.13808252 0.22690976]\n",
      " [0.11627481 0.2218439  0.13808368 0.22691004]\n",
      " [0.11627441 0.2218438  0.13808243 0.22690973]\n",
      " [0.20889342 0.5721561  0.86445206 0.78318185]\n",
      " [0.11627451 0.22184382 0.13808274 0.22690982]\n",
      " [0.11627441 0.2218438  0.13808243 0.22690973]\n",
      " [0.11627441 0.2218438  0.13808243 0.22690973]\n",
      " [0.11627441 0.2218438  0.13808243 0.22690973]]\n",
      "[[0.7346939  0.         1.         0.75409836]\n",
      " [0.1904762  0.24590164 0.877551   0.9508197 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.20408164 0.852459   0.40136054 1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.4262295  0.21088435 0.75409836]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.54821104 0.45214033 0.61379296 0.5215056 ]\n",
      " [0.39562735 0.47192565 0.5679931  0.5445336 ]\n",
      " [0.3867316  0.4523384  0.3931689  0.45320645]\n",
      " [0.3867316  0.4523384  0.3931689  0.45320642]\n",
      " [0.3867316  0.4523384  0.3931689  0.45320642]\n",
      " [0.3867317  0.45233843 0.39316925 0.45320648]\n",
      " [0.3867316  0.4523384  0.3931689  0.45320642]\n",
      " [0.3867348  0.45233893 0.39317903 0.4532079 ]\n",
      " [0.3867316  0.4523384  0.3931689  0.45320642]\n",
      " [0.41652924 0.534543   0.44545943 0.5485227 ]\n",
      " [0.3867318  0.45233843 0.39316955 0.45320654]\n",
      " [0.38675088 0.503662   0.4757891  0.540144  ]\n",
      " [0.38673162 0.4523384  0.39316905 0.45320645]\n",
      " [0.3867316  0.4523384  0.3931689  0.45320642]\n",
      " [0.3867316  0.4523384  0.3931689  0.45320642]\n",
      " [0.3867316  0.4523384  0.3931689  0.45320642]]\n",
      "[[0.30555555 0.         0.67460316 0.46232876]\n",
      " [0.24603175 0.13356164 0.9880952  0.6267123 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.71825397 0.65753424 0.98015875 0.8458904 ]\n",
      " [0.75396824 0.2260274  0.9722222  0.69863015]\n",
      " [0.6507937  0.77397263 0.88492066 0.9520548 ]\n",
      " [0.11111111 0.6438356  0.31349206 0.8561644 ]\n",
      " [0.17460318 0.2979452  0.3690476  0.7226027 ]\n",
      " [0.         0.7979452  0.22222222 1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.63492066 0.45890412 1.         0.8630137 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.3611111  0.5650685  0.65873015 0.97602737]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.4131749  0.26773974 0.57456475 0.40006477]\n",
      " [0.39351547 0.35333496 0.6853274  0.60704637]\n",
      " [0.30723372 0.27251896 0.31948757 0.2771803 ]\n",
      " [0.60531783 0.5200368  0.70045453 0.6583994 ]\n",
      " [0.6225959  0.4433397  0.70045453 0.5815072 ]\n",
      " [0.62292707 0.6476686  0.6930852  0.69148266]\n",
      " [0.33425045 0.5427978  0.4169278  0.64714384]\n",
      " [0.3357563  0.44256547 0.436277   0.5977607 ]\n",
      " [0.32744363 0.63028306 0.3854728  0.6870567 ]\n",
      " [0.30582592 0.27184954 0.316862   0.27600503]\n",
      " [0.51133204 0.5682144  0.65183353 0.68808055]\n",
      " [0.30582744 0.27185026 0.3168668  0.27600697]\n",
      " [0.30583057 0.27185184 0.3168767  0.27601117]\n",
      " [0.34371972 0.55803573 0.49633256 0.73160726]\n",
      " [0.30582777 0.27185038 0.31686777 0.27600735]\n",
      " [0.30582562 0.27184942 0.316861   0.2760046 ]]\n",
      "[[0.27727273 0.         0.6545454  0.3117284 ]\n",
      " [0.18181819 0.27469134 0.90909094 1.        ]\n",
      " [0.4        0.25617284 0.6363636  0.3611111 ]\n",
      " [0.8590909  0.59876543 1.         0.88580245]\n",
      " [0.79545456 0.30246913 0.98636365 0.63580245]\n",
      " [0.8363636  0.85493827 1.         1.        ]\n",
      " [0.         0.67901236 0.29090908 1.        ]\n",
      " [0.09090909 0.37345678 0.3        0.7160494 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.4371344  0.24844456 0.5560863  0.40457928]\n",
      " [0.40230948 0.3836952  0.66137266 0.7427418 ]\n",
      " [0.47763872 0.3587278  0.5420011  0.4080571 ]\n",
      " [0.59376454 0.5608083  0.66441846 0.6908468 ]\n",
      " [0.5940003  0.41183478 0.67083335 0.5911931 ]\n",
      " [0.6051759  0.6630678  0.6684773  0.7356906 ]\n",
      " [0.34621754 0.5776552  0.40297574 0.6891556 ]\n",
      " [0.3403247  0.4088551  0.43076122 0.60690325]\n",
      " [0.334535   0.25288388 0.34397522 0.25740245]\n",
      " [0.33451876 0.25287408 0.34392345 0.25737488]\n",
      " [0.33451876 0.25287408 0.34392345 0.25737488]\n",
      " [0.3345188  0.25287408 0.34392348 0.25737488]\n",
      " [0.33451876 0.25287408 0.34392345 0.25737488]\n",
      " [0.33451876 0.25287408 0.34392345 0.25737488]\n",
      " [0.33451876 0.25287408 0.34392345 0.25737488]\n",
      " [0.33451876 0.25287408 0.34392345 0.25737488]]\n",
      "[[0.72527474 0.01428571 1.         0.34285715]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.82417583 0.8       ]\n",
      " [0.5824176  0.02857143 0.7912088  0.44285715]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.06593407 0.2857143  0.21978022 1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.536697   0.4483153  0.5672159  0.4802604 ]\n",
      " [0.4348899  0.4492131  0.4385903  0.45013803]\n",
      " [0.4348899  0.4492131  0.4385903  0.45013803]\n",
      " [0.43517482 0.4517334  0.5427431  0.5360263 ]\n",
      " [0.5048848  0.45352048 0.5312413  0.48691046]\n",
      " [0.4348899  0.4492131  0.4385903  0.45013803]\n",
      " [0.4348899  0.4492131  0.4385903  0.45013803]\n",
      " [0.4348899  0.4492131  0.4385903  0.45013803]\n",
      " [0.4348899  0.4492131  0.4385903  0.45013803]\n",
      " [0.4348899  0.4492131  0.4385903  0.45013803]\n",
      " [0.4348899  0.4492131  0.4385903  0.45013803]\n",
      " [0.4348899  0.4492131  0.4385903  0.45013803]\n",
      " [0.4348899  0.4492131  0.4385903  0.45013803]\n",
      " [0.43365735 0.48071885 0.45329526 0.5517045 ]\n",
      " [0.4348899  0.4492131  0.4385903  0.45013803]\n",
      " [0.4348899  0.4492131  0.4385903  0.45013803]]\n",
      "[[0.07058824 0.17192982 0.6156863  1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.6666667  0.3508772 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.5058824  0.04561403 0.8117647  0.56842107]\n",
      " [0.6784314  0.47719297 1.         0.98245615]\n",
      " [0.07843138 0.27017543 0.13333334 0.56491226]\n",
      " [0.04313726 0.45964912 0.1882353  0.9157895 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.5647059  0.39298245 0.7019608  0.8701754 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.5294118  0.27719298 0.627451   0.47719297]\n",
      " [0.57254905 0.81754386 0.68235296 0.877193  ]\n",
      " [0.         0.         0.         0.        ]] [[0.3486933  0.3875052  0.5536232  0.6481255 ]\n",
      " [0.3181374  0.29388756 0.32896733 0.29781947]\n",
      " [0.31756148 0.29323256 0.32795763 0.29700837]\n",
      " [0.3692305  0.29408383 0.597859   0.49023816]\n",
      " [0.3178104  0.2934049  0.32977876 0.29841852]\n",
      " [0.5091495  0.3964956  0.60868454 0.5725431 ]\n",
      " [0.5713351  0.5189197  0.6320686  0.66379905]\n",
      " [0.34385815 0.44481105 0.44959807 0.5723684 ]\n",
      " [0.3443019  0.53446555 0.40813217 0.6551533 ]\n",
      " [0.3175487  0.2932246  0.32791784 0.2969905 ]\n",
      " [0.5808275  0.51279175 0.6208519  0.61615825]\n",
      " [0.3175489  0.2932247  0.32791847 0.29699075]\n",
      " [0.31771687 0.29364333 0.32862598 0.29789537]\n",
      " [0.48252752 0.36216623 0.54955935 0.4390908 ]\n",
      " [0.6488863  0.6942207  0.6883523  0.7105113 ]\n",
      " [0.31914976 0.30010098 0.3309898  0.30457282]]\n",
      "[[0.         0.         0.34444445 0.18978103]\n",
      " [0.11111111 0.06569343 0.98888886 0.810219  ]\n",
      " [0.11111111 0.08759124 0.53333336 0.29927006]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.18888889 0.5985401  0.5777778  0.79562044]\n",
      " [0.17777778 0.7080292  0.56666666 0.79562044]\n",
      " [0.17777778 0.60583943 0.45555556 0.7080292 ]\n",
      " [0.11111111 0.6423358  0.42222223 0.7007299 ]\n",
      " [0.6666667  0.72262776 1.         1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.42914528 0.3910406  0.48323357 0.4359836 ]\n",
      " [0.43662775 0.41054264 0.56170946 0.5500598 ]\n",
      " [0.44366604 0.4104799  0.48842606 0.46049878]\n",
      " [0.4306562  0.39295924 0.43460682 0.39491358]\n",
      " [0.430652   0.39295676 0.4345932  0.3949063 ]\n",
      " [0.45875588 0.52021116 0.5138074  0.5680398 ]\n",
      " [0.46599567 0.55192167 0.5009925  0.5750619 ]\n",
      " [0.45756352 0.5290561  0.4881531  0.5565746 ]\n",
      " [0.45511022 0.540008   0.48598123 0.5591841 ]\n",
      " [0.5293425  0.55368704 0.5715909  0.60897726]\n",
      " [0.43065202 0.39295676 0.43459326 0.3949063 ]\n",
      " [0.430652   0.39295676 0.4345932  0.3949063 ]\n",
      " [0.430652   0.39295676 0.4345932  0.3949063 ]\n",
      " [0.430652   0.39295676 0.4345932  0.3949063 ]\n",
      " [0.430652   0.39295676 0.4345932  0.3949063 ]\n",
      " [0.430652   0.39295676 0.4345932  0.3949063 ]]\n",
      "[[0.31750742 0.         0.7062315  0.42821783]\n",
      " [0.34421366 0.27722773 0.8189911  0.62623763]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.7804154  0.41831684 1.         0.59653467]\n",
      " [0.68249255 0.31930694 0.8189911  0.5866337 ]\n",
      " [0.76261127 0.36633664 0.9643917  0.5470297 ]\n",
      " [0.20771514 0.43069306 0.35014838 0.5569307 ]\n",
      " [0.24332345 0.27227724 0.34124628 0.49257424]\n",
      " [0.16320474 0.5        0.29970327 0.6732673 ]\n",
      " [0.48367953 0.7648515  0.7833828  1.        ]\n",
      " [0.54599404 0.6039604  0.810089   0.8589109 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.03560831 0.46534654 0.2908012  0.8019802 ]\n",
      " [0.16320474 0.43316832 0.40356082 0.5841584 ]\n",
      " [0.         0.7351485  0.18991098 0.87623763]\n",
      " [0.         0.         0.         0.        ]] [[0.4268341  0.2016705  0.56466645 0.40989012]\n",
      " [0.4237206  0.35281944 0.64627415 0.62284106]\n",
      " [0.258879   0.20688719 0.27258438 0.21222626]\n",
      " [0.6124015  0.429421   0.6973484  0.51988435]\n",
      " [0.57976866 0.4313358  0.65855694 0.5992073 ]\n",
      " [0.66173065 0.4270728  0.73944604 0.49354407]\n",
      " [0.354671   0.4235047  0.42831448 0.5313392 ]\n",
      " [0.37185484 0.40341577 0.45161813 0.5076708 ]\n",
      " [0.37180093 0.4765507  0.44191197 0.5507402 ]\n",
      " [0.5555731  0.65829617 0.6755657  0.79689014]\n",
      " [0.5319154  0.57773733 0.71871877 0.744748  ]\n",
      " [0.25891384 0.20691073 0.27269462 0.2122801 ]\n",
      " [0.26075917 0.56696296 0.36195362 0.7269047 ]\n",
      " [0.3657357  0.5319059  0.5069821  0.6833838 ]\n",
      " [0.25464344 0.47478414 0.30400166 0.52808297]\n",
      " [0.2588781  0.20688677 0.27258164 0.21222512]]\n",
      "[[0.5555556  0.         1.         0.5411765 ]\n",
      " [0.33968255 0.43137255 0.9809524  0.99607843]\n",
      " [0.7777778  0.39215687 0.8698413  0.49019608]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.0031746  0.92156863 0.18730159 1.        ]\n",
      " [0.22539683 0.93333334 0.33015874 0.99607843]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.83137256 0.24444444 0.99607843]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.55391216 0.29716417 0.73472214 0.5198359 ]\n",
      " [0.44260728 0.49886245 0.75056815 0.67108357]\n",
      " [0.62484634 0.42216447 0.6984167  0.5115811 ]\n",
      " [0.257282   0.30075893 0.27107623 0.30438763]\n",
      " [0.2572901  0.30076146 0.27110207 0.30439493]\n",
      " [0.2549887  0.6377331  0.4186411  0.7028409 ]\n",
      " [0.25789744 0.6628728  0.329432   0.7028409 ]\n",
      " [0.2572876  0.30076075 0.27109405 0.30439276]\n",
      " [0.259099   0.65114623 0.33722028 0.6903341 ]\n",
      " [0.257282   0.30075893 0.27107623 0.30438763]\n",
      " [0.257282   0.30075893 0.27107623 0.30438763]\n",
      " [0.257282   0.30075893 0.27107623 0.30438763]\n",
      " [0.257282   0.30075893 0.27107623 0.30438763]\n",
      " [0.257282   0.30075893 0.27107623 0.30438763]\n",
      " [0.257282   0.30075893 0.27107623 0.30438763]\n",
      " [0.257282   0.30075893 0.27107623 0.30438763]]\n",
      "[[0.         0.         0.2556391  0.34313726]\n",
      " [0.13909775 0.14215687 0.7781955  0.92156863]\n",
      " [0.13533835 0.15196079 0.28195488 0.3137255 ]\n",
      " [0.20300752 0.60294116 0.33082706 0.8627451 ]\n",
      " [0.20300752 0.7794118  0.2932331  0.8627451 ]\n",
      " [0.31203008 0.6617647  0.36090225 0.76960784]\n",
      " [0.20300752 0.7794118  0.2932331  0.8627451 ]\n",
      " [0.43609023 0.627451   0.66917294 0.92156863]\n",
      " [0.43609023 0.84313726 0.56766915 0.92156863]\n",
      " [0.41353384 0.75490195 0.46616542 0.8186275 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.68421054 0.85294116 1.         1.        ]\n",
      " [0.         0.14215687 0.16541353 0.34313726]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.29882896 0.3455891  0.3978973  0.4432339 ]\n",
      " [0.38364643 0.3779963  0.6079161  0.6236643 ]\n",
      " [0.35134244 0.3786841  0.4154821  0.4959762 ]\n",
      " [0.3692366  0.52246904 0.41957766 0.6166253 ]\n",
      " [0.38520253 0.58681697 0.41388837 0.6141529 ]\n",
      " [0.4118396  0.5546075  0.44819486 0.60412705]\n",
      " [0.41284424 0.5898253  0.43806672 0.61225706]\n",
      " [0.49858966 0.54990077 0.57282615 0.6316322 ]\n",
      " [0.49535143 0.6169429  0.5293694  0.6413728 ]\n",
      " [0.48151618 0.5598085  0.5498903  0.60166067]\n",
      " [0.30482152 0.3482063  0.31596315 0.3509833 ]\n",
      " [0.55296797 0.54816115 0.67569697 0.60176426]\n",
      " [0.31002834 0.38523862 0.3648642  0.44468728]\n",
      " [0.30479825 0.3481973  0.315892   0.350962  ]\n",
      " [0.30479825 0.3481973  0.315892   0.350962  ]\n",
      " [0.30479825 0.3481973  0.315892   0.350962  ]]\n",
      "[[0.58536583 0.         1.         0.72727275]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.09090909 0.7804878  1.        ]\n",
      " [0.5365854  0.33333334 0.76829267 0.8484849 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.50771445 0.4753465  0.56039697 0.51178956]\n",
      " [0.43982503 0.4754437  0.4432449  0.47589093]\n",
      " [0.43982503 0.4754437  0.4432449  0.47589093]\n",
      " [0.43867403 0.48055476 0.53489244 0.525     ]\n",
      " [0.4988812  0.491612   0.52297574 0.51713324]\n",
      " [0.43982503 0.4754437  0.4432449  0.47589093]\n",
      " [0.43982503 0.4754437  0.4432449  0.47589093]\n",
      " [0.43982503 0.4754437  0.4432449  0.47589093]\n",
      " [0.43982503 0.4754437  0.4432449  0.47589093]\n",
      " [0.43982503 0.4754437  0.4432449  0.47589093]\n",
      " [0.43982503 0.4754437  0.4432449  0.47589093]\n",
      " [0.43982503 0.4754437  0.4432449  0.47589093]\n",
      " [0.43982503 0.4754437  0.4432449  0.47589093]\n",
      " [0.43982503 0.4754437  0.4432449  0.47589093]\n",
      " [0.43982503 0.4754437  0.4432449  0.47589093]\n",
      " [0.43982503 0.4754437  0.4432449  0.47589093]]\n",
      "[[0.36734694 0.         1.         0.22885571]\n",
      " [0.0952381  0.13930348 0.9047619  0.6965174 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.04081633 0.2636816  0.27210885 0.52238804]\n",
      " [0.39455783 0.6467662  0.56462586 1.        ]\n",
      " [0.3605442  0.7761194  0.5442177  0.9900498 ]\n",
      " [0.00680272 0.48258707 0.2993197  0.86567163]\n",
      " [0.         0.6666667  0.14285715 0.8557214 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.48350006 0.35548064 0.60072243 0.44412738]\n",
      " [0.4071244  0.38547397 0.57876635 0.5536704 ]\n",
      " [0.39751908 0.35790816 0.40334332 0.360496  ]\n",
      " [0.39752695 0.35791245 0.40336868 0.36050847]\n",
      " [0.3958033  0.41107115 0.46780217 0.5169851 ]\n",
      " [0.47410735 0.5586987  0.5153156  0.6279566 ]\n",
      " [0.47495973 0.5877553  0.5138579  0.6347848 ]\n",
      " [0.40174678 0.50948095 0.45757946 0.6087945 ]\n",
      " [0.4008217  0.5216681  0.44118255 0.56723094]\n",
      " [0.39751935 0.3579083  0.40334418 0.36049643]\n",
      " [0.39751908 0.35790816 0.40334332 0.360496  ]\n",
      " [0.39751908 0.35790816 0.40334332 0.360496  ]\n",
      " [0.39751908 0.35790816 0.40334332 0.360496  ]\n",
      " [0.39751908 0.35790816 0.40334332 0.360496  ]\n",
      " [0.39751908 0.35790816 0.40334332 0.360496  ]\n",
      " [0.39751908 0.35790816 0.40334332 0.360496  ]]\n",
      "[[0.46706587 0.         0.8023952  0.23579545]\n",
      " [0.23353294 0.1590909  0.8622754  0.87215906]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.83233535 0.4943182  1.         0.6818182 ]\n",
      " [0.8023952  0.26988637 0.9700599  0.54829544]\n",
      " [0.8203593  0.65340906 0.98203593 0.82102275]\n",
      " [0.         0.53409094 0.28742516 0.74715906]\n",
      " [0.03592815 0.2528409  0.4011976  0.5823864 ]\n",
      " [0.07185629 0.69602275 0.2994012  0.86647725]\n",
      " [0.44311377 0.92045456 0.61676645 1.        ]\n",
      " [0.4790419  0.85227275 0.6706587  0.9375    ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.28143713 0.9375     0.49101797 0.99715906]\n",
      " [0.28742516 0.8380682  0.52095807 0.9630682 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.50144196 0.23334873 0.598288   0.36983868]\n",
      " [0.39216286 0.36307466 0.5790287  0.62965226]\n",
      " [0.37745085 0.2380678  0.3844207  0.2428417 ]\n",
      " [0.5861161  0.4869528  0.62651515 0.57373345]\n",
      " [0.5754099  0.39722154 0.62651515 0.48886248]\n",
      " [0.60715693 0.6258898  0.62651515 0.6621294 ]\n",
      " [0.3928085  0.5462847  0.4441286  0.6300938 ]\n",
      " [0.39449733 0.39083108 0.45984897 0.52154654]\n",
      " [0.40084073 0.63977516 0.42824274 0.6870601 ]\n",
      " [0.49474725 0.7095171  0.5569016  0.76666665]\n",
      " [0.46983963 0.6279106  0.53584194 0.7284831 ]\n",
      " [0.37747076 0.23809545 0.38448617 0.24290489]\n",
      " [0.43494225 0.6882423  0.50318354 0.7521701 ]\n",
      " [0.4136915  0.633752   0.49110395 0.76647836]\n",
      " [0.3774572  0.2380735  0.3844408  0.24285714]\n",
      " [0.3774485  0.2380659  0.3844134  0.24283639]]\n",
      "[[0.71794873 0.01219512 1.         0.46341464]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.75213677 0.         0.8803419  0.2195122 ]\n",
      " [0.         0.04878049 0.9059829  0.8780488 ]\n",
      " [0.60683763 0.06097561 0.85470086 0.5       ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.04273504 0.6585366  0.62393165 1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.54728496 0.4348339  0.59031177 0.4817449 ]\n",
      " [0.40985122 0.43593127 0.4149824  0.4370999 ]\n",
      " [0.53467137 0.43561587 0.5508674  0.4452597 ]\n",
      " [0.40829238 0.45002097 0.5870459  0.56144285]\n",
      " [0.5270582  0.44942248 0.5727282  0.48837438]\n",
      " [0.4098476  0.4359303  0.41497117 0.4370972 ]\n",
      " [0.4098476  0.4359303  0.41497117 0.4370972 ]\n",
      " [0.4098476  0.4359303  0.41497117 0.4370972 ]\n",
      " [0.4098476  0.4359303  0.41497117 0.4370972 ]\n",
      " [0.4098476  0.4359303  0.41497117 0.4370972 ]\n",
      " [0.4098476  0.4359303  0.41497117 0.4370972 ]\n",
      " [0.4098476  0.4359303  0.41497117 0.4370972 ]\n",
      " [0.4098476  0.4359303  0.41497117 0.4370972 ]\n",
      " [0.4089063  0.5257807  0.4233315  0.553017  ]\n",
      " [0.4098476  0.4359303  0.41497117 0.4370972 ]\n",
      " [0.4098476  0.4359303  0.41497117 0.4370972 ]]\n",
      "[[0.11409396 0.         0.97651005 0.70454544]\n",
      " [0.         0.3778409  1.         1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.34260252 0.233539   0.69959223 0.6082003 ]\n",
      " [0.2763404  0.44981587 0.7257576  0.76666665]\n",
      " [0.28131527 0.2380659  0.29374364 0.24283639]\n",
      " [0.28131527 0.2380659  0.29374364 0.24283639]\n",
      " [0.28131527 0.2380659  0.29374364 0.24283639]\n",
      " [0.28131527 0.2380659  0.29374364 0.24283639]\n",
      " [0.28131527 0.2380659  0.29374364 0.24283639]\n",
      " [0.28131527 0.2380659  0.29374364 0.24283639]\n",
      " [0.28131527 0.2380659  0.29374364 0.24283639]\n",
      " [0.28131533 0.23806593 0.29374385 0.24283648]\n",
      " [0.28131527 0.2380659  0.29374364 0.24283639]\n",
      " [0.28131527 0.2380659  0.29374364 0.24283639]\n",
      " [0.28131527 0.2380659  0.29374364 0.24283639]\n",
      " [0.28131527 0.2380659  0.29374364 0.24283639]\n",
      " [0.28131527 0.2380659  0.29374364 0.24283639]\n",
      " [0.28131527 0.2380659  0.29374364 0.24283639]]\n",
      "[[0.7256637  0.05       1.         0.275     ]\n",
      " [0.1238938  0.19166666 0.9380531  0.65833336]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.6283186  0.59166664 0.9292035  1.        ]\n",
      " [0.8053097  0.9        0.9292035  0.98333335]\n",
      " [0.28318584 0.45       0.50442475 0.975     ]\n",
      " [0.4159292  0.90833336 0.50442475 0.94166666]\n",
      " [0.         0.45833334 0.2920354  0.89166665]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.03539823 0.         0.16814159 0.3       ]\n",
      " [0.8761062  0.15833333 1.         0.25833333]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.5534154  0.40824154 0.587465   0.46360478]\n",
      " [0.44493866 0.43738592 0.5877462  0.52478486]\n",
      " [0.41500285 0.4084719  0.41983348 0.4101389 ]\n",
      " [0.41500288 0.40847194 0.41983357 0.41013893]\n",
      " [0.41500285 0.4084719  0.41983342 0.41013888]\n",
      " [0.5312542  0.5098036  0.58372533 0.5817741 ]\n",
      " [0.5577702  0.5627157  0.5735921  0.57856333]\n",
      " [0.45814538 0.5134652  0.5059342  0.5856064 ]\n",
      " [0.4852034  0.5729338  0.50584394 0.5890176 ]\n",
      " [0.42798516 0.47669116 0.47639582 0.5672964 ]\n",
      " [0.4150029  0.40847194 0.41983366 0.41013896]\n",
      " [0.41234663 0.415407   0.4466561  0.4763863 ]\n",
      " [0.5701132  0.43598112 0.5877462  0.46123415]\n",
      " [0.41500285 0.4084719  0.41983342 0.41013888]\n",
      " [0.41500285 0.4084719  0.41983342 0.41013888]\n",
      " [0.41500285 0.4084719  0.41983342 0.41013888]]\n",
      "[[0.03225806 0.         0.38709676 0.23809524]\n",
      " [0.         0.17460318 1.         1.        ]\n",
      " [0.11290322 0.1904762  0.38709676 0.5555556 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.45132858 0.44996333 0.4844519  0.47730955]\n",
      " [0.4524216  0.46495503 0.5458237  0.5501137 ]\n",
      " [0.46040335 0.46644053 0.48709184 0.4945983 ]\n",
      " [0.4522269  0.4507757  0.454942   0.45167223]\n",
      " [0.4522269  0.4507757  0.454942   0.45167223]\n",
      " [0.45222694 0.4507757  0.454942   0.45167223]\n",
      " [0.4522269  0.4507757  0.454942   0.45167223]\n",
      " [0.4522269  0.4507757  0.454942   0.45167223]\n",
      " [0.4522269  0.4507757  0.454942   0.45167223]\n",
      " [0.45222694 0.4507757  0.454942   0.45167223]\n",
      " [0.4522269  0.4507757  0.454942   0.45167223]\n",
      " [0.4522269  0.4507757  0.454942   0.45167223]\n",
      " [0.4522269  0.4507757  0.454942   0.45167223]\n",
      " [0.4522269  0.4507757  0.454942   0.45167223]\n",
      " [0.4522269  0.4507757  0.454942   0.45167223]\n",
      " [0.4522269  0.4507757  0.454942   0.45167223]]\n",
      "[[0.7234042  0.         1.         0.20588236]\n",
      " [0.27659574 0.11764706 0.9574468  0.7352941 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.65957445 0.7352941  0.7234042  1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.5744681  0.7352941  0.63829786 1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.5        0.29787233 0.7058824 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.51476467 0.4742529  0.5356061  0.48461166]\n",
      " [0.48725352 0.48052353 0.5337643  0.51174957]\n",
      " [0.46550944 0.4746996  0.46746966 0.47516036]\n",
      " [0.46550944 0.47469956 0.46746963 0.47516036]\n",
      " [0.46550944 0.47469956 0.4674696  0.47516036]\n",
      " [0.5120976  0.51386076 0.5190003  0.5251766 ]\n",
      " [0.46550944 0.47469956 0.4674696  0.47516036]\n",
      " [0.50598454 0.5137508  0.5121964  0.52575755]\n",
      " [0.46550944 0.47469956 0.4674696  0.47516036]\n",
      " [0.46585897 0.5005553  0.48566523 0.51252353]\n",
      " [0.46550944 0.47469956 0.4674696  0.47516036]\n",
      " [0.46550944 0.4746996  0.4674697  0.47516036]\n",
      " [0.46550944 0.47469956 0.4674696  0.47516036]\n",
      " [0.46550944 0.47469956 0.4674696  0.47516036]\n",
      " [0.46550944 0.47469956 0.4674696  0.47516036]\n",
      " [0.46550944 0.47469956 0.4674696  0.47516036]]\n",
      "[[0.7027027  0.22404371 1.         0.6612022 ]\n",
      " [0.09797297 0.04918033 0.8547297  0.8196721 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.6722973  0.75409836 0.8074324  0.96174866]\n",
      " [0.6722973  0.87431693 0.7432432  0.9672131 ]\n",
      " [0.4560811  0.73224044 0.5574324  0.9781421 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.5472973  0.8142077  0.6824324  1.        ]\n",
      " [0.5777027  0.91803277 0.6858108  1.        ]\n",
      " [0.3277027  0.6010929  0.4831081  0.9398907 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.1554054  0.4918033 ]\n",
      " [0.8513514  0.33333334 0.9695946  0.5355191 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.5589826  0.40525773 0.7124466  0.5571469 ]\n",
      " [0.32044706 0.37424377 0.65937936 0.59501064]\n",
      " [0.27735254 0.3604196  0.29000613 0.3629617 ]\n",
      " [0.60161924 0.56477404 0.6472329  0.6421023 ]\n",
      " [0.5616305  0.59215677 0.59947467 0.62829286]\n",
      " [0.55427134 0.5541159  0.6099863  0.63504255]\n",
      " [0.27735272 0.36041963 0.29000664 0.36296183]\n",
      " [0.4872279  0.57351565 0.5460019  0.6419389 ]\n",
      " [0.48024088 0.603282   0.51806927 0.62498415]\n",
      " [0.42390198 0.5456595  0.5179733  0.63889945]\n",
      " [0.27736357 0.36042282 0.29004043 0.36296967]\n",
      " [0.2746008  0.3908409  0.4246481  0.49818844]\n",
      " [0.60886526 0.48121318 0.6632203  0.5296139 ]\n",
      " [0.27735254 0.3604196  0.2900061  0.3629617 ]\n",
      " [0.27735254 0.3604196  0.2900061  0.3629617 ]\n",
      " [0.27735254 0.3604196  0.2900061  0.3629617 ]]\n",
      "[[0.         0.         0.43506494 0.32374102]\n",
      " [0.04545455 0.176259   0.97402596 0.5647482 ]\n",
      " [0.3051948  0.17985612 0.43506494 0.33093524]\n",
      " [0.32467532 0.49280575 0.66233766 0.7338129 ]\n",
      " [0.48701298 0.23381294 0.78571427 0.55755395]\n",
      " [0.19480519 0.68345326 0.45454547 0.89568347]\n",
      " [0.13636364 0.53956836 0.4090909  0.73021585]\n",
      " [0.09740259 0.4028777  0.3961039  0.58633095]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.47402596 0.74820143 0.85714287 0.98920864]\n",
      " [0.43506494 0.44604316 1.         0.79856116]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.35064936 0.9028777  0.5974026  1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.3845831  0.2841442  0.5189782  0.41723427]\n",
      " [0.42487034 0.37325597 0.6195833  0.55092907]\n",
      " [0.43023032 0.3639351  0.4880737  0.396959  ]\n",
      " [0.43854508 0.49680704 0.5212387  0.5770309 ]\n",
      " [0.48409158 0.3880215  0.56430185 0.52074575]\n",
      " [0.4351011  0.6001438  0.4807962  0.63770807]\n",
      " [0.42005157 0.5427814  0.48489028 0.59872246]\n",
      " [0.4292944  0.46377683 0.45509973 0.54132277]\n",
      " [0.38416746 0.28796312 0.39075997 0.2918304 ]\n",
      " [0.51030046 0.6278838  0.58950704 0.7158712 ]\n",
      " [0.49025735 0.5894734  0.6159694  0.7131308 ]\n",
      " [0.38416377 0.28796026 0.3907483  0.29182282]\n",
      " [0.49150056 0.66012967 0.5539022  0.7158712 ]\n",
      " [0.3845111  0.2883584  0.39141095 0.29242426]\n",
      " [0.3841632  0.28795987 0.3907465  0.2918217 ]\n",
      " [0.38416317 0.28795987 0.39074644 0.29182166]]\n",
      "[[0.         0.         0.46212122 0.527027  ]\n",
      " [0.2651515  0.3783784  1.         1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.36363637 0.16216215 0.84090906 0.6621622 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.40251195 0.44546497 0.48926133 0.5041392 ]\n",
      " [0.44175658 0.47814575 0.5975     0.55465907]\n",
      " [0.40555465 0.44631097 0.41092235 0.4472888 ]\n",
      " [0.44261944 0.45816144 0.54531693 0.5176753 ]\n",
      " [0.4055546  0.44631097 0.41092217 0.44728878]\n",
      " [0.4055546  0.44631097 0.41092217 0.44728878]\n",
      " [0.4055546  0.44631097 0.41092214 0.44728878]\n",
      " [0.4055546  0.44631097 0.41092214 0.44728878]\n",
      " [0.4055546  0.44631097 0.41092214 0.44728878]\n",
      " [0.40555465 0.44631097 0.4109223  0.4472888 ]\n",
      " [0.4055546  0.44631097 0.41092214 0.44728878]\n",
      " [0.4055546  0.44631097 0.41092214 0.44728878]\n",
      " [0.4055546  0.44631097 0.41092214 0.44728878]\n",
      " [0.4055546  0.44631097 0.41092214 0.44728878]\n",
      " [0.4055546  0.44631097 0.41092214 0.44728878]\n",
      " [0.4055546  0.44631097 0.41092214 0.44728878]]\n",
      "[[0.         0.         0.18548387 0.40151516]\n",
      " [0.6653226  0.95454544 0.7419355  1.        ]\n",
      " [0.51209676 0.8939394  0.58064514 0.9583333 ]\n",
      " [0.09677419 0.03030303 1.         0.5984849 ]\n",
      " [0.16935484 0.03030303 0.51209676 0.38636363]\n",
      " [0.61290324 0.5416667  0.69758064 0.78409094]\n",
      " [0.6653226  0.7878788  0.7419355  1.        ]\n",
      " [0.46774194 0.5378788  0.58064514 0.6818182 ]\n",
      " [0.516129   0.67424244 0.5927419  0.9583333 ]\n",
      " [0.8991935  0.40151516 0.9798387  0.56439394]\n",
      " [0.9314516  0.54924244 0.9919355  0.8068182 ]\n",
      " [0.8266129  0.45075756 0.92741936 0.594697  ]\n",
      " [0.86290324 0.5681818  0.9314516  0.78409094]\n",
      " [0.83064514 0.56060606 0.9596774  0.7689394 ]\n",
      " [0.9435484  0.7613636  0.9919355  0.8068182 ]\n",
      " [0.87096775 0.7462121  0.9233871  0.7878788 ]] [[0.3170485  0.3130498  0.40101865 0.44581234]\n",
      " [0.57406557 0.6686593  0.6020447  0.68156505]\n",
      " [0.5325186  0.6576766  0.559077   0.67178935]\n",
      " [0.37042677 0.31516686 0.675541   0.5310835 ]\n",
      " [0.37441528 0.33274773 0.46411246 0.47596872]\n",
      " [0.5197777  0.49672824 0.5588144  0.5905333 ]\n",
      " [0.55689156 0.5887344  0.5999223  0.691056  ]\n",
      " [0.5008275  0.49418628 0.5434332  0.59262204]\n",
      " [0.5275144  0.5973923  0.5626564  0.67804956]\n",
      " [0.63601196 0.44308823 0.67357767 0.51913875]\n",
      " [0.65144116 0.50769746 0.67408967 0.6039058 ]\n",
      " [0.6104567  0.45802623 0.6405909  0.5263865 ]\n",
      " [0.6378814  0.5153421  0.66415143 0.6019821 ]\n",
      " [0.64592546 0.436418   0.6787523  0.5290514 ]\n",
      " [0.64762217 0.58601046 0.6686269  0.6075807 ]\n",
      " [0.64241886 0.58358115 0.6633501  0.59906304]]\n",
      "[[0.03558719 0.         0.6939502  0.5538462 ]\n",
      " [0.         0.46153846 0.886121   0.9948718 ]\n",
      " [0.01067616 0.4076923  0.48398575 0.7205128 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.7580071  0.7025641  1.         1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.29142228 0.18977603 0.5777522  0.5301045 ]\n",
      " [0.27880666 0.48614034 0.668329   0.8102273 ]\n",
      " [0.30515033 0.43150386 0.48829752 0.6185675 ]\n",
      " [0.28353664 0.19532618 0.29596362 0.20093279]\n",
      " [0.59701645 0.6280339  0.7150218  0.8041111 ]\n",
      " [0.28348014 0.19527835 0.29578546 0.2008281 ]\n",
      " [0.28348014 0.19527835 0.29578546 0.2008281 ]\n",
      " [0.28348014 0.19527835 0.29578546 0.2008281 ]\n",
      " [0.28348014 0.19527835 0.29578546 0.2008281 ]\n",
      " [0.28348014 0.19527835 0.29578546 0.2008281 ]\n",
      " [0.28348014 0.19527835 0.29578546 0.2008281 ]\n",
      " [0.28348014 0.19527835 0.29578546 0.2008281 ]\n",
      " [0.28348014 0.19527835 0.2957855  0.20082812]\n",
      " [0.28348014 0.19527835 0.29578546 0.2008281 ]\n",
      " [0.28348014 0.19527835 0.29578546 0.2008281 ]\n",
      " [0.28348014 0.19527835 0.29578546 0.2008281 ]]\n",
      "[[0.07837838 0.         0.91351354 0.7117904 ]\n",
      " [0.         0.29257643 0.84594595 1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.71081084 0.33842796 1.         0.78165936]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.24716891 0.15303326 0.7274428  0.6841164 ]\n",
      " [0.22749518 0.4248815  0.7037257  0.8213523 ]\n",
      " [0.22847879 0.15918799 0.24391006 0.16539508]\n",
      " [0.22848168 0.1591894  0.24391913 0.165399  ]\n",
      " [0.6330831  0.46770167 0.77282006 0.784322  ]\n",
      " [0.22847876 0.15918796 0.24390994 0.16539502]\n",
      " [0.22847876 0.15918796 0.24390994 0.16539502]\n",
      " [0.22848277 0.1591899  0.24392259 0.16540046]\n",
      " [0.22847876 0.15918796 0.24390994 0.16539502]\n",
      " [0.22847876 0.15918796 0.24390994 0.16539502]\n",
      " [0.22847876 0.15918796 0.24390994 0.16539502]\n",
      " [0.22847876 0.15918796 0.24390994 0.16539502]\n",
      " [0.22847876 0.15918796 0.24390994 0.16539502]\n",
      " [0.22847876 0.15918796 0.24390994 0.16539502]\n",
      " [0.22847876 0.15918796 0.24390994 0.16539502]\n",
      " [0.22847876 0.15918796 0.24390994 0.16539502]]\n",
      "[[0.566879   0.26086956 0.9426752  1.        ]\n",
      " [0.02547771 0.         0.9044586  0.85507244]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.91719747 0.5942029  1.         1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.3057325  0.6376812  0.48407644 0.884058  ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.24637681 0.24840765 1.        ]\n",
      " [0.6369427  0.6666667  0.8089172  0.98550725]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.5384047  0.49032584 0.60642046 0.5509659 ]\n",
      " [0.39070424 0.45335606 0.5927928  0.5124122 ]\n",
      " [0.38766742 0.44993863 0.39405188 0.4508504 ]\n",
      " [0.5919673  0.48531753 0.6159659  0.5139353 ]\n",
      " [0.3876678  0.44993868 0.39405307 0.4508506 ]\n",
      " [0.38773328 0.44995782 0.39426026 0.45089507]\n",
      " [0.38766724 0.4499386  0.39405137 0.45085034]\n",
      " [0.38770372 0.44994545 0.39416412 0.45087045]\n",
      " [0.38766724 0.4499386  0.39405137 0.45085034]\n",
      " [0.42125145 0.5163273  0.4552849  0.5491236 ]\n",
      " [0.3877613  0.45000532 0.3943147  0.45094547]\n",
      " [0.38836437 0.48921287 0.45599777 0.5509659 ]\n",
      " [0.536488   0.52490646 0.5645426  0.54323864]\n",
      " [0.38766724 0.4499386  0.39405137 0.45085034]\n",
      " [0.38766724 0.4499386  0.39405137 0.45085034]\n",
      " [0.38766724 0.4499386  0.39405137 0.45085034]]\n",
      "[[0.         0.         0.8763251  0.8       ]\n",
      " [0.15547703 0.72987014 1.         1.        ]\n",
      " [0.27915195 0.71428573 0.77031803 0.8805195 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.296591   0.22325513 0.6768831  0.69173247]\n",
      " [0.3815207  0.63008296 0.70367426 0.77708334]\n",
      " [0.39872947 0.612651   0.5747653  0.7165935 ]\n",
      " [0.30270678 0.22783408 0.31391945 0.23279093]\n",
      " [0.30270678 0.22783409 0.31391948 0.23279095]\n",
      " [0.30270675 0.22783408 0.3139194  0.23279092]\n",
      " [0.30270675 0.22783408 0.3139194  0.23279092]\n",
      " [0.3027068  0.22783409 0.3139195  0.23279096]\n",
      " [0.30270675 0.22783408 0.3139194  0.23279092]\n",
      " [0.30270675 0.22783408 0.3139194  0.23279092]\n",
      " [0.30270675 0.22783408 0.3139194  0.23279092]\n",
      " [0.30270675 0.22783408 0.3139194  0.23279092]\n",
      " [0.30270693 0.22783417 0.3139199  0.23279116]\n",
      " [0.30270675 0.22783408 0.3139194  0.23279092]\n",
      " [0.30270675 0.22783408 0.3139194  0.23279092]\n",
      " [0.30270675 0.22783408 0.3139194  0.23279092]]\n",
      "[[0.6188119  0.         0.95049506 0.18895349]\n",
      " [0.17821783 0.10465116 0.9405941  0.8255814 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.4950495  0.7005814  1.         0.82848835]\n",
      " [0.48514852 0.7267442  0.990099   0.85465115]\n",
      " [0.         0.72965115 0.34158415 1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.54988605 0.24636601 0.63891983 0.34408855]\n",
      " [0.41146782 0.31494206 0.64920455 0.65702045]\n",
      " [0.35546994 0.25041845 0.36368394 0.254964  ]\n",
      " [0.35546997 0.25041845 0.36368397 0.254964  ]\n",
      " [0.35547304 0.25042048 0.36369377 0.2549698 ]\n",
      " [0.35546997 0.25041845 0.36368397 0.254964  ]\n",
      " [0.355474   0.25042114 0.36369672 0.25497153]\n",
      " [0.5239859  0.5695198  0.63575137 0.6986972 ]\n",
      " [0.5558698  0.6703094  0.61301863 0.71416235]\n",
      " [0.3513663  0.63269305 0.45302132 0.7540909 ]\n",
      " [0.35546997 0.25041848 0.363684   0.25496402]\n",
      " [0.35546994 0.25041845 0.36368394 0.25496396]\n",
      " [0.35546994 0.25041845 0.36368394 0.25496396]\n",
      " [0.35546994 0.25041845 0.36368394 0.25496396]\n",
      " [0.35546994 0.25041845 0.36368394 0.25496396]\n",
      " [0.35546994 0.25041845 0.36368394 0.25496396]]\n",
      "[[0.3104213  0.         1.         0.56639564]\n",
      " [0.41906872 0.45257452 0.7871397  0.8102981 ]\n",
      " [0.4789357  0.48238483 0.7871397  0.6395664 ]\n",
      " [0.58536583 0.6693767  0.9046563  0.99458   ]\n",
      " [0.63192904 0.7886179  0.902439   1.        ]\n",
      " [0.         0.11111111 0.52993345 0.6856369 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.38839197 0.23493463 0.77331257 0.5492353 ]\n",
      " [0.3916003  0.4503175  0.6248409  0.6370646 ]\n",
      " [0.52949446 0.42876095 0.7097117  0.54291284]\n",
      " [0.47926375 0.5728715  0.7020495  0.7073503 ]\n",
      " [0.49644774 0.6723467  0.65884256 0.763903  ]\n",
      " [0.19606312 0.4177816  0.4015621  0.6254494 ]\n",
      " [0.18558738 0.23914541 0.20345984 0.24389721]\n",
      " [0.1855857  0.23914489 0.20345461 0.24389572]\n",
      " [0.1855857  0.23914488 0.20345461 0.24389572]\n",
      " [0.18558572 0.23914489 0.20345464 0.24389574]\n",
      " [0.18558572 0.23914489 0.20345461 0.24389572]\n",
      " [0.18558572 0.23914489 0.20345461 0.24389572]\n",
      " [0.1855857  0.23914488 0.20345461 0.24389572]\n",
      " [0.1855857  0.23914488 0.20345461 0.24389572]\n",
      " [0.1855857  0.23914488 0.20345461 0.24389572]\n",
      " [0.1855857  0.23914488 0.20345461 0.24389572]]\n",
      "[[0.2682927  0.         0.6926829  0.34302327]\n",
      " [0.12195122 0.23837209 0.77560973 0.7790698 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.70731705 0.51744187 0.99512196 0.6976744 ]\n",
      " [0.8        0.5813953  1.         0.70348835]\n",
      " [0.3512195  0.65697676 0.5463415  1.        ]\n",
      " [0.43414634 0.872093   0.55609757 0.99418604]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.07804878 0.5813953  0.26341462 0.872093  ]\n",
      " [0.07804878 0.73255813 0.16585366 0.85465115]\n",
      " [0.         0.5813953  0.13658537 0.75      ]\n",
      " [0.36097562 0.0872093  0.6097561  0.30232558]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.40691984 0.36646503 0.5466054  0.45052764]\n",
      " [0.38120827 0.42964426 0.5784054  0.5806476 ]\n",
      " [0.34580165 0.36880973 0.35456523 0.37119907]\n",
      " [0.56411827 0.51224196 0.6395438  0.5724977 ]\n",
      " [0.5817655  0.54386073 0.6346569  0.5725464 ]\n",
      " [0.43955332 0.5348091  0.52720904 0.6335606 ]\n",
      " [0.46898133 0.6053199  0.51103896 0.6335606 ]\n",
      " [0.34580162 0.36880973 0.3545651  0.37119904]\n",
      " [0.3458016  0.3688097  0.35456505 0.371199  ]\n",
      " [0.35666585 0.54482013 0.41572338 0.5976503 ]\n",
      " [0.37642443 0.5813595  0.4058027  0.60304856]\n",
      " [0.34574577 0.52650154 0.40133145 0.5718957 ]\n",
      " [0.42588285 0.3973475  0.48787135 0.45091072]\n",
      " [0.3458016  0.3688097  0.35456505 0.371199  ]\n",
      " [0.3458016  0.3688097  0.35456505 0.371199  ]\n",
      " [0.3458016  0.3688097  0.35456505 0.371199  ]]\n",
      "[[0.         0.         0.7711864  0.5589225 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.2881356  0.08080808 1.         0.73400676]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.7330508  0.6296296  0.8559322  0.9124579 ]\n",
      " [0.7711864  0.8956229  0.84745765 0.97979796]\n",
      " [0.3940678  0.64646465 0.5508475  0.9292929 ]\n",
      " [0.47457626 0.9225589  0.5635593  1.        ]\n",
      " [0.8601695  0.54882157 0.9618644  0.79461277]\n",
      " [0.88559324 0.7811448  0.9618644  0.97306395]\n",
      " [0.8220339  0.5959596  0.88135594 0.7979798 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.91525424 0.65656567 0.9830508  0.9292929 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.3134835  0.26619953 0.57938874 0.5169846 ]\n",
      " [0.31816107 0.26794615 0.32851037 0.2721785 ]\n",
      " [0.31835878 0.2681724  0.32908028 0.27257225]\n",
      " [0.428616   0.28206533 0.6877273  0.6012045 ]\n",
      " [0.3181543  0.26794282 0.32848942 0.27216932]\n",
      " [0.586915   0.5800049  0.6287984  0.68843365]\n",
      " [0.5650177  0.6776027  0.6013762  0.73625004]\n",
      " [0.4439335  0.57498646 0.4899819  0.66158956]\n",
      " [0.48710018 0.66369283 0.5209521  0.73586583]\n",
      " [0.641785   0.5075629  0.6701268  0.59183943]\n",
      " [0.6565409  0.6143781  0.68436646 0.69015825]\n",
      " [0.5810422  0.57516134 0.60325015 0.6357074 ]\n",
      " [0.3181613  0.26794666 0.3285112  0.27217934]\n",
      " [0.649125   0.5814221  0.67514884 0.6704134 ]\n",
      " [0.3181544  0.26794285 0.32848966 0.27216944]\n",
      " [0.31815413 0.26794273 0.32848886 0.27216908]]\n",
      "[[0.         0.01796407 0.7119114  0.6437126 ]\n",
      " [0.53185594 0.         0.9889197  0.8203593 ]\n",
      " [0.53462607 0.00898204 0.8282548  0.55688626]\n",
      " [0.8725762  0.8473054  1.         1.        ]\n",
      " [0.8725762  0.8473054  1.         1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.22552113 0.24080963 0.6510506  0.6036029 ]\n",
      " [0.4452836  0.25862965 0.7306672  0.6893623 ]\n",
      " [0.57502127 0.26944035 0.7697873  0.5825814 ]\n",
      " [0.6929134  0.6051122  0.78032196 0.7266367 ]\n",
      " [0.66175145 0.64838797 0.7576243  0.7286489 ]\n",
      " [0.22846861 0.2452497  0.24391833 0.24989474]\n",
      " [0.22846039 0.24524672 0.24389262 0.24988644]\n",
      " [0.22846039 0.24524672 0.24389262 0.24988644]\n",
      " [0.22846039 0.24524672 0.24389262 0.24988644]\n",
      " [0.2284604  0.24524672 0.24389264 0.24988644]\n",
      " [0.2284604  0.24524672 0.24389262 0.24988644]\n",
      " [0.2284604  0.24524672 0.24389262 0.24988644]\n",
      " [0.22846138 0.24524708 0.24389575 0.24988744]\n",
      " [0.22846039 0.24524672 0.24389262 0.24988644]\n",
      " [0.22846039 0.24524672 0.24389262 0.24988644]\n",
      " [0.22846039 0.24524672 0.24389262 0.24988644]]\n",
      "[[0.         0.         1.         0.877193  ]\n",
      " [0.3432836  0.5814536  0.9594883  1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.00852878 0.2857143  0.48614073 0.877193  ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.12839328 0.18272933 0.8311932  0.7588483 ]\n",
      " [0.4097167  0.57841444 0.8730682  0.8173864 ]\n",
      " [0.13861987 0.18824631 0.15915796 0.19392414]\n",
      " [0.13861986 0.18824631 0.15915793 0.19392414]\n",
      " [0.13861986 0.18824631 0.15915793 0.19392414]\n",
      " [0.13861986 0.18824631 0.15915793 0.19392414]\n",
      " [0.13861986 0.18824631 0.15915793 0.19392414]\n",
      " [0.13861986 0.18824631 0.15915793 0.19392414]\n",
      " [0.13861986 0.18824631 0.15915793 0.19392414]\n",
      " [0.13861986 0.18824631 0.15915793 0.19392414]\n",
      " [0.13861986 0.18824631 0.15915793 0.19392414]\n",
      " [0.13861986 0.18824631 0.15915793 0.19392414]\n",
      " [0.14317413 0.39505    0.484532   0.70128417]\n",
      " [0.13861986 0.18824631 0.15915793 0.19392414]\n",
      " [0.13861986 0.18824631 0.15915793 0.19392414]\n",
      " [0.13861986 0.18824631 0.15915793 0.19392414]]\n",
      "[[0.54113925 0.1423077  1.         0.84615386]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.8259494  1.        ]\n",
      " [0.17721519 0.         0.8449367  0.5653846 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.17088607 0.86923075 0.45253164 1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.52310956 0.3449764  0.731635   0.60437274]\n",
      " [0.2739035  0.3113628  0.2867531  0.31479836]\n",
      " [0.2739035  0.3113628  0.2867531  0.31479836]\n",
      " [0.26949364 0.31218582 0.6799428  0.66404784]\n",
      " [0.34421903 0.3183514  0.58215225 0.59226656]\n",
      " [0.27392092 0.31136996 0.28680867 0.31481624]\n",
      " [0.2739035  0.3113628  0.2867531  0.31479836]\n",
      " [0.3314298  0.63768756 0.40892082 0.69204545]\n",
      " [0.2739035  0.3113628  0.2867531  0.31479836]\n",
      " [0.2739035  0.3113628  0.2867531  0.31479836]\n",
      " [0.2739035  0.3113628  0.2867531  0.31479836]\n",
      " [0.27390358 0.31136283 0.28675342 0.31479844]\n",
      " [0.2739035  0.3113628  0.2867531  0.31479836]\n",
      " [0.2739035  0.3113628  0.2867531  0.31479836]\n",
      " [0.2739035  0.3113628  0.2867531  0.31479836]\n",
      " [0.2739035  0.3113628  0.2867531  0.31479836]]\n",
      "[[0.         0.1691843  0.32207793 0.5226586 ]\n",
      " [0.09090909 0.30513597 0.8675325  0.90332323]\n",
      " [0.1038961  0.31722054 0.37402597 0.6767372 ]\n",
      " [0.3922078  0.70090634 0.54545456 1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.21558441 0.7915408  0.33766234 0.9848943 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.74025977 0.5981873  1.         0.8429003 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.6207792  0.6495468  0.7558442  0.9697885 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.66233766 0.         0.9506493  0.42598188]\n",
      " [0.         0.35951662 0.19480519 0.51963747]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.22426929 0.29553667 0.3794899  0.4791034 ]\n",
      " [0.2433471  0.43499625 0.7097954  0.7018496 ]\n",
      " [0.28222907 0.4653416  0.41856802 0.6082575 ]\n",
      " [0.4534737  0.58467054 0.5349212  0.729388  ]\n",
      " [0.23159818 0.2660082  0.24685347 0.27027017]\n",
      " [0.309332   0.61095136 0.39387208 0.72994936]\n",
      " [0.23159781 0.26600808 0.24685232 0.27026984]\n",
      " [0.6330919  0.55193126 0.7337384  0.7193583 ]\n",
      " [0.23159754 0.266008   0.24685149 0.27026957]\n",
      " [0.5628426  0.6228739  0.696548   0.73821974]\n",
      " [0.23160157 0.26600942 0.24686405 0.27027342]\n",
      " [0.523617   0.29285792 0.7489616  0.46514848]\n",
      " [0.24785721 0.40452474 0.34006107 0.489976  ]\n",
      " [0.23159754 0.266008   0.24685147 0.27026957]\n",
      " [0.23159754 0.266008   0.24685147 0.27026957]\n",
      " [0.23159754 0.266008   0.24685147 0.27026957]]\n",
      "[[0.05238095 0.         0.45238096 0.44489795]\n",
      " [0.         0.3510204  0.7095238  1.        ]\n",
      " [0.13333334 0.35510203 0.1952381  0.4244898 ]\n",
      " [0.7095238  0.8122449  1.         0.96734697]\n",
      " [0.5619048  0.51836735 0.88095236 0.922449  ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.35251674 0.31443295 0.474751   0.5100828 ]\n",
      " [0.34606785 0.43033785 0.5467753  0.655862  ]\n",
      " [0.38708112 0.43057    0.44316068 0.48893237]\n",
      " [0.55035186 0.5725281  0.652104   0.6673061 ]\n",
      " [0.54197323 0.47574794 0.65808153 0.62373877]\n",
      " [0.34589332 0.3176879  0.35465157 0.32100827]\n",
      " [0.3458933  0.3176879  0.35465154 0.32100827]\n",
      " [0.3458933  0.3176879  0.35465154 0.32100827]\n",
      " [0.3458933  0.3176879  0.35465154 0.32100827]\n",
      " [0.3458933  0.3176879  0.35465154 0.32100827]\n",
      " [0.3458933  0.3176879  0.35465154 0.32100827]\n",
      " [0.3458933  0.3176879  0.35465154 0.32100827]\n",
      " [0.3458933  0.3176879  0.35465154 0.32100827]\n",
      " [0.3458933  0.3176879  0.35465154 0.32100827]\n",
      " [0.3458933  0.3176879  0.35465154 0.32100827]\n",
      " [0.3458933  0.3176879  0.35465154 0.32100827]]\n",
      "[[0.47826087 0.         1.         0.31578946]\n",
      " [0.         0.05263158 0.9130435  0.59210527]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.6956522  0.55263156 0.8695652  0.9868421 ]\n",
      " [0.6956522  0.8947368  0.8695652  0.9868421 ]\n",
      " [0.41304347 0.56578946 0.5869565  1.        ]\n",
      " [0.45652175 0.9605263  0.5869565  1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.39473686 0.10869565 0.84210527]\n",
      " [0.         0.7763158  0.08695652 0.84210527]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.5023022  0.44290295 0.5357197  0.47715107]\n",
      " [0.46557695 0.44631356 0.52116346 0.4981093 ]\n",
      " [0.4653994  0.44203222 0.46736592 0.443088  ]\n",
      " [0.5089216  0.51793617 0.5223194  0.558969  ]\n",
      " [0.51605725 0.5460609  0.5268722  0.5538372 ]\n",
      " [0.49237213 0.5083906  0.50816023 0.55346924]\n",
      " [0.4923727  0.54749    0.5022506  0.55523634]\n",
      " [0.46539938 0.4420322  0.46736583 0.44308794]\n",
      " [0.4653994  0.44203222 0.46736592 0.443088  ]\n",
      " [0.46541956 0.48980623 0.4745909  0.5472555 ]\n",
      " [0.466069   0.5388775  0.47225165 0.5471431 ]\n",
      " [0.46540004 0.4420326  0.46736792 0.44308913]\n",
      " [0.46539956 0.4420323  0.46736643 0.44308826]\n",
      " [0.46539938 0.4420322  0.46736583 0.44308794]\n",
      " [0.46539938 0.4420322  0.46736583 0.44308794]\n",
      " [0.46539938 0.4420322  0.46736583 0.44308794]]\n",
      "[[0.48963732 0.17295597 1.         0.7924528 ]\n",
      " [0.00259067 0.         0.6683938  0.7421384 ]\n",
      " [0.38860103 0.12264151 0.6398964  0.7421384 ]\n",
      " [0.         0.6163522  0.27202073 0.990566  ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.11139897 0.42767295 0.47409326 1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.43464184 0.2673941  0.77345717 0.65363765]\n",
      " [0.20596138 0.25886    0.5156554  0.6749046 ]\n",
      " [0.3931479  0.30684143 0.5312237  0.67141926]\n",
      " [0.22034582 0.5432589  0.36313823 0.67997706]\n",
      " [0.20965602 0.2574506  0.22615762 0.26186824]\n",
      " [0.25161755 0.42135075 0.45767513 0.6815412 ]\n",
      " [0.20965604 0.25745064 0.22615771 0.26186827]\n",
      " [0.20965572 0.25745052 0.22615665 0.26186797]\n",
      " [0.20965572 0.25745052 0.22615665 0.26186797]\n",
      " [0.20965578 0.25745055 0.22615685 0.26186803]\n",
      " [0.20965573 0.25745052 0.22615668 0.26186797]\n",
      " [0.20965572 0.25745052 0.22615667 0.26186797]\n",
      " [0.20965573 0.25745052 0.22615671 0.261868  ]\n",
      " [0.20965572 0.25745052 0.22615665 0.26186797]\n",
      " [0.20965572 0.25745052 0.22615665 0.26186797]\n",
      " [0.20965572 0.25745052 0.22615665 0.26186797]]\n",
      "[[0.35384616 0.         1.         0.37      ]\n",
      " [0.         0.03       0.9846154  0.71      ]\n",
      " [0.2769231  0.14       0.9846154  0.47      ]\n",
      " [0.52307695 0.64       0.7846154  1.        ]\n",
      " [0.6        0.94       0.75384617 1.        ]\n",
      " [0.24615385 0.65       0.47692308 0.97      ]\n",
      " [0.2769231  0.88       0.47692308 0.97      ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.01538462 0.48       0.2        0.76      ]\n",
      " [0.01538462 0.69       0.16923077 0.76      ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.48719543 0.42442474 0.54766244 0.48204   ]\n",
      " [0.45233485 0.432985   0.54850614 0.525738  ]\n",
      " [0.4860545  0.44817019 0.53897756 0.48227006]\n",
      " [0.50304794 0.52876586 0.5312984  0.57144815]\n",
      " [0.50501144 0.5617605  0.5234576  0.573472  ]\n",
      " [0.47746772 0.5214946  0.5021666  0.5719846 ]\n",
      " [0.47698462 0.5618262  0.49323714 0.5722655 ]\n",
      " [0.4523004  0.42558697 0.45501152 0.42694235]\n",
      " [0.45230034 0.42558694 0.45501134 0.42694223]\n",
      " [0.45228878 0.5032505  0.4669723  0.5405127 ]\n",
      " [0.45422494 0.53418654 0.46690518 0.54568577]\n",
      " [0.45230147 0.4255876  0.45501485 0.4269441 ]\n",
      " [0.4523011  0.4255874  0.45501375 0.42694354]\n",
      " [0.4523003  0.4255869  0.4550112  0.42694217]\n",
      " [0.4523003  0.4255869  0.4550112  0.42694217]\n",
      " [0.4523003  0.4255869  0.4550112  0.42694217]]\n",
      "[[0.5935252  0.         0.9676259  0.4117647 ]\n",
      " [0.23021583 0.24836601 1.         1.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.43137255 0.26258993 0.7058824 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]] [[0.5473236  0.37900352 0.7176088  0.46998012]\n",
      " [0.39937878 0.44662207 0.7211364  0.6217046 ]\n",
      " [0.2857917  0.38045534 0.29796568 0.38263255]\n",
      " [0.2857917  0.38045534 0.29796565 0.38263255]\n",
      " [0.2857917  0.38045534 0.29796565 0.38263255]\n",
      " [0.28579175 0.38045537 0.29796585 0.38263258]\n",
      " [0.2857917  0.38045534 0.29796565 0.38263255]\n",
      " [0.28579175 0.38045534 0.29796582 0.38263258]\n",
      " [0.2857917  0.38045534 0.29796565 0.38263255]\n",
      " [0.2815784  0.48754922 0.4052911  0.56800044]\n",
      " [0.2857917  0.38045534 0.29796565 0.38263255]\n",
      " [0.2857917  0.38045534 0.29796565 0.38263255]\n",
      " [0.2857917  0.38045534 0.29796565 0.38263255]\n",
      " [0.2857917  0.38045534 0.29796565 0.38263255]\n",
      " [0.2857917  0.38045534 0.29796565 0.38263255]\n",
      " [0.2857917  0.38045534 0.29796565 0.38263255]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ww/3nytfk1917b8nz7dchtckxnm0000gn/T/ipykernel_74047/4170770996.py:93: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  result['obj_class'].replace(class_dict, inplace=True)\n",
      "/var/folders/ww/3nytfk1917b8nz7dchtckxnm0000gn/T/ipykernel_74047/4170770996.py:119: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['head' 'torso' 'rwing' ... 'lleg' 'rleg' 'tail']' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  result.loc[result['obj_class']==k, ['part_labels']] = result.loc[result['obj_class']==k,['part_labels']].replace(v).copy()\n"
     ]
    }
   ],
   "source": [
    "#testing loop\n",
    "model_path = ('/Users/amrutamuthal/Documents/training_runs/model/'+run_prefix+'/Obj-Box-'\n",
    "                        +str(learning_rate)\n",
    "                        +'-batch-'+str(batch_size)\n",
    "                        +'-h1-'+str(hidden1)\n",
    "                        +'-h2-'+str(hidden2)\n",
    "                        +'-h3-'+str(hidden3)+'-test')\n",
    "summary_path = ('/Users/amrutamuthal/Documents/training_runs/run/'+run_prefix+'/Obj-Box-'\n",
    "                        +str(learning_rate)\n",
    "                        +'-batch-'+str(batch_size)\n",
    "                        +'-h1-'+str(hidden1)\n",
    "                        +'-h2-'+str(hidden2)\n",
    "                        +'-h3-'+str(hidden3)+'-test')\n",
    "\n",
    "\n",
    "write_tensorboard = False\n",
    "if write_tensorboard:\n",
    "    writer = SummaryWriter(summary_path)\n",
    "\n",
    "vae = TwoStageAutoEncoder(latent_dims,\n",
    "                          num_nodes,\n",
    "                          bbx_size,\n",
    "                          num_classes,\n",
    "                          label_shape,\n",
    "                          hidden1,\n",
    "                          hidden2,\n",
    "                          hidden3,\n",
    "                          dense_hidden1,\n",
    "                          dense_hidden2,\n",
    "                          adaptive_margin,\n",
    "                          output_log,\n",
    "                          area_encoding,\n",
    "                          coupling,\n",
    "                          obj_bbx_conditioning,\n",
    "                          use_fft_on_bbx,\n",
    "                          use_gcn_in_decoder,\n",
    "                        )\n",
    "\n",
    "vae.load_state_dict(torch.load(model_path+ '/model_weights.pth'))\n",
    "\n",
    "# decoder = vae.decoder\n",
    "image_shape = [num_nodes, bbx_size]\n",
    "global_step = 250000\n",
    "class_dict = {0:'cow', 1:'sheep', 2:'bird', 3:'person', 4:'cat', 5:'dog', 6:'horse'}\n",
    "res_dfs = []\n",
    "for i, val_data in enumerate(batch_val_loader, 0):\n",
    "    \n",
    "    val_data\n",
    "    node_data_true = val_data.x\n",
    "    label_true = node_data_true[:,:1]\n",
    "    y_true = val_data.y\n",
    "    class_true = y_true[:, :num_classes]\n",
    "    X_obj_true = y_true[:, num_classes:]\n",
    "    X_obj_true_transformed = X_obj_true[:,2:]-X_obj_true[:,:2]\n",
    "    node_data_transformed = torch.cat(\n",
    "            [node_data_true[:, :-2],\n",
    "             node_data_true[:, -2:] -  node_data_true[:, -4:-2]], axis=-1)\n",
    "    adj_true = val_data.edge_index\n",
    "    class_true  = torch.flatten(class_true)\n",
    "    \n",
    "    output = vae(adj_true, node_data_transformed, X_obj_true_transformed, label_true , class_true, variational, coupling, refine_iter=2)\n",
    "    node_data_pred_test = output[0]\n",
    "    node_data_pred_test_refined = output[8]\n",
    "    node_data_pred_test_refined_new = torch.clip(torch.cat(\n",
    "            [node_data_pred_test_refined[:, :, :-2],\n",
    "             node_data_pred_test_refined[:, :, -2:] + node_data_pred_test_refined[:, :, -4:-2]], axis=-1), min=0.0, max=1.0)\n",
    "    X_obj_pred_test = output[1]\n",
    "#     node_data_pred_test, X_obj_pred_test, label_pred, z_mean, z_logvar, margin = output\n",
    "    X_obj_pred_test = torch.cat(((torch.tensor([1.0])-X_obj_pred_test)*torch.tensor([0.5]),\n",
    "                                (torch.tensor([1.0])+X_obj_pred_test)*torch.tensor([0.5])),\n",
    "                              -1)\n",
    "    res_dfs.append(metrics.get_metrics(node_data_true, X_obj_true, node_data_pred_test_refined_new,\n",
    "                               X_obj_pred_test,\n",
    "                               label_true,\n",
    "                               class_true,\n",
    "                               num_nodes,\n",
    "                               num_classes))\n",
    "    \n",
    "    if write_tensorboard:\n",
    "        \n",
    "        for j in range(int(len(node_data_true)/num_nodes)):\n",
    "            \n",
    "            image = plot_bbx(node_data_true[j].detach().to(\"cpu\").numpy().astype(float))/255\n",
    "            pred_image = plot_bbx(node_data_pred_test[j].detach().to(\"cpu\").numpy()).astype(float)/255\n",
    "\n",
    "            writer.add_image('test_result/images/'+str(j)+'-input/', image, global_step, dataformats='HWC')  \n",
    "            writer.add_image('test_result/images/'+str(j)+'-generated/', pred_image, global_step, dataformats='HWC')\n",
    "            \n",
    "\n",
    "result = pd.concat(res_dfs)\n",
    "result['obj_class'] = np.where(result['obj_class'].isna(), 0, result['obj_class'])\n",
    "result['obj_class'] = result['obj_class'].astype('int')\n",
    "result['obj_class'].replace(class_dict, inplace=True)\n",
    "result.where(result['part_labels']!=0, np.NaN, inplace=True)\n",
    "result['part_labels'] = np.where(result['part_labels'].isna(), 0, result['part_labels'])\n",
    "result['part_labels'] = result['part_labels'].astype('int')\n",
    "result['id'] = np.repeat(np.array(list(range(int(len(result)/num_nodes)))), 16)\n",
    "\n",
    "if write_tensorboard:\n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "\n",
    "bird_labels = {'head':1 , 'torso':2, 'neck':3, 'lwing':4, 'rwing':5, 'lleg':6, 'lfoot':7, 'rleg':8, 'rfoot':9, 'tail':10}\n",
    "cat_labels = {'head':1, 'torso':2, 'neck':3, 'lfleg':4, 'lfpa':5, 'rfleg':6, 'rfpa':7, 'lbleg':8, 'lbpa':9, 'rbleg':10, 'rbpa':11, 'tail':12}\n",
    "cow_labels = {'head':1,'lhorn':2, 'rhorn':3, 'torso':4, 'neck':5, 'lfuleg':6, 'lflleg':7, 'rfuleg':8, 'rflleg':9, 'lbuleg':10, 'lblleg':11, 'rbuleg':12, 'rblleg':13, 'tail':14}\n",
    "dog_labels = {'head':1,'torso':2, 'neck':3, 'lfleg':4, 'lfpa':5, 'rfleg':6, 'rfpa':7, 'lbleg':8, 'lbpa':9, 'rbleg':10, 'rbpa':11, 'tail':12, 'muzzle':13}\n",
    "horse_labels = {'head':1,'lfho':2, 'rfho':3, 'torso':4, 'neck':5, 'lfuleg':6, 'lflleg':7, 'rfuleg':8, 'rflleg':9, 'lbuleg':10, 'lblleg':11, 'rbuleg':12, 'rblleg':13, 'tail':14, 'lbho':15, 'rbho':16}\n",
    "person_labels = {'head':1, 'torso':2, 'neck': 3, 'llarm': 4, 'luarm': 5, 'lhand': 6, 'rlarm':7, 'ruarm':8, 'rhand': 9, 'llleg': 10, 'luleg':11, 'lfoot':12, 'rlleg':13, 'ruleg':14, 'rfoot':15}\n",
    "sheep_labels = cow_labels\n",
    "part_labels_combined_parts = {'bird': bird_labels, 'cat': cat_labels, 'cow': cow_labels, 'dog': dog_labels, 'sheep': sheep_labels, 'horse':horse_labels,'person':person_labels}\n",
    "\n",
    "for k, v in part_labels_combined_parts.items():\n",
    "    new_map = {}\n",
    "    for pk, pv in v.items():\n",
    "        new_map[pv]=pk\n",
    "    part_labels_combined_parts[k] = new_map\n",
    "    \n",
    "for k, v in part_labels_combined_parts.items():\n",
    "    result.loc[result['obj_class']==k, ['part_labels']] = result.loc[result['obj_class']==k,['part_labels']].replace(v).copy()\n",
    "\n",
    "# rel_metrics.csv')sult.to_csv(model_path+'/raw_metrics.csv')\n",
    "# res_obj_level = result.groupby(['obj_class', 'id']).mean().reset_index()\n",
    "# res_obj_level.groupby(['obj_class']).mean().reset_index()[['obj_class', 'IOU', 'MSE']].to_csv(model_path+'/obj_level_metrics.csv')\n",
    "# result.groupby(['obj_class', 'part_labels']).mean().reset_index()[['obj_class', 'part_labels',  'IOU', 'MSE']].to_csv(model_path+'/part_leve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9eeb5151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6070, 0.0029, 0.3850, 0.3738],\n",
       "        [0.2725, 0.2807, 0.8409, 0.7367],\n",
       "        [0.0157, 0.0089, 0.0275, 0.0089],\n",
       "        [0.0157, 0.0089, 0.0275, 0.0089],\n",
       "        [0.0157, 0.0089, 0.0275, 0.0089],\n",
       "        [0.0157, 0.0089, 0.0275, 0.0089],\n",
       "        [0.0157, 0.0089, 0.0275, 0.0089],\n",
       "        [0.0157, 0.0089, 0.0275, 0.0089],\n",
       "        [0.0157, 0.0089, 0.0275, 0.0089],\n",
       "        [0.0061, 0.4488, 0.2797, 0.3305],\n",
       "        [0.0157, 0.0089, 0.0275, 0.0089],\n",
       "        [0.0157, 0.0089, 0.0275, 0.0089],\n",
       "        [0.0157, 0.0089, 0.0275, 0.0089],\n",
       "        [0.0157, 0.0089, 0.0275, 0.0089],\n",
       "        [0.0157, 0.0089, 0.0275, 0.0089],\n",
       "        [0.0157, 0.0089, 0.0275, 0.0089]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_data_pred_test_refined[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "be575bf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000,  0.5406,  0.3783,  0.7061,  0.4785],\n",
       "        [ 2.0000,  0.3799,  0.4387,  0.7204,  0.6217],\n",
       "        [ 0.0000,  0.2781,  0.3783,  0.2781,  0.3783],\n",
       "        [ 0.0000,  0.2781,  0.3783,  0.2781,  0.3783],\n",
       "        [ 0.0000,  0.2781,  0.3783,  0.2781,  0.3783],\n",
       "        [ 0.0000,  0.2781,  0.3783,  0.2781,  0.3783],\n",
       "        [ 0.0000,  0.2781,  0.3783,  0.2781,  0.3783],\n",
       "        [ 0.0000,  0.2781,  0.3783,  0.2781,  0.3783],\n",
       "        [ 0.0000,  0.2781,  0.3783,  0.2781,  0.3783],\n",
       "        [10.0000,  0.2781,  0.4833,  0.3942,  0.5501],\n",
       "        [ 0.0000,  0.2781,  0.3783,  0.2781,  0.3783],\n",
       "        [ 0.0000,  0.2781,  0.3783,  0.2781,  0.3783],\n",
       "        [ 0.0000,  0.2781,  0.3783,  0.2781,  0.3783],\n",
       "        [ 0.0000,  0.2781,  0.3783,  0.2781,  0.3783],\n",
       "        [ 0.0000,  0.2781,  0.3783,  0.2781,  0.3783],\n",
       "        [ 0.0000,  0.2781,  0.3783,  0.2781,  0.3783]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_data_true[:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "78220c71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>obj_class</th>\n",
       "      <th>part_labels</th>\n",
       "      <th>IOU</th>\n",
       "      <th>MSE</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bird</td>\n",
       "      <td>head</td>\n",
       "      <td>0.637261</td>\n",
       "      <td>0.000272</td>\n",
       "      <td>3402.833737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bird</td>\n",
       "      <td>lfoot</td>\n",
       "      <td>0.229842</td>\n",
       "      <td>0.000729</td>\n",
       "      <td>3360.433414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bird</td>\n",
       "      <td>lleg</td>\n",
       "      <td>0.434429</td>\n",
       "      <td>0.000501</td>\n",
       "      <td>3281.058347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bird</td>\n",
       "      <td>lwing</td>\n",
       "      <td>0.492272</td>\n",
       "      <td>0.001229</td>\n",
       "      <td>3371.386892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bird</td>\n",
       "      <td>neck</td>\n",
       "      <td>0.498182</td>\n",
       "      <td>0.000383</td>\n",
       "      <td>3328.015717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>sheep</td>\n",
       "      <td>lhorn</td>\n",
       "      <td>0.357830</td>\n",
       "      <td>0.000578</td>\n",
       "      <td>3472.261146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>sheep</td>\n",
       "      <td>neck</td>\n",
       "      <td>0.675668</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>3339.884211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>sheep</td>\n",
       "      <td>rhorn</td>\n",
       "      <td>0.296405</td>\n",
       "      <td>0.000765</td>\n",
       "      <td>3403.758824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>sheep</td>\n",
       "      <td>tail</td>\n",
       "      <td>0.290373</td>\n",
       "      <td>0.000521</td>\n",
       "      <td>3272.528517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>sheep</td>\n",
       "      <td>torso</td>\n",
       "      <td>0.888579</td>\n",
       "      <td>0.000178</td>\n",
       "      <td>3359.622556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>86 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   obj_class part_labels       IOU       MSE           id\n",
       "0       bird        head  0.637261  0.000272  3402.833737\n",
       "1       bird       lfoot  0.229842  0.000729  3360.433414\n",
       "2       bird        lleg  0.434429  0.000501  3281.058347\n",
       "3       bird       lwing  0.492272  0.001229  3371.386892\n",
       "4       bird        neck  0.498182  0.000383  3328.015717\n",
       "..       ...         ...       ...       ...          ...\n",
       "81     sheep       lhorn  0.357830  0.000578  3472.261146\n",
       "82     sheep        neck  0.675668  0.000173  3339.884211\n",
       "83     sheep       rhorn  0.296405  0.000765  3403.758824\n",
       "84     sheep        tail  0.290373  0.000521  3272.528517\n",
       "85     sheep       torso  0.888579  0.000178  3359.622556\n",
       "\n",
       "[86 rows x 5 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[['obj_class', 'part_labels', 'IOU', 'MSE', 'id']].groupby(['obj_class', 'part_labels']).mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "02e6f6eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>obj_class</th>\n",
       "      <th>IOU</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bird</td>\n",
       "      <td>0.574106</td>\n",
       "      <td>0.000421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cat</td>\n",
       "      <td>0.626683</td>\n",
       "      <td>0.001157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cow</td>\n",
       "      <td>0.567391</td>\n",
       "      <td>0.000523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dog</td>\n",
       "      <td>0.509603</td>\n",
       "      <td>0.000940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>horse</td>\n",
       "      <td>0.439889</td>\n",
       "      <td>0.001150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>person</td>\n",
       "      <td>0.521325</td>\n",
       "      <td>0.002173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sheep</td>\n",
       "      <td>0.704240</td>\n",
       "      <td>0.000241</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  obj_class       IOU       MSE\n",
       "0      bird  0.574106  0.000421\n",
       "1       cat  0.626683  0.001157\n",
       "2       cow  0.567391  0.000523\n",
       "3       dog  0.509603  0.000940\n",
       "4     horse  0.439889  0.001150\n",
       "5    person  0.521325  0.002173\n",
       "6     sheep  0.704240  0.000241"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_obj_level = result[['obj_class', 'IOU', 'MSE', 'id']].groupby(['obj_class', 'id']).mean().reset_index()\n",
    "res_obj_level.groupby(['obj_class']).mean().reset_index()[['obj_class', 'IOU', 'MSE']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d9367e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(X_obj, part_decoder, obj_class, nodes, batch_size, latent_dims):\n",
    "    \n",
    "    nodes = torch.reshape(nodes,(batch_size,part_decoder.num_nodes))\n",
    "    obj_class = torch.reshape(obj_class, \n",
    "                              (batch_size, num_classes))\n",
    "    \n",
    "    # obj sampling\n",
    "    z_latent_obj = X_obj\n",
    "    \n",
    "    print(z_latent_obj.shape, obj_class.shape)\n",
    "    conditioned_obj_latent = torch.cat([obj_class, z_latent_obj],dim=-1)\n",
    "    conditioned_obj_latent = torch.cat([nodes, conditioned_obj_latent],dim=-1)\n",
    "\n",
    "    # part sampling\n",
    "    z_latent_part = torch.normal(torch.zeros([batch_size,latent_dims]))\n",
    "    conditioned_part_latent = torch.cat([conditioned_obj_latent, z_latent_part],dim=-1)\n",
    "    \n",
    "    x_bbx, _, _, _ = part_decoder(conditioned_part_latent)\n",
    "    \n",
    "    return x_bbx, X_obj\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c39d671",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#testing loop\n",
    "model_path = ('D:/meronym_data/model/'+run_prefix+'/Obj-Box-'\n",
    "                        +str(learning_rate)\n",
    "                        +'-batch-'+str(batch_size)\n",
    "                        +'-h1-'+str(hidden1)\n",
    "                        +'-h2-'+str(hidden2)\n",
    "                        +'-h3-'+str(hidden3)+'-test')\n",
    "summary_path = ('D:/meronym_data/runs/'+run_prefix+'/Obj-Box-'\n",
    "                        +str(learning_rate)\n",
    "                        +'-batch-'+str(batch_size)\n",
    "                        +'-h1-'+str(hidden1)\n",
    "                        +'-h2-'+str(hidden2)\n",
    "                        +'-h3-'+str(hidden3)+'-test')\n",
    "\n",
    "\n",
    "write_tensorboard = True\n",
    "if write_tensorboard:\n",
    "    writer = SummaryWriter(summary_path)\n",
    "\n",
    "vae = TwoStageAutoEncoder(latent_dims,\n",
    "                          num_nodes,\n",
    "                          bbx_size,\n",
    "                          num_classes,\n",
    "                          label_shape,\n",
    "                          hidden1,\n",
    "                          hidden2,\n",
    "                          hidden3,\n",
    "                          dense_hidden1,\n",
    "                          dense_hidden2,\n",
    "                          adaptive_margin,\n",
    "                          output_log,\n",
    "                          area_encoding,\n",
    "                          coupling,\n",
    "                          obj_bbx_conditioning,\n",
    "                        )\n",
    "\n",
    "\n",
    "vae.load_state_dict(torch.load(model_path+ '/model_weights.pth'))\n",
    "\n",
    "part_decoder = vae.gcn_decoder\n",
    "image_shape = [num_nodes, bbx_size]\n",
    "global_step = 250000\n",
    "class_dict = {0:'cow', 1:'sheep', 2:'bird', 3:'person', 4:'cat', 5:'dog', 6:'horse'}\n",
    "res_dfs = []\n",
    "for i, val_data in enumerate(batch_val_loader, 0):\n",
    "    \n",
    "    val_data\n",
    "    node_data_true = val_data.x\n",
    "    label_true = node_data_true[:,:1]\n",
    "    y_true = val_data.y\n",
    "    class_true = y_true[:, :num_classes]\n",
    "    X_obj_true = y_true[:, num_classes:]\n",
    "    adj_true = val_data.edge_index\n",
    "    class_true  = torch.flatten(class_true)\n",
    "    \n",
    "    output = inference(X_obj_true, part_decoder, class_true, label_true, batch_size, latent_dims)\n",
    "    node_data_pred_test = output[0]\n",
    "    X_obj_pred_test = output[1]\n",
    "#     node_data_pred_test, X_obj_pred_test, label_pred, z_mean, z_logvar, margin = output\n",
    "    \n",
    "#     res_dfs.append(metrics.get_metrics(node_data_true, X_obj_true, node_data_pred_test,\n",
    "#                                X_obj_pred_test,\n",
    "#                                label_true,\n",
    "#                                class_true,\n",
    "#                                num_nodes,\n",
    "#                                num_classes))\n",
    "    \n",
    "    if write_tensorboard:\n",
    "        \n",
    "        for j in range(int(len(node_data_true)/num_nodes)):\n",
    "            image = plot_utils.plot_bbx((node_data_true[num_nodes*j:num_nodes*(j+1)\n",
    "                                                       ,1:5]*label_true[num_nodes*j:num_nodes*(j+1)]).detach().to(\"cpu\").numpy().astype(float))/255\n",
    "            pred_image = plot_utils.plot_bbx((node_data_pred_test[j]*label_true[num_nodes*j:num_nodes*(j+1)]).detach().to(\"cpu\").numpy()).astype(float)/255\n",
    "\n",
    "            writer.add_image('test_result/images/'+str(j)+'-input/', image, global_step, dataformats='HWC')  \n",
    "            writer.add_image('test_result/images/'+str(j)+'-generated/', pred_image, global_step, dataformats='HWC')\n",
    "            \n",
    "\n",
    "result = pd.concat(res_dfs)\n",
    "result['obj_class'] = np.where(result['obj_class'].isna(), 0, result['obj_class'])\n",
    "result['obj_class'] = result['obj_class'].astype('int')\n",
    "result['obj_class'].replace(class_dict, inplace=True)\n",
    "result.where(result['part_labels']!=0, np.NaN, inplace=True)\n",
    "result['part_labels'] = np.where(result['part_labels'].isna(), 0, result['part_labels'])\n",
    "result['part_labels'] = result['part_labels'].astype('int')\n",
    "result['id'] = np.repeat(np.array(list(range(int(len(result)/num_nodes)))), 16)\n",
    "\n",
    "if write_tensorboard:\n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "\n",
    "bird_labels = {'head':1 , 'torso':2, 'neck':3, 'lwing':4, 'rwing':5, 'lleg':6, 'lfoot':7, 'rleg':8, 'rfoot':9, 'tail':10}\n",
    "cat_labels = {'head':1, 'torso':2, 'neck':3, 'lfleg':4, 'lfpa':5, 'rfleg':6, 'rfpa':7, 'lbleg':8, 'lbpa':9, 'rbleg':10, 'rbpa':11, 'tail':12}\n",
    "cow_labels = {'head':1,'lhorn':2, 'rhorn':3, 'torso':4, 'neck':5, 'lfuleg':6, 'lflleg':7, 'rfuleg':8, 'rflleg':9, 'lbuleg':10, 'lblleg':11, 'rbuleg':12, 'rblleg':13, 'tail':14}\n",
    "dog_labels = {'head':1,'torso':2, 'neck':3, 'lfleg':4, 'lfpa':5, 'rfleg':6, 'rfpa':7, 'lbleg':8, 'lbpa':9, 'rbleg':10, 'rbpa':11, 'tail':12, 'muzzle':13}\n",
    "horse_labels = {'head':1,'lfho':2, 'rfho':3, 'torso':4, 'neck':5, 'lfuleg':6, 'lflleg':7, 'rfuleg':8, 'rflleg':9, 'lbuleg':10, 'lblleg':11, 'rbuleg':12, 'rblleg':13, 'tail':14, 'lbho':15, 'rbho':16}\n",
    "person_labels = {'head':1, 'torso':2, 'neck': 3, 'llarm': 4, 'luarm': 5, 'lhand': 6, 'rlarm':7, 'ruarm':8, 'rhand': 9, 'llleg': 10, 'luleg':11, 'lfoot':12, 'rlleg':13, 'ruleg':14, 'rfoot':15}\n",
    "sheep_labels = cow_labels\n",
    "part_labels_combined_parts = {'bird': bird_labels, 'cat': cat_labels, 'cow': cow_labels, 'dog': dog_labels, 'sheep': sheep_labels, 'horse':horse_labels,'person':person_labels}\n",
    "\n",
    "for k, v in part_labels_combined_parts.items():\n",
    "    new_map = {}\n",
    "    for pk, pv in v.items():\n",
    "        new_map[pv]=pk\n",
    "    part_labels_combined_parts[k] = new_map\n",
    "    \n",
    "for k, v in part_labels_combined_parts.items():\n",
    "    result.loc[result['obj_class']==k, ['part_labels']] = result.loc[result['obj_class']==k,['part_labels']].replace(v).copy()\n",
    "\n",
    "result.to_csv(model_path+'/raw_metrics.csv')\n",
    "res_obj_level = result.groupby(['obj_class', 'id']).mean().reset_index()\n",
    "res_obj_level.groupby(['obj_class']).mean().reset_index()[['obj_class', 'IOU', 'MSE']].to_csv(model_path+'/obj_level_metrics.csv')\n",
    "result.groupby(['obj_class', 'part_labels']).mean().reset_index()[['obj_class', 'part_labels',  'IOU', 'MSE']].to_csv(model_path+'/part_level_metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a379c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = ('D:/meronym_data/model/'+run_prefix+'/Obj-Box-'\n",
    "                        +str(learning_rate)\n",
    "                        +'-batch-'+str(batch_size)\n",
    "                        +'-h1-'+str(hidden1)\n",
    "                        +'-h2-'+str(hidden2)\n",
    "                        +'-h3-'+str(hidden3)+'-test')\n",
    "summary_path = ('D:/meronym_data/runs/'+run_prefix+'/Obj-Box-'\n",
    "                        +str(learning_rate)\n",
    "                        +'-batch-'+str(batch_size)\n",
    "                        +'-h1-'+str(hidden1)\n",
    "                        +'-h2-'+str(hidden2)\n",
    "                        +'-h3-'+str(hidden3)+'-test')\n",
    "\n",
    "result = pd.read_csv(model_path+'/raw_metrics.csv')\n",
    "result.groupby(['obj_class', 'part_labels']).mean().reset_index()[['obj_class', 'part_labels',\n",
    "                                                                   'IOU', 'MSE']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756ef248",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d2d2cbb0",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "agg function failed [how->mean,dtype->object]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/groupby/groupby.py:1942\u001b[0m, in \u001b[0;36mGroupBy._agg_py_fallback\u001b[0;34m(self, how, values, ndim, alt)\u001b[0m\n\u001b[1;32m   1941\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1942\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_grouper\u001b[38;5;241m.\u001b[39magg_series(ser, alt, preserve_dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1943\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/groupby/ops.py:864\u001b[0m, in \u001b[0;36mBaseGrouper.agg_series\u001b[0;34m(self, obj, func, preserve_dtype)\u001b[0m\n\u001b[1;32m    862\u001b[0m     preserve_dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 864\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aggregate_series_pure_python(obj, func)\n\u001b[1;32m    866\u001b[0m npvalues \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mmaybe_convert_objects(result, try_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/groupby/ops.py:885\u001b[0m, in \u001b[0;36mBaseGrouper._aggregate_series_pure_python\u001b[0;34m(self, obj, func)\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(splitter):\n\u001b[0;32m--> 885\u001b[0m     res \u001b[38;5;241m=\u001b[39m func(group)\n\u001b[1;32m    886\u001b[0m     res \u001b[38;5;241m=\u001b[39m extract_result(res)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/groupby/groupby.py:2454\u001b[0m, in \u001b[0;36mGroupBy.mean.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   2451\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2452\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cython_agg_general(\n\u001b[1;32m   2453\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m-> 2454\u001b[0m         alt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: Series(x, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mmean(numeric_only\u001b[38;5;241m=\u001b[39mnumeric_only),\n\u001b[1;32m   2455\u001b[0m         numeric_only\u001b[38;5;241m=\u001b[39mnumeric_only,\n\u001b[1;32m   2456\u001b[0m     )\n\u001b[1;32m   2457\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroupby\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/series.py:6549\u001b[0m, in \u001b[0;36mSeries.mean\u001b[0;34m(self, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m   6541\u001b[0m \u001b[38;5;129m@doc\u001b[39m(make_doc(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m, ndim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m   6542\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmean\u001b[39m(\n\u001b[1;32m   6543\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   6547\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   6548\u001b[0m ):\n\u001b[0;32m-> 6549\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m NDFrame\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;28mself\u001b[39m, axis, skipna, numeric_only, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/generic.py:12420\u001b[0m, in \u001b[0;36mNDFrame.mean\u001b[0;34m(self, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m  12413\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmean\u001b[39m(\n\u001b[1;32m  12414\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m  12415\u001b[0m     axis: Axis \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  12418\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m  12419\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[0;32m> 12420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stat_function(\n\u001b[1;32m  12421\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m, nanops\u001b[38;5;241m.\u001b[39mnanmean, axis, skipna, numeric_only, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m  12422\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/generic.py:12377\u001b[0m, in \u001b[0;36mNDFrame._stat_function\u001b[0;34m(self, name, func, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m  12375\u001b[0m validate_bool_kwarg(skipna, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskipna\u001b[39m\u001b[38;5;124m\"\u001b[39m, none_allowed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m> 12377\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reduce(\n\u001b[1;32m  12378\u001b[0m     func, name\u001b[38;5;241m=\u001b[39mname, axis\u001b[38;5;241m=\u001b[39maxis, skipna\u001b[38;5;241m=\u001b[39mskipna, numeric_only\u001b[38;5;241m=\u001b[39mnumeric_only\n\u001b[1;32m  12379\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/series.py:6457\u001b[0m, in \u001b[0;36mSeries._reduce\u001b[0;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[0m\n\u001b[1;32m   6453\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   6454\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeries.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not allow \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwd_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnumeric_only\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   6455\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith non-numeric dtypes.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   6456\u001b[0m     )\n\u001b[0;32m-> 6457\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op(delegate, skipna\u001b[38;5;241m=\u001b[39mskipna, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/nanops.py:147\u001b[0m, in \u001b[0;36mbottleneck_switch.__call__.<locals>.f\u001b[0;34m(values, axis, skipna, **kwds)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 147\u001b[0m     result \u001b[38;5;241m=\u001b[39m alt(values, axis\u001b[38;5;241m=\u001b[39maxis, skipna\u001b[38;5;241m=\u001b[39mskipna, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/nanops.py:404\u001b[0m, in \u001b[0;36m_datetimelike_compat.<locals>.new_func\u001b[0;34m(values, axis, skipna, mask, **kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m     mask \u001b[38;5;241m=\u001b[39m isna(values)\n\u001b[0;32m--> 404\u001b[0m result \u001b[38;5;241m=\u001b[39m func(values, axis\u001b[38;5;241m=\u001b[39maxis, skipna\u001b[38;5;241m=\u001b[39mskipna, mask\u001b[38;5;241m=\u001b[39mmask, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m datetimelike:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/nanops.py:720\u001b[0m, in \u001b[0;36mnanmean\u001b[0;34m(values, axis, skipna, mask)\u001b[0m\n\u001b[1;32m    719\u001b[0m the_sum \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39msum(axis, dtype\u001b[38;5;241m=\u001b[39mdtype_sum)\n\u001b[0;32m--> 720\u001b[0m the_sum \u001b[38;5;241m=\u001b[39m _ensure_numeric(the_sum)\n\u001b[1;32m    722\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(the_sum, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/nanops.py:1701\u001b[0m, in \u001b[0;36m_ensure_numeric\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1699\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   1700\u001b[0m     \u001b[38;5;66;03m# GH#44008, GH#36703 avoid casting e.g. strings to numeric\u001b[39;00m\n\u001b[0;32m-> 1701\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not convert string \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to numeric\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1702\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: Could not convert string 'headtorsorwinglleglfootrlegrfoot' to numeric",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[77], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m res_obj_level \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mgroupby([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobj_class\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[1;32m      2\u001b[0m res_obj_level\u001b[38;5;241m.\u001b[39mgroupby([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobj_class\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mreset_index()[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobj_class\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIOU\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMSE\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/groupby/groupby.py:2452\u001b[0m, in \u001b[0;36mGroupBy.mean\u001b[0;34m(self, numeric_only, engine, engine_kwargs)\u001b[0m\n\u001b[1;32m   2445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_numba_agg_general(\n\u001b[1;32m   2446\u001b[0m         grouped_mean,\n\u001b[1;32m   2447\u001b[0m         executor\u001b[38;5;241m.\u001b[39mfloat_dtype_mapping,\n\u001b[1;32m   2448\u001b[0m         engine_kwargs,\n\u001b[1;32m   2449\u001b[0m         min_periods\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   2450\u001b[0m     )\n\u001b[1;32m   2451\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2452\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cython_agg_general(\n\u001b[1;32m   2453\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2454\u001b[0m         alt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: Series(x, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mmean(numeric_only\u001b[38;5;241m=\u001b[39mnumeric_only),\n\u001b[1;32m   2455\u001b[0m         numeric_only\u001b[38;5;241m=\u001b[39mnumeric_only,\n\u001b[1;32m   2456\u001b[0m     )\n\u001b[1;32m   2457\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroupby\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/groupby/groupby.py:1998\u001b[0m, in \u001b[0;36mGroupBy._cython_agg_general\u001b[0;34m(self, how, alt, numeric_only, min_count, **kwargs)\u001b[0m\n\u001b[1;32m   1995\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agg_py_fallback(how, values, ndim\u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39mndim, alt\u001b[38;5;241m=\u001b[39malt)\n\u001b[1;32m   1996\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m-> 1998\u001b[0m new_mgr \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mgrouped_reduce(array_func)\n\u001b[1;32m   1999\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_agged_manager(new_mgr)\n\u001b[1;32m   2000\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m how \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midxmin\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midxmax\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/internals/managers.py:1469\u001b[0m, in \u001b[0;36mBlockManager.grouped_reduce\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m   1465\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m blk\u001b[38;5;241m.\u001b[39mis_object:\n\u001b[1;32m   1466\u001b[0m     \u001b[38;5;66;03m# split on object-dtype blocks bc some columns may raise\u001b[39;00m\n\u001b[1;32m   1467\u001b[0m     \u001b[38;5;66;03m#  while others do not.\u001b[39;00m\n\u001b[1;32m   1468\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sb \u001b[38;5;129;01min\u001b[39;00m blk\u001b[38;5;241m.\u001b[39m_split():\n\u001b[0;32m-> 1469\u001b[0m         applied \u001b[38;5;241m=\u001b[39m sb\u001b[38;5;241m.\u001b[39mapply(func)\n\u001b[1;32m   1470\u001b[0m         result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[1;32m   1471\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/internals/blocks.py:393\u001b[0m, in \u001b[0;36mBlock.apply\u001b[0;34m(self, func, **kwargs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Block]:\n\u001b[1;32m    389\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;124;03m    apply the function to my values; return a block if we are not\u001b[39;00m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;124;03m    one\u001b[39;00m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 393\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    395\u001b[0m     result \u001b[38;5;241m=\u001b[39m maybe_coerce_values(result)\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split_op_result(result)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/groupby/groupby.py:1995\u001b[0m, in \u001b[0;36mGroupBy._cython_agg_general.<locals>.array_func\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m   1992\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m   1994\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m alt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1995\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agg_py_fallback(how, values, ndim\u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39mndim, alt\u001b[38;5;241m=\u001b[39malt)\n\u001b[1;32m   1996\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/core/groupby/groupby.py:1946\u001b[0m, in \u001b[0;36mGroupBy._agg_py_fallback\u001b[0;34m(self, how, values, ndim, alt)\u001b[0m\n\u001b[1;32m   1944\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magg function failed [how->\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhow\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,dtype->\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mser\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1945\u001b[0m     \u001b[38;5;66;03m# preserve the kind of exception that raised\u001b[39;00m\n\u001b[0;32m-> 1946\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(err)(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   1948\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ser\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[1;32m   1949\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m res_values\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: agg function failed [how->mean,dtype->object]"
     ]
    }
   ],
   "source": [
    "res_obj_level = result.groupby(['obj_class', 'id']).mean().reset_index()\n",
    "res_obj_level.groupby(['obj_class']).mean().reset_index()[['obj_class', 'IOU', 'MSE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2a0a03e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IOU    0.558476\n",
       "MSE    0.001006\n",
       "dtype: float64"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_obj_level[['IOU', 'MSE']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e231a317",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_obj_pred[:10,2:]-X_obj_pred[:10,:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90832eb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_obj_true[:10, 2:]-X_obj_true[:10, :2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b789243a",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_obj_level.to_csv(model_path+'/obj_metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838534e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7e5b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = 'D:/meronym_data/generate_boxes.npy'\n",
    "with open(outfile, 'wb') as pickle_file:\n",
    "    pred_boxes = np.concatenate(pred_boxes)\n",
    "    pickle.dump(pred_boxes, pickle_file)\n",
    "outfile = 'D:/meronym_data/generate_boxesobj_class.npy'\n",
    "with open(outfile, 'wb') as pickle_file:\n",
    "    pickle.dump(classes,pickle_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
