{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a3c56c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5138e161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_LAUNCH_BLOCKING=1\n"
     ]
    }
   ],
   "source": [
    "%set_env CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7acac192",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import collections\n",
    "import cv2\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "import copy\n",
    "import numpy as np\n",
    "import pickle\n",
    "sys.path.append(\"/home/tanvikamble/MeronymNet-PyTorch/src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f552fc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "from torch_geometric.data import DataLoader\n",
    "import torch.utils.data as data_utils\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eaa6bf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from losses import BoxVAE_losses as loss\n",
    "from components.AutoEncoder import GCNAutoEncoder\n",
    "from components.Decoder import Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40fd3654",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "699d8022",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [(1, 0, 0),\n",
    "          (0.737, 0.561, 0.561),\n",
    "          (0.255, 0.412, 0.882),\n",
    "          (0.545, 0.271, 0.0745),\n",
    "          (0.98, 0.502, 0.447),\n",
    "          (0.98, 0.643, 0.376),\n",
    "          (0.18, 0.545, 0.341),\n",
    "          (0.502, 0, 0.502),\n",
    "          (0.627, 0.322, 0.176),\n",
    "          (0.753, 0.753, 0.753),\n",
    "          (0.529, 0.808, 0.922),\n",
    "          (0.416, 0.353, 0.804),\n",
    "          (0.439, 0.502, 0.565),\n",
    "          (0.784, 0.302, 0.565),\n",
    "          (0.867, 0.627, 0.867),\n",
    "          (0, 1, 0.498),\n",
    "          (0.275, 0.51, 0.706),\n",
    "          (0.824, 0.706, 0.549),\n",
    "          (0, 0.502, 0.502),\n",
    "          (0.847, 0.749, 0.847),\n",
    "          (1, 0.388, 0.278),\n",
    "          (0.251, 0.878, 0.816),\n",
    "          (0.933, 0.51, 0.933),\n",
    "          (0.961, 0.871, 0.702)]\n",
    "colors = (np.asarray(colors)*255)\n",
    "canvas_size = 660\n",
    "\n",
    "def plot_bbx(bbx):\n",
    "    bbx = bbx*canvas_size\n",
    "    canvas = np.ones((canvas_size,canvas_size,3), np.uint8) * 255\n",
    "    for i, coord in enumerate(bbx):\n",
    "        x_minp, y_minp,x_maxp , y_maxp= coord\n",
    "        if [x_minp, y_minp,x_maxp , y_maxp]!=[0,0,0,0]:\n",
    "            cv2.rectangle(canvas, (int(x_minp), int(y_minp)), (int(x_maxp) , int(y_maxp) ), colors[i], 6)\n",
    "    return canvas\n",
    "\n",
    "# def plot_bbx(bbx):\n",
    "#     bbx = bbx*canvas_size\n",
    "#     canvas = np.ones((canvas_size,canvas_size,3), np.uint8) * 255\n",
    "#     for i, coord in enumerate(bbx):\n",
    "#         x, y, w ,h = coord\n",
    "#         if [x, y, w ,h]!=[0,0,0,0]:\n",
    "#             cv2.rectangle(canvas, (int(x), int(y)), (int(x + w) , int(y + h) ), colors[i], 6)\n",
    "#     return canvas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13f0f177",
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = '/home/tanvikamble/MeronymNet-PyTorch/src/processed_data/X_train.np'\n",
    "with open(outfile, 'rb') as pickle_file:\n",
    "    X_train = pickle.load(pickle_file)\n",
    "\n",
    "outfile = '/home/tanvikamble/MeronymNet-PyTorch/src/processed_data/class_v.np'\n",
    "with open(outfile, 'rb') as pickle_file:\n",
    "    class_v = pickle.load(pickle_file)\n",
    "\n",
    "outfile = '/home/tanvikamble/MeronymNet-PyTorch/src/processed_data/adj_train.np'\n",
    "with open(outfile, 'rb') as pickle_file:\n",
    "    adj_train = pickle.load(pickle_file)\n",
    "\n",
    "outfile = '/home/tanvikamble/MeronymNet-PyTorch/src/processed_data/X_train_val.np'\n",
    "with open(outfile, 'rb') as pickle_file:\n",
    "    X_train_val = pickle.load(pickle_file)\n",
    "\n",
    "outfile = '/home/tanvikamble/MeronymNet-PyTorch/src/processed_data/class_v_val.np'\n",
    "with open(outfile, 'rb') as pickle_file:\n",
    "    class_v_val = pickle.load(pickle_file)\n",
    "    \n",
    "outfile = '/home/tanvikamble/MeronymNet-PyTorch/src/processed_data/adj_train_val.np'\n",
    "with open(outfile, 'rb') as pickle_file:\n",
    "    adj_train_val = pickle.load(pickle_file)\n",
    "    \n",
    "outfile = '/home/tanvikamble/MeronymNet-PyTorch/src/processed_data/X_test.np'\n",
    "with open(outfile, 'rb') as pickle_file:\n",
    "    X_test = pickle.load(pickle_file)\n",
    "\n",
    "outfile = '/home/tanvikamble/MeronymNet-PyTorch/src/processed_data/adj_test.np'\n",
    "with open(outfile, 'rb') as pickle_file:\n",
    "    adj_test = pickle.load(pickle_file)\n",
    "    \n",
    "outfile = '/home/tanvikamble/MeronymNet-PyTorch/src/processed_data/class_v_test.np'\n",
    "with open(outfile, 'rb') as pickle_file:\n",
    "    class_v_test = pickle.load(pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9614b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[X_train<=0] = 0\n",
    "X_train_val[X_train_val<=0] = 0\n",
    "X_test[X_test<=0] = 0\n",
    "\n",
    "X_train[X_train>=1] = 1\n",
    "X_train_val[X_train_val>=1] = 1\n",
    "X_test[X_test>=1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d594c578",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(100)\n",
    "train_idx = np.random.randint(1,len(X_train),len(X_train))\n",
    "val_idx = np.random.randint(1,len(X_train_val),len(X_train_val))\n",
    "test_idx = np.random.randint(1,len(X_test),len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8c50aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tanvikamble/miniconda3/lib/python3.9/site-packages/torch_geometric/deprecation.py:13: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "seed = 345\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "validation = True\n",
    "if validation:\n",
    "    train_list =[]\n",
    "    for idx, batch in enumerate(zip(copy.deepcopy(X_train[train_idx]),\n",
    "                                    copy.deepcopy(class_v[train_idx]),\n",
    "                                    copy.deepcopy(adj_train[train_idx]))):\n",
    "        edge_index, _ = dense_to_sparse(torch.from_numpy(batch[2]).cuda().float())\n",
    "        train_list.append(Data(x = torch.from_numpy(batch[0]).cuda().float(),\n",
    "                               y = torch.from_numpy(batch[1]).cuda().float(),\n",
    "                               edge_index = edge_index\n",
    "                                    )\n",
    "                         )\n",
    "\n",
    "    batch_train_loader = DataLoader(train_list, batch_size=batch_size)\n",
    "\n",
    "    val_list = []\n",
    "    for idx, batch in enumerate(zip(copy.deepcopy(X_train_val[val_idx]),\n",
    "                                    copy.deepcopy(class_v_val[val_idx]), \n",
    "                                    copy.deepcopy(adj_train_val[val_idx]))):\n",
    "        edge_index, _ = dense_to_sparse(torch.from_numpy(batch[2]).cuda().float())\n",
    "        val_list.append(Data(x = torch.from_numpy(batch[0]).cuda().float(),\n",
    "                             y = torch.from_numpy(batch[1]).cuda().float(),\n",
    "                             edge_index = edge_index\n",
    "                                    )\n",
    "                         )\n",
    "    batch_val_loader = DataLoader(val_list, batch_size=batch_size)\n",
    "else:\n",
    "    train_list =[]\n",
    "    for idx, batch in enumerate(zip(copy.deepcopy(X_train[train_idx]),\n",
    "                                    copy.deepcopy(class_v[train_idx]),\n",
    "                                    copy.deepcopy(adj_train[train_idx]))):\n",
    "        edge_index, _ = dense_to_sparse(torch.from_numpy(batch[2]).cuda().float())\n",
    "        train_list.append(Data(x = torch.from_numpy(batch[0]).cuda().float(),\n",
    "                               y = torch.from_numpy(batch[1]).cuda().float(),\n",
    "                               edge_index = edge_index\n",
    "                                    )\n",
    "                         )\n",
    "    \n",
    "    for idx, batch in enumerate(zip(copy.deepcopy(X_train_val[val_idx]),\n",
    "                                    copy.deepcopy(class_v_val[val_idx]), \n",
    "                                    copy.deepcopy(adj_train_val[val_idx]))):\n",
    "        edge_index, _ = dense_to_sparse(torch.from_numpy(batch[2]).cuda().float())\n",
    "        train_list.append(Data(x = torch.from_numpy(batch[0]).cuda().float(),\n",
    "                             y = torch.from_numpy(batch[1]).cuda().float(),\n",
    "                             edge_index = edge_index\n",
    "                                    )\n",
    "                         )\n",
    "    batch_train_loader = DataLoader(train_list, batch_size=batch_size)\n",
    "    \n",
    "    val_list = []\n",
    "    for idx, batch in enumerate(zip(copy.deepcopy(X_test[test_idx]),\n",
    "                                    copy.deepcopy(class_v_test[test_idx]), \n",
    "                                    copy.deepcopy(adj_test[test_idx]))):\n",
    "        edge_index, _ = dense_to_sparse(torch.from_numpy(batch[2]).cuda().float())\n",
    "        val_list.append(Data(x = torch.from_numpy(batch[0]).cuda().float(),\n",
    "                             y = torch.from_numpy(batch[1]).cuda().float(),\n",
    "                             edge_index = edge_index\n",
    "                                    )\n",
    "                         )\n",
    "    batch_val_loader = DataLoader(val_list, batch_size=batch_size)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6445d4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_list\n",
    "del val_list\n",
    "del X_train\n",
    "del class_v\n",
    "del adj_train\n",
    "del X_train_val\n",
    "del class_v_val\n",
    "del adj_train_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a71e729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3072, 5])\n",
      "torch.Size([3072, 5])\n",
      "torch.Size([3072, 5])\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "for data in batch_train_loader:\n",
    "    idx+=1\n",
    "    print(data.x.shape)\n",
    "    if idx==3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13ef3074",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dims = 64\n",
    "batch_size = 128\n",
    "# batch_size = 8\n",
    "num_nodes = 24\n",
    "bbx_size = 4\n",
    "num_classes = 10\n",
    "label_shape = 1\n",
    "# nb_epochs = 350\n",
    "nb_epochs = 1\n",
    "klw = loss.frange_cycle_linear(nb_epochs)\n",
    "learning_rate = 0.000065\n",
    "hidden1 = 32\n",
    "hidden2 = 16\n",
    "hidden3 = 128\n",
    "adaptive_margin = True\n",
    "# adaptive_margin = False\n",
    "run_prefix = \"Reconstruction-Temp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "878f8e38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4cf0708b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8367915b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "400\n",
      "600\n",
      "[1,   629] loss: 44.519\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "reconstruction_loss_arr = []\n",
    "kl_loss_arr = []\n",
    "bbox_loss_arr = []\n",
    "adj_loss_arr = []\n",
    "\n",
    "# node_loss_arr = []\n",
    "\n",
    "reconstruction_loss_val_arr = []\n",
    "kl_loss_val_arr = []\n",
    "bbox_loss_val_arr = []\n",
    "adj_loss_val_arr = []\n",
    "\n",
    "\n",
    "iou_loss_dict={}\n",
    "mse_loss_dict={}\n",
    "iou_loss_dict_final={}\n",
    "mse_loss_dict_final={}\n",
    "\n",
    "# node_loss_val_arr = []\n",
    "\n",
    "class_dict = ['cow', 'sheep', 'bird', 'person', 'cat', 'dog', 'horse', 'aeroplane',\n",
    "              'motorbike', 'bicycle', 'car']\n",
    "\n",
    "vae = GCNAutoEncoder(latent_dims,\n",
    "                     num_nodes,\n",
    "                     bbx_size,\n",
    "                     num_classes,\n",
    "                     label_shape,\n",
    "                     hidden1,\n",
    "                     hidden2,\n",
    "                     hidden3,\n",
    "                     dynamic_margin=adaptive_margin)\n",
    "vae.to(device)\n",
    "optimizer = optim.Adam(vae.parameters(), lr=learning_rate)\n",
    "model_path = ('/home/tanvikamble/MeronymNet-PyTorch/src/model/'+run_prefix+'/GCN-lr-'\n",
    "                        +str(learning_rate)\n",
    "                        +'-batch-'+str(batch_size)\n",
    "                        +'-h1-'+str(hidden1)\n",
    "                        +'-h2-'+str(hidden2)\n",
    "                        +'-h3-'+str(hidden3)+'-test')\n",
    "summary_path = ('/home/tanvikamble/MeronymNet-PyTorch/src/runs/'+run_prefix+'/GCN-lr-'\n",
    "                        +str(learning_rate)\n",
    "                        +'-batch-'+str(batch_size)\n",
    "                        +'-h1-'+str(hidden1)\n",
    "                        +'-h2-'+str(hidden2)\n",
    "                        +'-h3-'+str(hidden3)+'-test')\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "writer = SummaryWriter(summary_path)\n",
    "icoef = 0\n",
    "\n",
    "for epoch in range(nb_epochs):\n",
    "\n",
    "    batch_loss = torch.tensor([0.0])\n",
    "    batch_kl_loss = torch.tensor([0.0])\n",
    "    batch_bbox_loss = torch.tensor([0.0])\n",
    "#     batch_node_loss = torch.tensor([0.0])\n",
    "    images = []\n",
    "    iou_loss_dict.clear()\n",
    "    mse_loss_dict.clear()\n",
    "   \n",
    "    vae.train()\n",
    "    i=0\n",
    "    for train_data in batch_train_loader:\n",
    "        node_data_true = train_data.x\n",
    "        label_true = node_data_true[:,:1]\n",
    "        class_true = train_data.y\n",
    "        adj_true = train_data.edge_index\n",
    "        batch = train_data.batch\n",
    "        \n",
    "        for param in vae.parameters():\n",
    "            param.grad=None\n",
    "        \n",
    "        output = vae(adj_true, node_data_true, label_true , class_true)\n",
    "#         node_data_pred, label_pred, z_mean, z_logvar, margin = output\n",
    "#         print(len(output))\n",
    "        node_data_pred, z_mean, z_logvar, margin = output\n",
    "        \n",
    "        kl_loss = loss.kl_loss(z_mean, z_logvar)\n",
    "        \n",
    "        bbox_loss = loss.bbox_loss(node_data_pred, node_data_true[:,1:], margin)\n",
    "        \n",
    "        iou_loss,mse_loss=loss.calc_losses(node_data_pred, node_data_true[:,1:])\n",
    "\n",
    "        \n",
    "        \n",
    "        for j in range(iou_loss.shape[0]):\n",
    "            class_i = class_dict[int(np.argmax(class_true[10*j:10*(j+1)].detach().to('cpu').numpy()).tolist())]\n",
    "            if class_i in iou_loss_dict and class_i in mse_loss_dict:\n",
    "                iou_loss_dict[class_i].append(float(iou_loss[j]))\n",
    "                mse_loss_dict[class_i].append(float(mse_loss[j]))\n",
    "            else:\n",
    "                iou_loss_dict[class_i]=[float(iou_loss[j])]\n",
    "                mse_loss_dict[class_i]=[float(mse_loss[j])]\n",
    "\n",
    "\n",
    "        kl_weight=0\n",
    "        if kl_weight>0:\n",
    "            reconstruction_loss = kl_loss*kl_weight + (bbox_loss)*24*5\n",
    "        else:\n",
    "            reconstruction_loss = (bbox_loss)*24*5\n",
    "            \n",
    "        reconstruction_loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        i+=1\n",
    "      \n",
    "        batch_loss += reconstruction_loss\n",
    "        batch_kl_loss += kl_loss\n",
    "        batch_bbox_loss += bbox_loss\n",
    "#         batch_node_loss += node_loss\n",
    "    \n",
    "        if i%200==0:\n",
    "            print(i)\n",
    "            global_step = epoch*len(batch_train_loader)+i\n",
    "            \n",
    "            writer.add_scalar(\"Loss/train/reconstruction_loss\", batch_loss.item()/(i+1), global_step)\n",
    "            writer.add_scalar(\"Loss/train/kl_loss\", batch_kl_loss.item()/(i+1), global_step)\n",
    "            writer.add_scalar(\"Loss/train/bbox_loss\", batch_bbox_loss.item()/(i+1), global_step)\n",
    "\n",
    "    for class_i in iou_loss_dict:\n",
    "        temp_list_1=np.array(iou_loss_dict[class_i])\n",
    "        temp_list_2=np.array(mse_loss_dict[class_i])\n",
    "        temp_mean_1=np.mean(temp_list_1)\n",
    "        temp_mean_2=np.mean(temp_list_2)\n",
    "        if class_i in iou_loss_dict_final:\n",
    "            iou_loss_dict_final[class_i].append(temp_mean_1)\n",
    "            mse_loss_dict_final[class_i].append(temp_mean_2)\n",
    "        else:\n",
    "            iou_loss_dict_final[class_i] = [temp_mean_1]\n",
    "            mse_loss_dict_final[class_i] = [temp_mean_2]\n",
    "    global_step = epoch*len(batch_train_loader)+i\n",
    "    image_shape = [num_nodes, bbx_size]\n",
    "\n",
    "    image = plot_bbx(np.reshape((node_data_true[:24,1:]*label_true[:24]).detach().to(\"cpu\").numpy(),\n",
    "                                image_shape)).astype(float)/255\n",
    "    writer.add_image('train/images/input', image, global_step, dataformats='HWC')\n",
    "    image = plot_bbx((node_data_pred[0]*label_true[:24]).detach().to(\"cpu\").numpy()).astype(float)/255\n",
    "    writer.add_image('train/images/generated', image, global_step, dataformats='HWC')\n",
    "    \n",
    "    reconstruction_loss_arr.append(batch_loss.detach().item()/(i+1))\n",
    "    kl_loss_arr.append(batch_kl_loss.detach().item()/(i+1))\n",
    "    bbox_loss_arr.append(batch_bbox_loss.detach().item()/(i+1))\n",
    "#     node_loss_arr.append(batch_node_loss.detach().item()/(i+1))\n",
    "    \n",
    "    print('[%d, %5d] loss: %.3f' %\n",
    "              (epoch + 1, i + 1, batch_loss/(i+1) ))\n",
    "    \n",
    "    batch_loss = torch.tensor([0.0])\n",
    "    batch_kl_loss = torch.tensor([0.0])\n",
    "    batch_bbox_loss = torch.tensor([0.0])\n",
    "#     batch_node_loss = torch.tensor([0.0])\n",
    "    images = []\n",
    "    vae.eval()\n",
    "    for i, val_data in enumerate(batch_val_loader, 0):\n",
    "        node_data_true = val_data.x\n",
    "        label_true = node_data_true[:,:1]\n",
    "        class_true = val_data.y\n",
    "        adj_true = val_data.edge_index\n",
    "        batch = val_data.batch\n",
    "        \n",
    "#         kl_weight = klw[icoef]\n",
    "        kl_weight=0\n",
    "        \n",
    "        output = vae(adj_true, node_data_true, label_true , class_true)\n",
    "        \n",
    "#         node_data_pred, label_pred, z_mean, z_logvar, margin = output\n",
    "        node_data_pred, z_mean, z_logvar, margin = output\n",
    "        kl_loss = loss.kl_loss(z_mean, z_logvar)\n",
    "        bbox_loss = loss.bbox_loss(node_data_pred, node_data_true[:,1:], margin)\n",
    "        \n",
    "#         node_loss = loss.node_loss(label_pred,label_true)\n",
    "        \n",
    "        reconstruction_loss = kl_loss*kl_weight + (bbox_loss)*24*5\n",
    "        \n",
    "        batch_loss += reconstruction_loss\n",
    "        batch_kl_loss += kl_loss\n",
    "        batch_bbox_loss += bbox_loss\n",
    "#         batch_node_loss += node_loss\n",
    "    image = plot_bbx(np.reshape((node_data_true[:24,1:]*label_true[:24]).detach().to(\"cpu\").numpy(),\n",
    "                                image_shape)).astype(float)/255\n",
    "    writer.add_image('val/images/input', image, global_step, dataformats='HWC')\n",
    "    image = plot_bbx((node_data_pred[0]*label_true[:24]).detach().to(\"cpu\").numpy()).astype(float)/255\n",
    "    writer.add_image('val/images/generated', image, global_step, dataformats='HWC')\n",
    "\n",
    "    reconstruction_loss_val_arr.append(batch_loss.detach().item()/(i+1))\n",
    "    kl_loss_val_arr.append(batch_kl_loss.detach().item()/(i+1))\n",
    "    bbox_loss_val_arr.append(batch_bbox_loss.detach().item()/(i+1))\n",
    "#     node_loss_val_arr.append(batch_node_loss.detach().item()/(i+1))\n",
    "    \n",
    "    writer.add_scalar(\"Loss/val/reconstruction_loss\", batch_loss.detach()/(i+1), global_step)\n",
    "    writer.add_scalar(\"Loss/val/kl_loss\", batch_kl_loss.detach()/(i+1), global_step)\n",
    "    writer.add_scalar(\"Loss/val/bbox_loss\", batch_bbox_loss.detach()/(i+1), global_step)\n",
    "#     writer.add_scalar(\"Loss/val/node_loss\", batch_node_loss.detach()/(i+1), global_step)\n",
    "       \n",
    "    if epoch%50 == 0:\n",
    "        torch.save(vae.state_dict(), model_path + '/model_weights.pth')\n",
    "\n",
    "#     if kl_loss_arr[-1]>0.5 and abs(bbox_loss_arr[-1] - bbox_loss_val_arr[-1]) < 0.012 and bbox_loss_arr[-1]<0.08 and epoch>60:\n",
    "#         icoef = icoef + 1  \n",
    "\n",
    "torch.save(vae.state_dict(),model_path + '/model_weights.pth')\n",
    "\n",
    "for i in range(100):    \n",
    "    image = plot_bbx(np.reshape((node_data_true[24*i:24*(i+1),1:]*label_true[24*i:24*(i+1)]).detach().to(\"cpu\").numpy(),\n",
    "                                    image_shape)).astype(float)/255\n",
    "    writer.add_image('result/images/'+str(i)+'-input', image, global_step, dataformats='HWC')\n",
    "    image = plot_bbx((node_data_pred[i]*label_true[24*i:24*(i+1)]).detach().to(\"cpu\").numpy()).astype(float)/255\n",
    "    writer.add_image('result/images/'+str(i)+'-generated', image, global_step, dataformats='HWC')\n",
    "    \n",
    "writer.flush()\n",
    "writer.close()\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a15631de",
   "metadata": {},
   "outputs": [],
   "source": [
    "iou_loss_val_dict={}\n",
    "mse_loss_val_dict={}\n",
    "\n",
    "iou_loss_val_dict_final={}\n",
    "mse_loss_val_dict_final={}\n",
    "\n",
    "#testing loop\n",
    "model_path = ('/home/tanvikamble/MeronymNet-PyTorch/src/model/'+run_prefix+'/GCN-lr-'\n",
    "                        +str(learning_rate)\n",
    "                        +'-batch-'+str(batch_size)\n",
    "                        +'-h1-'+str(hidden1)\n",
    "                        +'-h2-'+str(hidden2)\n",
    "                        +'-h3-'+str(hidden3)+'-test')\n",
    "summary_path = ('/home/tanvikamble/MeronymNet-PyTorch/src/runs/'+run_prefix+'/GCN-lr-'\n",
    "                        +str(learning_rate)\n",
    "                        +'-batch-'+str(batch_size)\n",
    "                        +'-h1-'+str(hidden1)\n",
    "                        +'-h2-'+str(hidden2)\n",
    "                        +'-h3-'+str(hidden3)+'-test')\n",
    "\n",
    "write_tensorboard = True\n",
    "if write_tensorboard:\n",
    "    writer = SummaryWriter(summary_path)\n",
    "\n",
    "vae = GCNAutoEncoder(latent_dims,num_nodes,bbx_size,num_classes,label_shape,hidden1, hidden2, hidden3, dynamic_margin=adaptive_margin)\n",
    "vae.load_state_dict(torch.load(model_path+ '/model_weights.pth'))\n",
    "\n",
    "decoder = vae.decoder\n",
    "image_shape = [num_nodes, bbx_size]\n",
    "global_step = 250000\n",
    "pred_boxes = []\n",
    "classes = []\n",
    "for i, val_data in enumerate(batch_val_loader, 0):\n",
    "    \n",
    "    val_data.cuda()\n",
    "    node_data_true = val_data.x\n",
    "    label_true = node_data_true[:,:1]\n",
    "    class_true = val_data.y\n",
    "    val_batch_size = int(class_true.shape[0]/10)\n",
    "    adj_true = val_data.edge_index\n",
    "    # First Model\n",
    "    output = vae(adj_true, node_data_true, label_true , class_true)\n",
    "    node_data_pred_test = output[0]\n",
    "    iou_loss,mse_loss=loss.calc_losses(node_data_pred_test, node_data_true[:,1:])\n",
    "   \n",
    "    \n",
    "    for j in range(iou_loss.shape[0]):\n",
    "        class_i = class_dict[int(np.argmax(class_true[10*j:10*(j+1)].detach().to('cpu').numpy()).tolist())]\n",
    "        if class_i in iou_loss_val_dict:\n",
    "            iou_loss_val_dict[class_i].append(float(iou_loss[j]))\n",
    "            mse_loss_val_dict[class_i].append(float(mse_loss[j]))\n",
    "            \n",
    "        else:\n",
    "            iou_loss_val_dict[class_i]=[float(iou_loss[j])]\n",
    "            mse_loss_val_dict[class_i]=[float(mse_loss[j])]\n",
    "            \n",
    "\n",
    "    \n",
    "for class_i in iou_loss_val_dict:\n",
    "    # First Model\n",
    "    temp_list_1=np.array(iou_loss_val_dict[class_i])\n",
    "    temp_list_2=np.array(mse_loss_val_dict[class_i])\n",
    "    temp_mean_1=np.mean(temp_list_1)\n",
    "    temp_mean_2=np.mean(temp_list_2)\n",
    "    iou_loss_val_dict_final[class_i] = temp_mean_1\n",
    "    mse_loss_val_dict_final[class_i] = temp_mean_2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ffb68b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------------+---------------------+\n",
      "|   Class   |  Average MSE value  |  Average IOU value  |\n",
      "+-----------+---------------------+---------------------+\n",
      "|   horse   |  1.1585874563718543 |  0.1304553826460067 |\n",
      "|    dog    |  0.6491665771285157 |  0.1855664219398098 |\n",
      "| motorbike | 0.24669410396784847 |  0.1284652777759052 |\n",
      "| aeroplane |  0.6427315825591404 |  0.1564387275240878 |\n",
      "|  bicycle  |  0.3345112708917557 | 0.11089806449317847 |\n",
      "|    bird   |  0.2780839392612426 |  0.1207820267998426 |\n",
      "|    cat    |  0.8539269100734145 | 0.16412315713764736 |\n",
      "|   person  |  0.6277235774950111 | 0.13391586611338702 |\n",
      "|   sheep   |  0.2972480461302706 | 0.17574471460984034 |\n",
      "|    cow    | 0.41189058063540807 |  0.1681610206123771 |\n",
      "+-----------+---------------------+---------------------+\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "# Specify the Column Names while initializing the Table\n",
    "myTable = PrettyTable([\"Class\", \"Average MSE value\", \"Average IOU value\"])\n",
    "\n",
    "for class_i in mse_loss_val_dict_final:\n",
    "    myTable.add_row([class_i,str(mse_loss_val_dict_final[class_i]),str(iou_loss_val_dict_final[class_i])])\n",
    " \n",
    "print(myTable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaacf557",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
