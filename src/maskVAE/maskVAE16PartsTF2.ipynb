{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "253ae26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pickle\n",
    "import math\n",
    "from tensorflow.keras import backend as K\n",
    "import sys\n",
    "import cv2\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a0f7f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a0f725f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class maskVAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.train_mask_loss = keras.metrics.Mean(name=\"train_mask_loss\")\n",
    "        self.val_mask_loss = keras.metrics.Mean(name=\"val_mask_loss\")\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "        self.kl_weight = 0.0\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.train_mask_loss,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "    \n",
    "    def reconstruction_loss(self, true_masks, pred_masks, z_mean, z_logvar, z_latent):\n",
    "    \n",
    "        kl_loss = tf.reduce_mean(0.5 * tf.reduce_sum(tf.square(z_mean) + tf.square(tf.exp(z_logvar)) - 2*(z_logvar) - 1,\n",
    "                                                     axis=1))\n",
    "        mask_loss = tf.reduce_mean(tf.keras.backend.binary_crossentropy(true_masks,pred_masks))\n",
    "\n",
    "        return mask_loss, kl_loss\n",
    "\n",
    "    def train_step(self, data):\n",
    "        (bx, cls, mask) = data[0]\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z_latent = self.encoder(data)\n",
    "            pred_mask = self.decoder((bx, cls, z_latent))\n",
    "            mask_loss, kl_loss = self.reconstruction_loss(mask, pred_mask, z_mean, z_log_var, z_latent)\n",
    "            total_loss = mask_loss + kl_loss*self.kl_weight\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.train_mask_loss.update_state(mask_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "#         self.update_kl_weight()\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"mask_loss\": self.train_mask_loss.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n",
    "    \n",
    "    def test_step(self, data):\n",
    "        \n",
    "        z_mean, z_log_var, z_latent = self.encoder(data)\n",
    "        (bx, cls, mask) = data[0]\n",
    "        pred_mask = self.decoder((bx, cls, z_latent))\n",
    "        mask_loss, kl_loss = self.reconstruction_loss(mask, pred_mask, z_mean, z_log_var, z_latent)\n",
    "        total_loss = mask_loss + kl_loss*self.kl_weight\n",
    "        self.val_mask_loss.update_state(mask_loss)\n",
    "        return {\n",
    "            \"mask_loss\": self.val_mask_loss.result(),\n",
    "        }\n",
    "    \n",
    "    def generate(self, bbx_cond, class_cond):\n",
    "        \n",
    "        batch = len(bbx_cond)\n",
    "        dim = 64\n",
    "        z_latent = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        pred_mask = self.decoder((bbx_cond, class_cond, z_latent))\n",
    "        \n",
    "        return pred_mask\n",
    "    \n",
    "    def reconstruct(self, data):\n",
    "        \n",
    "        z_mean, z_log_var, z_latent = self.encoder(data)\n",
    "        (bx, cls, mask) = data[0]\n",
    "        pred_mask = self.decoder((bx, cls, z_latent))\n",
    "        \n",
    "        return pred_mask\n",
    "        \n",
    "    \n",
    "    @tf.function\n",
    "    def update_kl_weight(self):\n",
    "        if (self.kl_loss_tracker.result() > 10.0 \n",
    "            and abs(self.train_mask_loss.result() - self.val_mask_loss.result())< 0.1 \n",
    "            and self.kl_weight<0.5):\n",
    "            self.kl_weight += 0.01 \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de3f0b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_postfix):\n",
    "    \n",
    "    outfile = 'D:/meronym_data/X_train'+file_postfix+'.np'\n",
    "    with open(outfile, 'rb') as pickle_file:\n",
    "        X_train = pickle.load(pickle_file)\n",
    "\n",
    "    outfile = 'D:/meronym_data/class_v'+file_postfix+'.np'\n",
    "    with open(outfile, 'rb') as pickle_file:\n",
    "        class_v = pickle.load(pickle_file)\n",
    "\n",
    "    outfile = 'D:/meronym_data/masks_train'+file_postfix+'.np'\n",
    "    with open(outfile, 'rb') as pickle_file:\n",
    "        masks = pickle.load(pickle_file)\n",
    "\n",
    "    outfile = 'D:/meronym_data/X_val'+file_postfix+'.np'\n",
    "    with open(outfile, 'rb') as pickle_file:\n",
    "        X_train_val = pickle.load(pickle_file)\n",
    "\n",
    "    outfile = 'D:/meronym_data/class_v_val'+file_postfix+'.np'\n",
    "    with open(outfile, 'rb') as pickle_file:\n",
    "        class_v_val = pickle.load(pickle_file)\n",
    "\n",
    "    outfile = 'D:/meronym_data/masks_val'+file_postfix+'.np'\n",
    "    with open(outfile, 'rb') as pickle_file:\n",
    "        masks_val = pickle.load(pickle_file)\n",
    "\n",
    "#     outfile = 'D:/meronym_data/X_test'+part_data_post_fix+'.np'\n",
    "#     with open(outfile, 'rb') as pickle_file:\n",
    "#         X_test = pickle.load(pickle_file)\n",
    "\n",
    "#     outfile = 'D:/meronym_data/X_test'+obj_data_postfix+'.np'\n",
    "#     with open(outfile, 'rb') as pickle_file:\n",
    "#         X_obj_test = pickle.load(pickle_file)\n",
    "        \n",
    "        \n",
    "    return X_train, class_v, masks, X_train_val, class_v_val, masks_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98aaf51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_latent(a, b, c, d=None):\n",
    "    p = np.random.permutation(len(a))\n",
    "    if d is None:\n",
    "        return a[p], b[p], c[p]\n",
    "    return a[p], b[p], c[p], d[p]\n",
    "\n",
    "def sampling(z_mean, z_log_var):\n",
    "    epsilon = tf.random_normal(tf.shape(z_log_var), name=\"epsilon\")\n",
    "    return z_mean + epsilon * tf.exp(z_log_var)\n",
    "\n",
    "def frange_cycle_linear(n_iter, start=0.0, stop=1.0,  n_cycle=4, ratio=0.5):\n",
    "    L = np.ones(n_iter) * stop\n",
    "    period = n_iter/n_cycle\n",
    "    step = (stop-start)/(period*ratio)\n",
    "\n",
    "    for c in range(n_cycle):\n",
    "        v, i = start, 0\n",
    "        while v <= stop and (int(i+c*period) < n_iter):\n",
    "            L[int(i+c*period)] = v\n",
    "            v += step\n",
    "            i += 1\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86301980",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruction_loss(true_masks, pred_masks, z_mean, z_logvar, z_latent):\n",
    "    \n",
    "    kl_loss = tf.reduce_mean(0.5 * tf.reduce_sum(tf.square(z_mean) + tf.square(tf.exp(z_logvar)) - 2*(z_logvar) - 1,\n",
    "                                                 axis=1))  \n",
    "    mask_loss = tf.reduce_mean(tf.keras.backend.binary_crossentropy(true_masks,pred_masks))\n",
    "\n",
    "    return mask_loss, kl_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c591a08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "max_num_node = 16\n",
    "latent_dims = 64\n",
    "\n",
    "latent_inputs = keras.Input(shape=(latent_dims,))\n",
    "\n",
    "true_maps = keras.Input(shape=([max_num_node, 64, 64, 1]), dtype=tf.float32)\n",
    "true_masks = keras.Input(shape=([max_num_node, 64, 64, 1]), dtype=tf.float32)\n",
    "true_edges = keras.Input(shape=([max_num_node, 64, 64, 1]), dtype=tf.float32)\n",
    "\n",
    "true_bbxs = keras.Input(shape=([max_num_node, 4]), dtype=tf.float32)\n",
    "cond_bbxs = keras.Input(shape=([max_num_node, 4]), dtype=tf.int32)\n",
    "\n",
    "true_lbls = keras.Input(shape=([max_num_node, 1]), dtype=tf.float32)\n",
    "cond_lbls = keras.Input(shape=([max_num_node, 1]), dtype=tf.float32)\n",
    "\n",
    "true_classes = keras.Input(shape=([7]), dtype=tf.float32)\n",
    "cond_classes = keras.Input(shape=([7]), dtype=tf.float32)\n",
    "\n",
    "\n",
    "rnn_bbxs = layers.Bidirectional(layers.GRU(4, return_sequences=True))(true_bbxs)\n",
    "concatenated_bbx_lbl = rnn_bbxs\n",
    "dense_cond = layers.Dense(64, activation='tanh')(true_classes)\n",
    "enc = layers.TimeDistributed(layers.Conv2D(8, kernel_size=3))(true_masks)\n",
    "enc = layers.TimeDistributed(layers.BatchNormalization(trainable = False))(enc)\n",
    "enc = layers.TimeDistributed(layers.Activation('relu'))(enc)\n",
    "\n",
    "enc = layers.TimeDistributed(layers.Conv2D(16, kernel_size=3))(enc)\n",
    "enc = layers.TimeDistributed(layers.BatchNormalization(trainable = False))(enc)\n",
    "enc = layers.TimeDistributed(layers.Activation('relu'))(enc)\n",
    "\n",
    "enc = layers.TimeDistributed(layers.MaxPooling2D(pool_size=(2, 2)))(enc)\n",
    "\n",
    "enc = layers.TimeDistributed(layers.Conv2D(32, kernel_size=3, activation='relu'))(enc)\n",
    "enc = layers.TimeDistributed(layers.BatchNormalization(trainable = False))(enc)\n",
    "enc = layers.TimeDistributed(layers.Activation('relu'))(enc)\n",
    "enc = layers.TimeDistributed(layers.Flatten())(enc)\n",
    "TDD = layers.TimeDistributed(layers.Dense(64, activation='relu', name = 'encoded_bitmaps'))\n",
    "dense_enc_maps = TDD(enc)\n",
    "\n",
    "BGRU = layers.Bidirectional(layers.GRU(32, return_sequences=True))\n",
    "rnn_maps = BGRU(dense_enc_maps)\n",
    "\n",
    "D = layers.Dense(64, activation='tanh')\n",
    "attention = D(concatenated_bbx_lbl)\n",
    "sent_representation = layers.Multiply()([rnn_maps, attention])\n",
    "sent_representation = layers.Multiply()([sent_representation, dense_cond])\n",
    "images_with_attention = layers.Lambda(lambda xin: K.sum(xin, axis=-2),\n",
    "                                            output_shape=(128,))(sent_representation)\n",
    "\n",
    "z_mean = layers.Dense(64, activation='tanh')(images_with_attention)\n",
    "z_log_var = layers.Dense(64, activation='tanh', name='z_logvar')(images_with_attention)\n",
    "\n",
    "z_latent = z_mean #sampling(z_mean, z_log_var)\n",
    "# z_latent = Sampling()([z_mean, z_log_var])\n",
    "encoder = keras.Model(inputs=[true_bbxs, true_classes, true_masks],\n",
    "                      outputs=[z_mean, z_log_var, z_latent], \n",
    "                      name='encoder')\n",
    "\n",
    "# Decoder\n",
    "cond_bbx = layers.Lambda(lambda xin: K.sum(xin, axis=-1), output_shape=(4,))(cond_bbxs)\n",
    "cond_cat = cond_bbx\n",
    "\n",
    "cond_fully_cat = layers.Dense(64, activation='relu')(cond_cat)\n",
    "cond_class_ = layers.Dense(64, activation='relu')(cond_classes)\n",
    "conditioned_z = layers.concatenate([cond_fully_cat, latent_inputs], axis=-1, name='conditioned_z_1')\n",
    "conditioned_z = layers.concatenate([conditioned_z, cond_class_], axis=-1, name='conditioned_z_2')\n",
    "decoded = layers.RepeatVector(max_num_node)(conditioned_z)\n",
    "decoded = layers.Bidirectional(layers.GRU(32, return_sequences=True))(decoded)\n",
    "dec_dense = layers.TimeDistributed(layers.Dense(25088, activation='relu',  name = 'encoding'))(decoded)\n",
    "dec_conv = layers.TimeDistributed(layers.Reshape((28, 28, 32)))(dec_dense)\n",
    "\n",
    "dec = layers.TimeDistributed(layers.Conv2DTranspose(32, kernel_size=3, padding='same'))(dec_conv)\n",
    "dec = layers.TimeDistributed(layers.BatchNormalization(trainable = False))(dec)\n",
    "dec = layers.TimeDistributed(layers.Activation('relu'))(dec)\n",
    "\n",
    "dec = layers.TimeDistributed(layers.Conv2DTranspose(16, kernel_size=3))(dec)\n",
    "dec = layers.TimeDistributed(layers.BatchNormalization(trainable = False))(dec)\n",
    "dec = layers.TimeDistributed(layers.Activation('relu'))(dec)\n",
    "\n",
    "dec = layers.TimeDistributed(layers.UpSampling2D(size=(2, 2)))(dec)\n",
    "\n",
    "dec = layers.TimeDistributed(layers.Conv2DTranspose(8, kernel_size=3))(dec)\n",
    "dec = layers.TimeDistributed(layers.BatchNormalization(trainable = False))(dec)\n",
    "dec = layers.TimeDistributed(layers.Activation('relu'))(dec)\n",
    "\n",
    "decoder_bitmaps = layers.TimeDistributed(layers.Conv2DTranspose(1, kernel_size=3,\n",
    "                                                                            activation='sigmoid', \n",
    "                                                                            name = 'decoded_mask'))(dec)\n",
    "decoder = keras.Model(inputs=[cond_bbxs, cond_classes, latent_inputs],\n",
    "                      outputs=decoder_bitmaps, \n",
    "                      name='decoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a245ecef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "213/213 [==============================] - 66s 266ms/step - loss: 0.6899 - mask_loss: 0.6899 - kl_loss: 0.8401 - val_mask_loss: 0.6431\n",
      "Epoch 2/200\n",
      "213/213 [==============================] - 54s 255ms/step - loss: 0.6214 - mask_loss: 0.6214 - kl_loss: 3.4949 - val_mask_loss: 0.5933\n",
      "Epoch 3/200\n",
      "213/213 [==============================] - 54s 253ms/step - loss: 0.5487 - mask_loss: 0.5487 - kl_loss: 3.9965 - val_mask_loss: 0.5552\n",
      "Epoch 4/200\n",
      "213/213 [==============================] - 55s 257ms/step - loss: 0.4933 - mask_loss: 0.4933 - kl_loss: 4.3346 - val_mask_loss: 0.5284\n",
      "Epoch 5/200\n",
      "213/213 [==============================] - 54s 252ms/step - loss: 0.4687 - mask_loss: 0.4687 - kl_loss: 4.6711 - val_mask_loss: 0.5096\n",
      "Epoch 6/200\n",
      "213/213 [==============================] - 54s 254ms/step - loss: 0.4601 - mask_loss: 0.4601 - kl_loss: 4.8441 - val_mask_loss: 0.4953\n",
      "Epoch 7/200\n",
      "213/213 [==============================] - 54s 254ms/step - loss: 0.4477 - mask_loss: 0.4477 - kl_loss: 5.0003 - val_mask_loss: 0.4841\n",
      "Epoch 8/200\n",
      "213/213 [==============================] - 54s 252ms/step - loss: 0.4425 - mask_loss: 0.4425 - kl_loss: 5.1049 - val_mask_loss: 0.4753\n",
      "Epoch 9/200\n",
      "213/213 [==============================] - 54s 254ms/step - loss: 0.4395 - mask_loss: 0.4395 - kl_loss: 5.2370 - val_mask_loss: 0.4681\n",
      "Epoch 10/200\n",
      "213/213 [==============================] - 55s 257ms/step - loss: 0.4336 - mask_loss: 0.4336 - kl_loss: 5.3858 - val_mask_loss: 0.4620\n",
      "Epoch 11/200\n",
      "213/213 [==============================] - 54s 254ms/step - loss: 0.4359 - mask_loss: 0.4359 - kl_loss: 5.5318 - val_mask_loss: 0.4570\n",
      "Epoch 12/200\n",
      "213/213 [==============================] - 54s 252ms/step - loss: 0.4313 - mask_loss: 0.4313 - kl_loss: 5.6818 - val_mask_loss: 0.4526\n",
      "Epoch 13/200\n",
      "213/213 [==============================] - 54s 254ms/step - loss: 0.4296 - mask_loss: 0.4296 - kl_loss: 5.7820 - val_mask_loss: 0.4488\n",
      "Epoch 14/200\n",
      "213/213 [==============================] - 54s 255ms/step - loss: 0.4285 - mask_loss: 0.4285 - kl_loss: 6.0451 - val_mask_loss: 0.4454\n",
      "Epoch 15/200\n",
      "213/213 [==============================] - 54s 256ms/step - loss: 0.4290 - mask_loss: 0.4290 - kl_loss: 6.2530 - val_mask_loss: 0.4424\n",
      "Epoch 16/200\n",
      "213/213 [==============================] - 54s 252ms/step - loss: 0.4232 - mask_loss: 0.4232 - kl_loss: 6.4051 - val_mask_loss: 0.4397\n",
      "Epoch 17/200\n",
      "213/213 [==============================] - 54s 254ms/step - loss: 0.4224 - mask_loss: 0.4224 - kl_loss: 6.6047 - val_mask_loss: 0.4373\n",
      "Epoch 18/200\n",
      "213/213 [==============================] - 54s 252ms/step - loss: 0.4265 - mask_loss: 0.4265 - kl_loss: 6.8240 - val_mask_loss: 0.4351\n",
      "Epoch 19/200\n",
      "213/213 [==============================] - 54s 252ms/step - loss: 0.4229 - mask_loss: 0.4229 - kl_loss: 7.0491 - val_mask_loss: 0.4330\n",
      "Epoch 20/200\n",
      "213/213 [==============================] - 54s 256ms/step - loss: 0.4182 - mask_loss: 0.4182 - kl_loss: 7.2816 - val_mask_loss: 0.4311\n",
      "Epoch 21/200\n",
      "213/213 [==============================] - 54s 252ms/step - loss: 0.4192 - mask_loss: 0.4192 - kl_loss: 7.4801 - val_mask_loss: 0.4294\n",
      "Epoch 22/200\n",
      "213/213 [==============================] - 54s 253ms/step - loss: 0.4182 - mask_loss: 0.4182 - kl_loss: 7.6870 - val_mask_loss: 0.4277\n",
      "Epoch 23/200\n",
      "213/213 [==============================] - 54s 252ms/step - loss: 0.4164 - mask_loss: 0.4164 - kl_loss: 7.9459 - val_mask_loss: 0.4262\n",
      "Epoch 24/200\n",
      "213/213 [==============================] - 55s 259ms/step - loss: 0.4149 - mask_loss: 0.4149 - kl_loss: 8.1352 - val_mask_loss: 0.4247\n",
      "Epoch 25/200\n",
      "213/213 [==============================] - 53s 250ms/step - loss: 0.4101 - mask_loss: 0.4101 - kl_loss: 8.3495 - val_mask_loss: 0.4233\n",
      "Epoch 26/200\n",
      "213/213 [==============================] - 53s 251ms/step - loss: 0.4089 - mask_loss: 0.4089 - kl_loss: 8.6105 - val_mask_loss: 0.4220\n",
      "Epoch 27/200\n",
      "213/213 [==============================] - 54s 253ms/step - loss: 0.4122 - mask_loss: 0.4122 - kl_loss: 8.8437 - val_mask_loss: 0.4208\n",
      "Epoch 28/200\n",
      "213/213 [==============================] - 53s 251ms/step - loss: 0.4089 - mask_loss: 0.4089 - kl_loss: 9.0861 - val_mask_loss: 0.4195\n",
      "Epoch 29/200\n",
      "213/213 [==============================] - 54s 252ms/step - loss: 0.4094 - mask_loss: 0.4094 - kl_loss: 9.3313 - val_mask_loss: 0.4184\n",
      "Epoch 30/200\n",
      "213/213 [==============================] - 55s 257ms/step - loss: 0.4103 - mask_loss: 0.4103 - kl_loss: 9.5389 - val_mask_loss: 0.4172\n",
      "Epoch 31/200\n",
      "213/213 [==============================] - 54s 252ms/step - loss: 0.4031 - mask_loss: 0.4031 - kl_loss: 9.8394 - val_mask_loss: 0.4161\n",
      "Epoch 32/200\n",
      "213/213 [==============================] - 54s 252ms/step - loss: 0.3987 - mask_loss: 0.3987 - kl_loss: 9.9844 - val_mask_loss: 0.4151\n",
      "Epoch 33/200\n",
      "213/213 [==============================] - 55s 259ms/step - loss: 0.4015 - mask_loss: 0.4015 - kl_loss: 10.1762 - val_mask_loss: 0.4140\n",
      "Epoch 34/200\n",
      "153/213 [====================>.........] - ETA: 14s - loss: 0.3949 - mask_loss: 0.3949 - kl_loss: 10.2223"
     ]
    }
   ],
   "source": [
    "lr = 0.00002\n",
    "file_postfix = '_combined_mask_data'\n",
    "\n",
    "mask_vae_model = maskVAE(encoder, decoder)\n",
    "X_train, class_v, masks, X_train_val, class_v_val, masks_val = load_data(file_postfix)\n",
    "gc.collect()\n",
    "# train_dataset = tf.data.Dataset.from_tensor_slices((X_train[:,:,1:], class_v, masks))\n",
    "# val_dataset = tf.data.Dataset.from_tensor_slices((X_train_val[:,:,1:], class_v_val, masks_val))\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    elif epoch%10==0:\n",
    "        return lr * tf.math.exp(-0.1)\n",
    "    else:\n",
    "        return lr\n",
    "    \n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=\"D:/meronym_data/runs/mask_generation_model_tf2_reconstruction/lr00002/maskvae.ckpt\",\n",
    "        save_freq=10),\n",
    "    tf.keras.callbacks.LearningRateScheduler(scheduler)]\n",
    "\n",
    "mask_vae_model.compile(optimizer=keras.optimizers.Adam(lr))\n",
    "mask_vae_model.fit(((X_train[:,:,1:], class_v, masks)),\n",
    "                   epochs=200, batch_size=16, \n",
    "                   validation_data=((X_train_val[:,:,1:], class_v_val, masks_val),),\n",
    "                   callbacks=callbacks, \n",
    "                   shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3cc1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"D:/meronym_data/runs/mask_generation_model_tf2_reconstruction/lr00002/maskvae.ckpt\"\n",
    "mask_vae_model.load_weights(checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bc9a61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generated_masks = []\n",
    "batch_size = 100\n",
    "for i in range(len(X_train_val)//batch_size):\n",
    "    generated_masks.append(\n",
    "        mask_vae_model.reconstruct(\n",
    "            (\n",
    "                np.float32(X_train_val[i:batch_size*(i+1),:,1:]),\n",
    "                np.float32(class_v_val[i:batch_size*(i+1)])\n",
    "                np.float32(masks_val[i:batch_size*(i+1)])\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a27e4ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "outfile = 'D:/meronym_data/generated_masks.npy'\n",
    "with open(outfile, 'wb') as pickle_file:\n",
    "    generated_masks = np.concatenate(generated_masks)\n",
    "    pickle.dump(generated_masks, pickle_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
